{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Latest Blog </p> <ul> <li>Developing Artist Recommendation </li> <li>Measuring Impact of New Initiative with A/B Test</li> <li>Give Offer to Right User</li> </ul>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/","title":"Measuring Effect of Product Recommendation Initiative to decide whether to  launch feature to all user","text":"<p> Photo by Isaac Smith on Unsplash </p> <p>PT Buah Alam , is an online grocery store that are currently growing at an grow stages of startup, to reach profitability they need to make a move. One of the eandeavor by approaching by boosting sales on platform. </p> <p>How to do that ? There are lot of ways. One of them through upselling / cross selling </p> <p>The Data Science Team already developed the recommendation algorithm in checkout pages </p> <p></p> <p>Here's a catch : </p> <ol> <li> <p>If we implement new initiative to all of our user , the cost is too big and does not guarantee better marketing performance</p> </li> <li> <p>If we implement don't implement / atleast try, we have opportunity cost , \"maybe\" this initiative works well and yield better marketing performance </p> </li> </ol> <p>Solution ? </p> <p>There are several ways to prove causality / causal relationship </p> <p></p> <p>One of the most reliable tool is through Randomized Control Trial (RCT) </p> <p>For example, we want to estimate the effect on changing call to action in payment button</p> <p></p> <p>However there are still other potential cause or what we called as confounding variable. </p> <p></p> <p>To remove the bias , we could add randomization to make other confounder equal , by assignming randomly which user get button change</p> <p></p> <p>That the basic idea of Randomized Control Trial or commonly called as A/B Test</p> <p>So when does the A/B Test works ? </p> <ol> <li> <p>When each experiment unit doesnot interfere / influence other experiment unit (Stable Unit Treatment Value Assumption)    For example in marketplace where there is network effect</p> </li> <li> <p>When assigning experiment is possible (for example it's unethical to assign people to get lower education)</p> </li> <li> <p>When period is too long For example experimenting on ads campaign to measure number of deals made for property housing </p> </li> </ol> <p>So is it possible in our case ? Yes ! It's possible because : </p> <ol> <li> <p>Each user can be assigned to different feature </p> </li> <li> <p>Each user doesnot interfere with other users</p> </li> <li> <p>The period to measure causal impact still feasible </p> </li> </ol> <p>So how do we do A/B test? Don't worry here is the steps : </p> <p></p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#1-setting-up-problem","title":"1. Setting Up Problem","text":"<p>Our focus is about increasing revenue ? But which revenue ? There are lot of revenues. The Revenue we are using is from user transaction because we can intervene on it. </p> <p>After that we should have idea what is the current funnel / user journey in buying phase </p> <p></p> <p>After that we should decide what metrics as our target of experimentation. Back to our goal this recommender system initiative to boost revenue. Spesifically Revenue from User Transaction. Hence our metrics is Revenue per User</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#2-designing-experiment","title":"2. Designing Experiment","text":""},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#21-experiment-unit","title":"2.1. Experiment Unit","text":"<p>Our Experiment unit is user level , but which user since the recommendation system is located checkout process , our user pool is user who proceed into checkout process </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#22-group","title":"2.2. Group","text":"<ul> <li>Control Group : Users who dont receive product recommendation during checkout process </li> <li>Treatment Group : User who receive product recommendation during checkout process </li> </ul>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#23-hypothesis","title":"2.3. Hypothesis","text":"<p>We want to check if revenue per user for treatment group is &gt; control group </p> <p>\\(H_0 = \\mu_{control} &gt;= \\mu_{treatment}\\)</p> <p>\\(H_1 = \\mu_{control} &lt; \\mu_{treatment}\\)</p> <p>Our statistical test aim to prove the hypothesis whether it is rejected or fail to reject </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#23-number-of-samples","title":"2.3. Number of Samples","text":"<p>As Information , we have 15 Million Active User , with median monthly in-checkout-process user of 20.000 . We use median because number of user in checkout process is vary. </p> <p>So how many number of user we need for both treatment and control ? Well it's depend on Treatment Effect you want to capture , the smaller the effect you want to capture the more sample we need to add because of the experiment precision</p> <p>Decision to role out new feature based on whether new initiative meet certain criteria such as :</p> <ol> <li> <p>Statistically Significant     We'll talk about it later</p> </li> <li> <p>Practically Significant    Related to Business Requirement</p> </li> </ol>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#231-business-requirement","title":"2.3.1 Business Requirement","text":"<p>Our new initiative by recommending purchase is indeed costly, we have to calculate the cost first </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#2311-cost-of-making-recommendation","title":"2.3.1.1. Cost of Making Recommendation","text":"<ol> <li> <p>Man Power     The Development process assumed in six month , people that are involved : </p> <ul> <li> <p>Data Scientist , assume monthly rate $ 8000 to $ 12000 / month x 6 month = $ 48.000 - $ 72.000 </p> </li> <li> <p>Data Engineer  , assume monthly rate $8000 to $12000 / month x 6 month = $48.000 - $72.000 </p> </li> <li> <p>MLOps , assume monthly rate $9000 to $10000 / month x 6 month = $45.000 - $60.000 </p> </li> <li> <p>Software Engineer , assume monthly rate $9000 to $10000 / month x 6 month = $45.000 - $60.000 </p> Development Cost Development Cost Role Lower Bound Cost Upper Bound Cost Data Scientist 48000 72000 Data Engineer 48000 72000 MLOps 45000 60000 Software Engineer 45000 60000 Total Cost 186000 264000 </li> </ul> </li> <li> <p>Cloud      to handle ~ 100k users per  day x 30 day, another information is that the calculation is in batch manner, we don't need real time recommendation yet. Let say the cost isUSD \\(1000-2000 per month  x 12 month =\\)1.000 -$24.000  annualy</p> </li> <li> <p>Cost Summarization</p> </li> </ol> Cost Attribute Lower Bound Cost Upper Bound Cost Development Cost 186000 264000 Annual Infra Cost 12000 24000 Total 198000 288000"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#2312-reasonable-revenue","title":"2.3.1.2. Reasonable Revenue","text":"<p>We have seen that in order to make recommender system is not cheap at this scale, hence at what incremental level should we roll out this feature to all user ? Of Course covering costs + expected increase in user revenue. </p> <p>About cost we have already know let say we take  USD 288,000 , what about expected increase in user revenue. As per now median per user revenue was about USD 1 daily.  Median  of monthly user checkout 100.000 user  minimum revenue should be \\(Revenue &gt;= 288.000\\) </p> <p>Current Revenue per User/ annual  = 1 x 20.000 x 12  =  USD 240.000  Gaps = Not Yet Covered Cost + Expected Growth USD 48.000 / (12 x 20.000 ) = USD 0.2 </p> <p>Revenue / User / Daily &gt;= USD 1.2 </p> <p>If we continue to success we can have USD 1 increase from USD 1 to USD 2 We can have Additional Revenue of </p> <p>First Year</p> <p>2-1.2 x 20.000 x 12 = USD 192.000</p> <p>Second Year or more</p> <p>2 x 20.000 x 12 - (Annual Infra Cost) </p> <p>2 x 20.000 x 12 - (24.000) = USD 456.000</p> <p>So in this case we have to able to detect USD 1 Change in revenue per user. This will relate to the next problem which is power Analysis</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#231-power-analysis","title":"2.3.1 Power Analysis","text":"<p>In statistical testing there are two possible Errors : </p> <ul> <li>Rejecting when it should failed to be Rejected, or (Type I Error) </li> <li>Fail to Reject when it should be Rejected or (Type II Error)</li> </ul> <p>Typically Type I Error influence what we called as significance level or \\(\\alpha\\). While the second one is influenced by Power \\((1-\\beta)\\)</p> <p>Will be focusing on Power Analysis </p> <ul> <li>Power is the probability to reject \\(H_{0}\\) given \\(H_{0}\\) is false.</li> </ul> \\[ \\text{power} = 1-\\beta = P(\\text{reject } H_{0} \\ | \\ H_{0} \\text{ is false}) \\] <ul> <li>Power is dependent to Factor such as </li> <li>Effect size</li> <li>Data variance</li> <li>Sample size (\\(n\\))</li> </ul> <p>We will simulate how above mentioned factor related to power </p> <pre><code>import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport math \nimport scipy.stats as stats\n</code></pre> <pre><code>def generate_data(mu_1,mu_2,std_dev_1,std_dev_2,n_sample_1,n_sample_2, N=10000, n_repeat=1000):\n\n    # generate data \n    data_1 = np.random.normal(mu_1, std_dev_1, N)\n    data_2 = np.random.normal(mu_2, std_dev_2, N)\n\n    sample_1_mean = []\n    sample_2_mean = []\n\n    for i in range(n_repeat):\n        # Generate data\n        sample_1 = np.random.choice(data_1, size=(1, n_sample_1), replace=False)\n        sample_2 = np.random.choice(data_2, size=(1, n_sample_2), replace=False)\n\n        # obtain mean \n        mean_1 = sample_1.mean()\n        mean_2 = sample_2.mean()\n\n        # Append mean to the list\n        sample_1_mean.append(mean_1)\n        sample_2_mean.append(mean_2)\n\n\n    return sample_1_mean,sample_2_mean\n</code></pre> <pre><code>def generate_ab_viz(mean_1,mean_2) : \n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 7))\n\n    sns.histplot(mean_1,alpha = 1, bins = 30,kde=True,\n                label = r\"Group 1 - $H_0$\",color='red')\n    sns.histplot(mean_2,alpha = 1, bins = 30,kde=True,\n                label = r\"Group 2 - $H_1$\",color='green')\n    plt.axvline(np.mean(mean_1),color = 'b',label='group 1 mean',linestyle='--')\n    plt.axvline(np.mean(mean_2),color = 'yellow',label='group 2 mean',linestyle='--')\n    ax.legend()\n    plt.show()\n</code></pre> <p>Let's generate some simulation </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#variance","title":"Variance","text":"<p>Bigger</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':20 , 'std_dev_2':20, \n    'n_sample_1':100,'n_sample_2':100,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>The More Variance we get, is getting harder to see the effect size of two groups</p> <p>Smaller</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':1 , 'std_dev_2':1, \n    'n_sample_1':100,'n_sample_2':100,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>The Less Variance we get we can see the difference clearly now </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#sample-size","title":"Sample Size","text":"<p>Smaller</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':100,'n_sample_2':100,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>Bigger</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':500,'n_sample_2':500,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#effect-size","title":"Effect Size","text":"<p>or Change / Increase we want to detect</p> <p>Smaller</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 1+0.01 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':500,'n_sample_2':500,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>Bigger</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 1+5 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':500,'n_sample_2':500,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>The more small effect size we want to detect \u2192 more power required</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#wrapping-up","title":"Wrapping Up","text":"<p>In this case we have already know how much effect we wanna test , in this case USD 1 Increase in Revenue oer User </p> <p>to calculate minimum sample size we can use :   </p> <p>with : </p> <ol> <li>\\(\\delta\\) : Effect Size</li> <li>\\(\\sigma\\) : variance of Population (estimated) from revenue per user variance </li> <li>\\(\\alpha/2\\) : alpha ,  \\(1- Confidence Interval\\)</li> <li>\\(\\beta\\) : beta, 1- Power Level</li> <li>\\(z\\) : z score</li> </ol> <p>Deciding \\(\\alpha\\) and \\(\\beta\\) , in a typical experiment </p> <ol> <li>\\(\\alpha\\) is 5% or Confidence is 95%</li> <li>\\(\\beta\\) is 20%  or Power is 80%</li> </ol> <pre><code>def calculate_minimum_sample(effect_size,std,alpha,beta) : \n    # obtain both zscore\n    z_alpha_2 = stats.norm.ppf(1 - alpha/2)\n    z_beta = stats.norm.ppf(1 - beta)\n\n    upper = 2*(std**2)*(z_alpha_2 + z_beta)**2\n    lower = effect_size**2\n\n    n_sample = math.ceil(upper/lower)\n    return n_sample\n</code></pre> <pre><code>n_sample = calculate_minimum_sample(effect_size=1,std=10,alpha=0.05,beta=0.2)\nn_sample\n</code></pre> <pre><code>1570\n</code></pre> <p>For each variant we should assign 1570 randomization unit, in this case is user </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#24-experiment-durations","title":"2.4. Experiment Durations","text":"<p>A good experiment duration &lt; 1 month , since the variation is only 2, we can try to conduct it by 1 week. Starting from February 2<sup>nd</sup>, 2024 to February 8<sup>th</sup>, 2024</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#3-running-experiments","title":"3. Running Experiments","text":"<p>Since we don't have a privilege to experimentation platform we will use generated data. </p> <p>But before running the experiments we will have to do A/A test ? </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#31-before-experiment-aa-tests","title":"3.1. Before Experiment : A/A Tests","text":"<p>Why ? Because there are maybe still not a ceterus paribus condition no bias , for example user may have experienced delayed response time that cause user to differ in behaviour which eventually will influence our metrics of interest</p> <p>How ? In order to make that not happen, we can compare same variant by having the same metrics. </p> <p>But, it's expensive to run experiment before experiment. There is a hackish way, by using metrics from last observation, for example last week metrics , and then randomly assign user two different group. </p> <p>Workflow : </p> <ol> <li>Obtain Last Week metrics from each user (revenue per user)</li> <li>For n iteration do : <ul> <li>Randomly Assign user into two groups</li> <li>Obtain means of each metrics from both group</li> <li>Conduct statistical test, to check differences of mean metrics between two variant using t-test</li> <li>Collect p-value</li> </ul> </li> <li>Test the p-value distribution whether followed uniform distribution using Goodness of Fit test such as Kolmogorov Smirnoff Test</li> <li>If the p-value &gt; \\(\\alpha\\) can be concluded that the samples is the same from referenced distribution ( in this case uniform distribution) </li> <li>Plot the p-value from n_iteration, the p value should be uniform (indicating variant is the same) </li> </ol> <p>We will generate using synthetic data </p> <pre><code>num_samples = int((100_000/30)*7)\n\ndef generate_data(num_samples,metric_name) : \n    dummy_aa_test = pd.DataFrame({\n        'user_id': np.arange(1, num_samples + 1),\n       f'{metric_name}': stats.halfnorm(loc=1, scale=10).rvs(num_samples) # constraint revenue to positive\n    })\n    return dummy_aa_test\n\ndummy_aa_test = generate_data(num_samples,metric_name='revenue_per_user')\ndummy_aa_test\n</code></pre> user_id revenue_per_user 0 1 5.264200 1 2 11.863647 2 3 5.423547 3 4 14.484032 4 5 15.570061 ... ... ... 23328 23329 9.137816 23329 23330 6.039538 23330 23331 16.040150 23331 23332 1.436674 23332 23333 5.646788 <p>23333 rows \u00d7 2 columns</p> <pre><code>def simulate_aa_test(n_iter,metric_name,num_samples) : \n    data = generate_data(num_samples,metric_name=metric_name)\n\n    p_vals_t_test = []\n    for i in range(n_iter) : \n        data['group'] = np.random.choice([1,2 ], size=data.shape[0])\n        metric_g1 = data[data['group'] == 1][metric_name]\n        metric_g2 = data[data['group'] == 2][metric_name]\n\n        mu_1 = metric_g1.mean()\n        mu_2 = metric_g2.mean()\n\n        t_stat, p_value = stats.ttest_ind(metric_g1, metric_g2)\n\n        p_vals_t_test.append(p_value)\n\n    # perform k-s test\n    ks_stat,p_val_ks_test = stats.kstest(p_vals_t_test,stats.uniform.cdf)    \n    return p_vals_t_test,p_val_ks_test\n</code></pre> <pre><code>p_value,ks_p_val = simulate_aa_test(n_iter=1000,metric_name='revenue_per_user',num_samples=num_samples) \n</code></pre> <pre><code>sns.histplot(p_value,alpha = 1, bins = 30,kde=True,\n            label = r\"p-value\",color='blue')\n</code></pre> <pre><code>&lt;Axes: ylabel='Count'&gt;\n</code></pre> <p></p> <pre><code>ks_p_val\n</code></pre> <pre><code>0.7919638330879013\n</code></pre> <p>The simulated A/A test is from the same distribution (uniform distribution) </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#4-analyzing-data","title":"4. Analyzing Data","text":"<p>Let's Recap our Experimentation Plan </p> <p>Metrics of Experimentation : Revenue per User </p> <p>Experiment Unit : User in Checkout Process </p> <p>Variant : </p> <pre><code>1. Control Group : No Product Recommendation during checkout process \n2. Treatment Group : Product Recommendation during checkout process y\n</code></pre> <p>Number of Samples : 1570 from each variant </p> <p>Experiment Duration : Experiment was conducted for a week , starting from February 2<sup>nd</sup>, 2024 to February 8<sup>th</sup>, 2024</p> <p>Our Hypothesis : by adding product recommendation could boost revenue per user</p> <p>So what do we do when it comes after experiment. We obtain data from experiment log from both of each group. However as Data Scientist we should be sceptical hence we need to perform Sanity Check </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#41-data-check","title":"4.1. Data Check","text":"<pre><code>experiment_data = pd.read_csv('experiment_collected_data.csv',parse_dates=['datetime'])\nexperiment_data\n</code></pre> datetime user_id group revenue_per_user 0 2024-02-02 d5683160-da8e-4027-80eb-f1543a6203e5 control 16.433941 1 2024-02-03 c829d253-d79e-42b8-9fe0-f2f69bd9d7d0 treatment 2.274594 2 2024-02-03 4307a4d6-1125-4191-8aa2-a473c5f46c5c control 6.293043 3 2024-02-07 f23f605a-7a25-45bc-b4bf-3c9ddb4dfacb control 29.030031 4 2024-02-06 fb404923-91df-4c01-956c-3ede39361914 control 2.196385 ... ... ... ... ... 3135 2024-02-04 e56ea29f-a89f-46d0-a81d-1ad6759f26f3 treatment 13.885899 3136 2024-02-04 2597b6c8-71e2-4acc-87a0-8d775e7bf83f treatment 6.362146 3137 2024-02-02 5e758638-e6e2-49c8-81ab-187156b59f97 control 3.460954 3138 2024-02-03 310130df-086e-4c5e-911c-1401979195a5 control 1.898356 3139 2024-02-06 532dc215-a179-4874-9e8b-89d141fe10c9 control 2.944030 <p>3140 rows \u00d7 4 columns</p> <p>Check number of unique user from each group</p> <pre><code>experiment_data.groupby('group')['user_id'].nunique()\n</code></pre> <pre><code>group\ncontrol      1570\ntreatment    1570\nName: user_id, dtype: int64\n</code></pre> <p>Number of Unique User is like what we expect</p> <p>Next, we should check the experiment datetime </p> <pre><code>filter_1 = experiment_data['datetime'] &lt;= '2024-08-02'\nfilter_2 = experiment_data['datetime'] &gt;= '2024-02-02'\n\nexperiment_data.loc[~(filter_1&amp;filter_2)]\n</code></pre> datetime user_id group revenue_per_user <p>Our data is valid in terms of datetime as experiment plan</p> <p>Next, we should check if there is duplicate record from user assignment </p> <pre><code>experiment_data.duplicated(subset=['user_id','group']).sum()\n</code></pre> <pre><code>0\n</code></pre> <p>It's safe from duplicate user assignment</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#411-checking-guadrail-metrics","title":"4.1.1. Checking Guadrail Metrics","text":"<p>In real experiment platform, we should check , what we called as Guadrail Metrics, which should not be different. Otherwise it will affect the experiment outcome become bias, for example latency</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#42-aggregating-data","title":"4.2. Aggregating Data","text":"<p>Next step after we have clean &amp; valid data, we should aggregate data to reflect metrics from both group </p> <pre><code>summarize_data = experiment_data.groupby('group').agg({'revenue_per_user':np.mean})\nsummarize_data\n</code></pre> <pre><code>C:\\Users\\fakhr\\AppData\\Local\\Temp\\ipykernel_21012\\1678331338.py:1: FutureWarning:\n\nThe provided callable &lt;function mean at 0x000001AD8D28E480&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n</code></pre> revenue_per_user group control 8.949105 treatment 9.852804 <pre><code>summarize_data['lift'] = summarize_data['revenue_per_user'].diff(1)\nsummarize_data\n</code></pre> revenue_per_user lift group control 8.949105 NaN treatment 9.852804 0.903699"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#43-performing-statistical-test","title":"4.3. Performing Statistical Test","text":"<p>Condition :  We are interested whether by providing product recommendation during checkout will increase revenue per user. </p> <p>Why we need to perform statistical test ? We can just compare the difference between mean Yes! at some point it is right, however we only have small sample, and trying to estimate the population, hence what we can use to leverage the statistical test to add confidence in our result </p> <p>Type of Statistical Test, we want to compare the revenue, in terms of mean difference between two groups (independent sample), we can leverage T-student Test for mean from two sample.  Since we are going to check its bigger or not, its one tailed test</p> <p>Hypothesis : </p> <p>\\(H_0 : \\mu_{\\text{treatment}} &lt;= \\mu_{\\text{control}}\\)</p> <p>\\(H_1 : \\mu_{\\text{treatment}} &gt; \\mu_{\\text{control}}\\)</p> <p>At experiment plan we want Confidence Interval of 95% or \\(\\alpha=0.05\\)</p> <p>Assumption : we don;t know variance about population, and both variance assumed to be equal </p> <p>How to conclude ?  1. If p-value &gt; \\(\\alpha\\) fail to reject the null Hypothesis, otherwise reject the null hypothesis 2. equivalent to if t-test value &lt; t-table fail to reject the null Hypothesis </p> <pre><code>control_group = experiment_data.loc[experiment_data['group']=='control','revenue_per_user'].values\ntreatment_group = experiment_data.loc[experiment_data['group']=='treatment','revenue_per_user'].values\n</code></pre> <pre><code>t_stat,p_val_t_test = stats.ttest_ind(treatment_group,control_group, equal_var=True, random_state=45, alternative='greater')\n</code></pre> <pre><code>p_val_t_test\n</code></pre> <pre><code>1.5326415297769846e-05\n</code></pre> <p>Proven that we can reject null hypothesis</p> <pre><code># Parameters\nn_samples = 2*1570\nalpha = 0.05\ndf = n_samples-2\ncritical_value = stats.t.ppf(1 - alpha, df)  # one tailed \n\n# t-distribution\nx = np.linspace(-4, 4, 1000) # generate some data \ny = stats.t.pdf(x, df)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label=f't-distribution with {df} degrees of freedom')\nplt.fill_between(x, y, where= (x &gt; critical_value), color='red', alpha=0.5, label='Rejection Region')\n\n# Add critical values\nplt.axvline(critical_value, color='black', linestyle='--', label=f'Critical Value: {critical_value:.3f}')\nplt.axvline(t_stat, color='green', linestyle='--', label=f'T statistic value')\n\n# Labels and title\nplt.title('Two-tailed t-test Rejection Region')\nplt.xlabel('t-value')\nplt.ylabel('pdf')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>Clearly we   reject the null hypothesis. Next Step we can generate confidence interval for our treatment effect using standard error </p> \\[ (\\bar{x}_{\\text{treatment}} - \\bar{x}_{\\text{control}} )  -  t_{\\alpha/2,df} \\cdot SE \\leq \\mu_{\\text{treatment}}- \\mu_{\\text{treatment}} \\leq (\\bar{x}_{\\text{treatment}} - \\bar{x}_{\\text{control}} )  +  t_{\\alpha/2,df} \\cdot SE\\] \\[SE = \\sqrt{(\\cfrac{s_1^2}{n_1} + \\cfrac{s_2^2}{n_2}})  \\] <p>with : </p> <ol> <li>\\(SE\\) : Standard error</li> <li>\\(s_1^2\\) : variance from group 1 , control group</li> <li>\\(s_2^2\\) : variance from group 2 , treatment group</li> <li>\\(n_1\\) : number of sample from group 1 , control group</li> <li>\\(n_2\\) : number of sample from group 2 , treatment group</li> <li>\\(t_{\\alpha/2 , df}\\) : t table value given alpha and degrees of freedom</li> </ol> <pre><code>difference = treatment_group.mean() - control_group.mean() \nn_1 = 1570 \nn_2 = 1570\ns_1 = control_group.var()\ns_2 = treatment_group.var()\n\nse = np.sqrt((s_1/n_1) + (s_2/n_2))\n\nt_alpha = stats.t.ppf(alpha/2, df=n_1+n_2-2)\n\n\nc_i = t_alpha*se\n\n\n\nprint(f'Effect Confidence Interval {difference} +/-{c_i}')\n</code></pre> <pre><code>Effect Confidence Interval 0.903699066036344 +/--0.4243055658869349\n</code></pre> <pre><code>c_i\n</code></pre> <pre><code>-0.4243055658869349\n</code></pre> <pre><code>difference\n</code></pre> <pre><code>0.903699066036344\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nplt.figure(figsize=(8, 6))\nplt.errorbar(y=1, x=difference, xerr=-c_i, fmt='o', ecolor='black', capsize=5, capthick=2, elinewidth=1, label='Revenue per User + CI')\nplt.vlines(x = 0, color = 'red',ymin = 0, ymax = 2, label = 'Statistically not Significant',linestyle='--')\nplt.vlines(x = 1, color = 'green',ymin = 0, ymax = 2, label = 'Practically Not Significant',linestyle='--')\n\nplt.title('Comparing Practically and Statistically Significant')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()\n</code></pre> <p></p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#5-inferring-decision","title":"5. Inferring Decision","text":"<p>We already reach the end of the experimentation. We have to decided whether to launch product recommendation system during checkout process or not to all users. </p> <p>To rollout this feature , the result should be :  1. Statistically Significant 2. Practically Significant</p> <p>From the Result it looks like only Statistical Significance . However the revenue per user still not fullfill our minimum USD 1 Revenue per user increase, so we have to decide not to launch the product.</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/","title":"Targeting problem","text":"<p> Photo by engin akyurt on Unsplash</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#giving-offer-to-right-user","title":"Giving Offer to Right User","text":""},{"location":"003-making%20your%20customer%20stay/targeting_problem/#business-context","title":"Business Context","text":"<p>In a ride-hailing company , keeping both customer and driver active is crucial important, because both form equilibrium. </p> <ol> <li>Lack of Driver Availability lead to scarcity and ride service cannot be provided, also can lead to extreme price, due to dynamic pricing mechanism </li> <li>Lack of Customer of course hurt a lot, driver, company, etc. </li> </ol> <p>To balance those several ways can be approached, for example by giving a promo or coupon </p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#business-metrics","title":"Business Metrics","text":"<p>What is the suitable metrics for this? To help answer the question, we should answer  : </p> <ol> <li>What is the goal of promo ? One of the sign that user is active can be derive from Gross Booking </li> </ol> <p>Here is the relation with Revenue </p> <p>$$</p> <p>\\begin{align} \\text{Revenue from Service} &amp;= \\text{Average Revenue Per User}  \\cdot  \\text{Number of Active User} \\ \\text{Revenue from Service} &amp;= \\text{Average Gross Booking Per User } \\cdot \\text{Take Rate} \\cdot  (1-P(\\text{Churn Rate})) \\cdot  \\text{Total User} \\</p> <p>\\text{Revenue from Service} &amp;= \\cfrac{\\text{ Gross Booking  }}{\\text{Number of User }} \\cdot \\text{Take Rate} \\cdot  (1-P(\\text{Churn Rate})) \\cdot\\text{Total User} \\  \\text{Revenue from Service} &amp;= \\cfrac{\\text{ Gross Booking  }}{((1-P(\\text{Churn Rate})) \\cdot\\text{Total User})} \\cdot \\text{Take Rate} \\cdot  (1-P(\\text{Churn Rate})) \\cdot\\text{Total User} \\  \\end{align}</p> <p>$$</p> <ul> <li>Take Rate : Similar to Net Profit Margin </li> </ul>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#business-process","title":"Business Process","text":"<p>To better understand the problem, we should understand the business process first  </p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#problems","title":"Problems","text":"<p>The problem is we have limited budget constraint, let say called it B we cannot surpass. How can we maximize the benefit from giving promo and still within the constraint</p> <p>$$  </p> <p>$$</p> <p>$$  </p> <p>$$</p> <ol> <li>GB :  Gross Booking </li> <li>w_i : Binary 0/1 indicating whether user i receive promo or not</li> <li>IGB_i : Incremental Gross Booking from promo </li> <li>cost_i : Cost of Promo (Homogen) from user i</li> </ol> <p>So, how do we approach this problem ? </p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#candidate-solutions","title":"Candidate Solutions","text":"<p>There are several ways to approach a problem , we can do some literature review, on how industry approach this </p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#solution-1-using-machine-learning","title":"Solution 1 : Using Machine Learning","text":"<p>The first approach we can predict the outcome variable given treatment assign or not using any machine learning model. </p> <p>Data Looks : </p> userID X_1 .. X_n Treatment Outcome (GB) 1 <p>Pros : </p> <p>Easy to do just like machine learning case with regression or classification</p> <p>Cons : </p> <p>We cannot adding incrementality / the effect of treatment, hence it's biased to user which Outcome is the highest (for example user who are member )</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#solution-2-enter-causality","title":"Solution 2 : Enter Causality","text":"<p>Instead of just using machine learning to predict outcome variable, we can leverage causality to measure the incrementality of each user if user receive treatment (promo)</p> <p>Pros : </p> <p>Account for causality / incrementality factor</p> <p>Cons : </p> <p>Each user only can be observed one potential outcome, receiving treatment or not. Don't worry we still can estimate the counterfactual. </p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#choosen-solution","title":"Choosen Solution :","text":"<p>Considering our need to measure incrementality we choose the Solution 2. </p> <p>Step by Step Solution : </p> <p>1. Running Experiment</p> <p>First we need to design experiment by giving randomly coupon to user assign  , however we should be aware that people who offered coupon can either accept or do not accept their coupon </p> <p><pre><code>graph LR\n    A(Coupon Assignment) --&gt;B(Coupon Claims)\n    B --&gt; C(Gross Booking)\n    D(Number of Bookings) --&gt; B\n    E(Income) --&gt; D\n    F(Membership) --&gt; B\n    E --&gt; F\n    F --&gt; C \n    D --&gt; C\n</code></pre> The variable we can observe :  1. Number of Bookings  2. Membership  3. Coupon Assignment </p> <p>Planning Experiment</p> <p>The truth is this is that the assignment is random and the Coupon Claims is not random hence, this is suitable for Causal Inference for Observational Data</p> <ul> <li>Experiment Duration : 1 Week only </li> <li>Randomization Unit : User Level </li> <li>Chance of getting Treatment : 30% </li> <li>Which User is candidate : User who still active in past week </li> <li>Coupon Budget : $1 </li> <li>Number of Users Active in Past Week : 2000</li> <li>Hence, the cost of experiment : 2000 x $ 1 = $2000</li> </ul> <p>2. Estimating Treatment Effect </p> <p>After we have obtained the experiment data we can train model to estimate the treatment effect </p> <p>3. Choosing Whose to be Treated </p> <p>After we have estimated the treatment effect on individual level, we can observe that treating all user is not good way, hence we need to choose what fraction of user to be treated</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#introduction-to-causal-inference","title":"Introduction to Causal Inference","text":"<p>Sometimes we are interested in question such as \"Does Implementing Minimum 9 Years Education impact GDP\" and so on. That is study of Causal Inference </p> <p>The Gold Standard of Measuring Causality is by Randomized Control Trial / Experiment, however sometimes treatment assignment imposible to be random or it's unethical. </p> <p>There is another method , called Causal Inference with Observational Data. </p> <p>Before diving more, we often heard , Correlation Does not Imply Causation</p> <p>But How do we can claim causality, here is several condition  to fullfil causality</p> <ol> <li>Temporal Sequencing </li> </ol> <p>If we have treatment (T) and Outcome Variable (Y). It means we do some treatment first then we observe the outcome variable. For example we want to measure the effect of rising price of commodity we want to measure the Demand. </p> <pre><code>   graph LR\n      A(Commodity Price) --&gt;B(Demand)\n</code></pre> <ol> <li>Non Spurious Relationship</li> </ol> <p>One meme about this </p> <p></p> <ol> <li>Removing alternate Causes (Confounding)</li> </ol> <p>Previously we want to measure the effect of rising price of commodity on demand, however there are many factors affecting demand,such as seasonality </p> <pre><code>   graph LR\n      A(Commodity Price) --&gt;B(Demand)\n      C(Seasonality Confounding) --&gt; B\n</code></pre> <p>How to remove this bias ? </p> <ol> <li>We could use randomization / random experiment such as A/B Test </li> <li>Controlling the Confounder for example by adding the confounding variable to model </li> </ol> <p>Potential Outcome</p> <p>The one who developed causality theory is Donald B Rubin, or called as Rubin Causality framework of potential outcome framework. </p> <p>So what is potential outcome, let say we have  a headache, how can we cure the headache, for example : we can take medicine. </p> <p>The outcome or result after taking medicine </p> <pre><code>   graph LR\n      A[Headache] --&gt;B[Take Pills]\n      B --&gt; C[Headache Gone]\n      B --&gt; D[Still Headache]\n\n      A --&gt; E[Doesnot Take Pills]\n      E --&gt; F[Headache Gone]\n      E --&gt; G[Still Headache]\n</code></pre> <p>Taking Pills or Not Taking Pills is a form of treatment say we denote as T , each person only can take one treatment at a time (take pills or not ) and only observe one outcome say we denote Y (still headache or not)</p> <p>those outcome, called potential outcome. </p> <p>Hence if we want to denote the outcome of each person, say \\(i\\) after taking pills is \\(Y_{1i}\\) </p> <p>The Treatment effect on individuals can be define as </p> <p>\\(\\text{Individual Treatment Effect}_i = Y_{1i} - Y_{0i}\\)</p> <p>However, again we can observe both at the same time then usually we come up with Average Treatment Effect </p> <p>which can be define as </p> <p>\\(\\text{Average Treatment Effect} = \\text{Average Outcome on Treated individual} - \\text{Average Outcome on Not Treated individual}\\)</p> <p>or </p> <p>\\(\\text{Average Treatment Effect} = E[Y_1 - Y_0 ]\\)</p> <p>with \\(E\\) as expected value or simply average </p> <p>Individual Treatment Effect </p> <p>You said that estimating treatment effect is quite impossible, how could you over with this? Don't worry we'll talk about industry implementation</p> <p>So why would you use individual treatment effects instead of Average Treatment Effect. </p> <p>Typical use of Average Treatment Effect :  - Program Evaluation  - Research  - Decision </p> <p>The reason about using Individual Treatment Effect is Personalization , why ? </p> <p>Individuals may react differently in different degree , for example in ride hailing case : </p> <ul> <li>We give discount offer to individual A \u2192 one week after the discount the user keep using  / ordering to ride hailing apps </li> <li>But that could be different in user Z \u2192 offered discount but not reacting at all </li> </ul> <p>Again what's the company interest ? Gaining Maximum Shareholder value , How ?  - Could from rising the revenue  - Or from reducing the cost </p> <p>In case we choose the revenue, how could we give treatment such as offer so that we could maximize the revenue given budget constraint : </p> <ul> <li>We select individual if we treat \u2192 gain highest treatment effect / incremental value </li> </ul> <p>\\(\\text{Individual Treatment Effect}_i = Y_{1i} - Y_{0i}\\)</p> <p>We only have one outcome at a time, hence we need to estimate the other one using model </p> <p>Let's dive in </p> <p>Modelling Workflow </p> <pre><code>   graph LR\n      A[Data Preparation] --&gt;B[Exploratory Data Analysis]\n      B --&gt; C[Data Preprocessing]\n      C --&gt; D[Modelling Phase]\n      D --&gt; E[Finding Best Model]\n</code></pre>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#data-preparation","title":"Data Preparation","text":"<pre><code>import pandas as pd \nimport numpy as np \nimport scipy.stats as st \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nfrom IPython.display import display\n\nimport warnings\n\n# Suppress all warnings\nwarnings.filterwarnings('ignore')\n</code></pre> <p>Here is the dataset definition</p> Variable Name Definition Data Type PromoID ID of Promos INT PromoType Type of Promos Offered  to Treatment Effect String PromoDate Date of Promos Given to Treated User String UserID UserID String Treatment Binary Value (0/1) indicating user receive treatment or not String GrossBooking Outcome Variable , Gross Booking in 7 Days String Age Customer Age String Gender Customer Gender String NumberOfBooking Count of user Booking in Last 7 Days String ResponseToPastPromo Binary Value (0/1) indicating whether user responded / claim last offered promo String LastBooking(Days) Number of Days  from last booking, given experiment date cutoff String isMember Binary Value (0/1) indicating whether user is a member String average_apps_open_week Number of user app open in 7 days String <pre><code>data = pd.read_csv('dataset/promo_continous_outcome.csv')\n</code></pre> <pre><code>data\n</code></pre> PromoID PromoType PromoDate UserID Treatment GrossBooking Age Gender NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week 0 1 NaN NaN 1 0 98.346167 56 male 16 1 148 0 30.940793 1 1 NaN NaN 2 0 99.150442 69 male 49 1 339 0 32.466263 2 1 NaN NaN 3 0 93.619220 46 male 39 0 115 0 30.170032 3 1 NaN NaN 4 0 87.382590 32 female 44 0 213 0 31.393513 4 1 NaN NaN 5 0 100.478985 60 male 31 1 293 0 30.839952 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2995 3 NaN 2024-01-07 996 1 107.387370 60 female 17 0 87 0 30.010098 2996 3 NaN NaN 997 0 97.674515 64 male 36 0 302 0 30.216715 2997 3 NaN NaN 998 0 90.007692 62 female 44 1 167 0 30.675957 2998 3 NaN NaN 999 0 108.604006 35 male 31 0 169 0 33.005855 2999 3 NaN NaN 1000 0 94.680165 55 male 35 0 134 0 30.483874 <p>3000 rows \u00d7 13 columns</p> <pre><code>input_col = ['Age', 'Gender', 'NumberOfBooking','ResponseToPastPromo','LastBooking(Days)', \n            'isMember','average_apps_open_week']\ntreatment_col = ['Treatment']\noutput_col = ['GrossBooking']\n</code></pre> <pre><code>X = data[input_col + treatment_col]\ny = data[output_col]\n</code></pre> <pre><code>X.head()\n</code></pre> Age Gender NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week Treatment 0 56 male 16 1 148 0 30.940793 0 1 69 male 49 1 339 0 32.466263 0 2 46 male 39 0 115 0 30.170032 0 3 32 female 44 0 213 0 31.393513 0 4 60 male 31 1 293 0 30.839952 0 <pre><code>y.head()\n</code></pre> GrossBooking 0 98.346167 1 99.150442 2 93.619220 3 87.382590 4 100.478985 <p>Split data into training and testing </p> <pre><code>from sklearn.model_selection import train_test_split\n\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nX_val,X_test,y_val,y_test = train_test_split(X_test,y_test,test_size=0.5)\n</code></pre> <pre><code>X_train\n</code></pre> Age Gender NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week Treatment 970 53 male 26 0 45 0 30.041950 0 159 66 male 23 1 258 0 31.328888 0 2513 31 female 4 1 103 0 30.189209 0 1604 54 male 43 0 170 0 30.413598 0 436 33 female 1 1 224 1 30.281640 0 ... ... ... ... ... ... ... ... ... 826 23 female 15 0 178 0 30.325069 0 2839 36 male 1 1 338 1 30.525431 0 1112 34 male 31 1 25 1 30.312170 0 130 41 male 31 0 162 0 30.799777 0 744 54 male 45 0 16 0 30.807659 0 <p>2400 rows \u00d7 8 columns</p> <pre><code>y_train\n</code></pre> GrossBooking 970 101.384184 159 103.983185 2513 96.753668 1604 97.215699 436 103.875343 ... ... 826 90.591134 2839 101.575794 1112 101.633420 130 107.497184 744 102.238127 <p>2400 rows \u00d7 1 columns</p> <p>Data Preprocessing</p> <ul> <li>Encoding Categorical Variable</li> <li>Creating Feature : Interaction between Treatment and other covariates</li> </ul> <p>Encoding Categorical Variable</p> <pre><code>categorical_columns = ['Gender']\n</code></pre> <pre><code>from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse_output=False)\n\nencoder.fit(X_train[categorical_columns].values.reshape(-1,1))\n</code></pre> <pre>OneHotEncoder(sparse_output=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.OneHotEncoder<pre>OneHotEncoder(sparse_output=False)</pre> <pre><code>from IPython.display import display\n</code></pre> <pre><code>def encode_categorical(X,categorical_columns, train=True,encoder=None) : \n    X = X.copy()\n    X.index = [x for x in range(X.shape[0])]\n    if train : \n        encoder = OneHotEncoder(sparse_output=False)\n\n        encoder.fit(X[categorical_columns].values.reshape(-1,1))\n        feature_names = encoder.categories_[0]\n        encoded_values = encoder.transform(X[categorical_columns].values.reshape(-1,1))\n        encoded_df  = pd.DataFrame(encoded_values, columns=feature_names)\n\n\n        X.drop(categorical_columns,axis=1,inplace=True)\n\n        X_encoded = pd.concat([X,encoded_df],axis=1)\n\n        return X_encoded,encoder\n    else : \n        if encoder is None : \n            raise ValueError('you should add fitted encoder for other than training data')\n        feature_names = encoder.categories_[0]\n        encoded_values = encoder.transform(X[categorical_columns].values.reshape(-1,1))\n        encoded_df  = pd.DataFrame(encoded_values, columns=feature_names)\n\n        X.drop(categorical_columns,axis=1,inplace=True)\n\n        X_encoded = pd.concat([X,encoded_df],axis=1,join='inner')\n        return X_encoded,_\n</code></pre> <pre><code>X_train_encoded,encoder = encode_categorical(X=X_train,categorical_columns=categorical_columns,\n                                             train=True)\nX_test_encoded,_ = encode_categorical(X=X_test,categorical_columns=categorical_columns,\n                                             train=False,encoder=encoder)\nX_val_encoded,_ = encode_categorical(X=X_val,categorical_columns=categorical_columns,\n                                             train=False,encoder=encoder)\n</code></pre> <pre><code>X_train_encoded.shape\n</code></pre> <pre><code>(2400, 9)\n</code></pre> <pre><code>X_test_encoded.shape\n</code></pre> <pre><code>(300, 9)\n</code></pre> <pre><code>def generate_interaction(X,input_col,treatment_col) : \n    X = X.copy()\n    for col in input_col : \n        for t in treatment_col : \n            X[f'{col}_{t}_interaction'] = X[col] * X[t]\n    return X\n</code></pre> <pre><code>input_col = ['Age', 'female','male', 'NumberOfBooking','ResponseToPastPromo','LastBooking(Days)', \n            'isMember','average_apps_open_week']\ntreatment_col = ['Treatment']\noutput_col = ['GrossBooking']\n</code></pre> <pre><code>X_train_interaction = generate_interaction(X=X_train_encoded,input_col=input_col,treatment_col=treatment_col)\nX_test_interaction = generate_interaction(X=X_test_encoded,input_col=input_col,treatment_col=treatment_col)\nX_val_interaction = generate_interaction(X=X_val_encoded,input_col=input_col,treatment_col=treatment_col)\n</code></pre> <p>dropping column of non interaction column</p> <pre><code>X_train_interaction = X_train_interaction.drop(input_col,axis=1)\nX_test_interaction = X_test_interaction.drop(input_col,axis=1)\nX_val_interaction = X_val_interaction.drop(input_col,axis=1)\n</code></pre> <pre><code>X_train_interaction.shape\n</code></pre> <pre><code>(2400, 9)\n</code></pre> <pre><code>X_test_interaction.shape\n</code></pre> <pre><code>(300, 9)\n</code></pre> <pre><code>data_train = pd.concat([X_train_encoded.reset_index(drop=True),y_train.reset_index(drop=True)],axis=1,ignore_index=False)\ndata_test = pd.concat([X_test_encoded.reset_index(drop=True),y_test.reset_index(drop=True)],axis=1,ignore_index=False)\ndata_val = pd.concat([X_val_encoded.reset_index(drop=True),y_val.reset_index(drop=True)],axis=1,ignore_index=False)\n</code></pre> <p>hence  1. \\(X\\) or Covariate : <code>'GrossBookingValue', 'Age', 'Gender', 'NumberOfBooking','ResponseToPastPromo','LastBooking(Days)',              'isMember','average_apps_open_week'</code> 2. \\(T\\) or Treatment Variable  : <code>Treatment</code> 3. \\(Y\\) or Outcome Variable : <code>GrossBookingValue</code></p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#modelling-phase","title":"Modelling Phase","text":""},{"location":"003-making%20your%20customer%20stay/targeting_problem/#1st-linear-regression","title":"\\(1^{st}\\) Linear Regression","text":"<p>$$ \\begin{align} Y = \\sum^{K-1}{i=1} \\beta \\cdot X_i + \\beta_K \\cdot T \\</p> <p>\\dfrac{\\partial Y}{\\partial T} = \\beta_K \\ </p> <p>\\beta_K = \\textbf{ATE}</p> <p>\\end{align}</p> <p>$$</p> <p>It produces the ATE however what we need is is persnalized effect, hence we can add interaction term with covariate $$ \\begin{align} Y = \\sum^{K-1}{i=1} \\beta} \\cdot X_i + \\beta_K \\cdot T +  \\sum^{K-1{i=1} \\beta \\cdot X_i \\cdot T \\</p> <p>\\dfrac{\\partial Y}{\\partial T} = \\sum^{K-1}{i=1} \\beta \\cdot X_i  \\ </p> <p>\\sum^{K-1}{i=1} \\beta} \\cdot X_i = \\textbf{CATE</p> <p>\\end{align}</p> <p>$$</p> <p>However Linear Regression of course has limitation such as :  - Linearity Assumption </p> <p>Can we use any estimator that can produce non linearity ? </p> <p>Of Course : We could take a look at another estimator</p> <pre><code>from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code>model_cate_lr = LinearRegression()\n\nmodel_cate_lr.fit(X_train_interaction,y_train)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre> <pre><code>ate_prediction = model_cate_lr.coef_ @ X_train_interaction.to_numpy().T\n</code></pre> <pre><code>model_cate_lr.predict(X_train_interaction)\n</code></pre> <pre><code>array([[99.80225631],\n       [99.80225631],\n       [99.80225631],\n       ...,\n       [99.80225631],\n       [99.80225631],\n       [99.80225631]])\n</code></pre> <pre><code>def predict_cate_lr(model,X) : \n    ate_prediction = model.coef_ @ X.to_numpy().T\n    return ate_prediction\n</code></pre> <pre><code>cate = X_train_interaction.copy()\ncate['predicted_cate'] = predict_cate_lr(model=model_cate_lr,X=X_train_interaction).flatten()\ncate = cate.sort_values('predicted_cate',ascending=False)\ncate\n</code></pre> Treatment Age_Treatment_interaction female_Treatment_interaction male_Treatment_interaction NumberOfBooking_Treatment_interaction ResponseToPastPromo_Treatment_interaction LastBooking(Days)_Treatment_interaction isMember_Treatment_interaction average_apps_open_week_Treatment_interaction predicted_cate 1822 1 46 0.0 1.0 47 1 148 0 31.044854 202.942291 576 1 47 0.0 1.0 46 1 188 0 31.266090 202.225657 418 1 25 0.0 1.0 40 1 194 1 31.692579 200.104725 513 1 25 0.0 1.0 40 1 194 1 31.692579 200.104725 2039 1 59 0.0 1.0 43 1 59 0 31.798532 200.035397 ... ... ... ... ... ... ... ... ... ... ... 840 0 0 0.0 0.0 0 0 0 0 0.000000 0.000000 841 0 0 0.0 0.0 0 0 0 0 0.000000 0.000000 842 0 0 0.0 0.0 0 0 0 0 0.000000 0.000000 2328 1 47 1.0 0.0 3 0 260 0 30.093732 -0.619641 2323 1 48 1.0 0.0 1 0 272 0 30.298811 -4.556660 <p>2400 rows \u00d7 10 columns</p> <pre><code>fig,ax = plt.subplots(nrows=2,ncols=2)\nsns.kdeplot(cate['predicted_cate'].values,ax=ax[0][0],label='Linear Regression CATE')\nax[0][0].set_title('LinReg CATE')\nplt.tight_layout()\nfig\n</code></pre> <p></p> <p></p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#2nd-meta-learners-models","title":"\\(2^{nd}\\) Meta Learners Models","text":"<p>Previously, we learn about we can extract the interaction term and use it to extract CATE, however the relationship is linear , what if we can implement non linear model such as Tree Based Method </p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#meta-learners-models-s-learners","title":"Meta Learners Models : \\(S-Learners\\)","text":"<p>The approach is quite simple </p> <ol> <li> <p>Train any machine learning model to all data  <pre><code>   graph LR\n      A(Covariate) --&gt;B(Y)\n      C(Treatment Variable) --&gt; B\n</code></pre></p> </li> <li> <p>During prediction phase : </p> </li> <li>Predict \\(Y_1\\) by input  Covariate + <code>Treatment=1</code> </li> <li>Predict \\(Y_0\\) Covariate + <code>Treatment=0</code></li> <li>Measure difference of  \\(Y_1\\) and \\(Y_0\\), That is our CATE</li> </ol> <p>Let's use boosting model such as AdaBoost </p> <pre><code>X_train_encoded\n</code></pre> Age NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week Treatment female male 0 53 26 0 45 0 30.041950 0 0.0 1.0 1 66 23 1 258 0 31.328888 0 0.0 1.0 2 31 4 1 103 0 30.189209 0 1.0 0.0 3 54 43 0 170 0 30.413598 0 0.0 1.0 4 33 1 1 224 1 30.281640 0 1.0 0.0 ... ... ... ... ... ... ... ... ... ... 2395 23 15 0 178 0 30.325069 0 1.0 0.0 2396 36 1 1 338 1 30.525431 0 0.0 1.0 2397 34 31 1 25 1 30.312170 0 0.0 1.0 2398 41 31 0 162 0 30.799777 0 0.0 1.0 2399 54 45 0 16 0 30.807659 0 0.0 1.0 <p>2400 rows \u00d7 9 columns</p> <pre><code>from sklearn.ensemble import AdaBoostRegressor\n\nslearner = AdaBoostRegressor(n_estimators=500)\n\nslearner.fit(data_train.drop(['GrossBooking'],axis=1),\n             data_train['GrossBooking'])\n</code></pre> <pre>AdaBoostRegressor(n_estimators=500)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostRegressor<pre>AdaBoostRegressor(n_estimators=500)</pre> <p>Time to estimate the cate </p> <pre><code>y_1_slearner = slearner.predict(data_train.drop(['GrossBooking'],axis=1).assign(Treatment=1))\ny_0_slearner = slearner.predict(data_train.drop(['GrossBooking'],axis=1).assign(Treatment=0))\n\ncate_slearner = y_1_slearner - y_0_slearner\ncate_slearner\n</code></pre> <pre><code>array([110.78624859, 104.76189742,  66.05373313, ...,  70.98508093,\n       110.78624859, 179.04466601])\n</code></pre> <pre><code>def predict_cate_slearner(data,model=slearner) : \n    y_1_slearner = model.predict(data.drop(['GrossBooking'],axis=1).assign(Treatment=1))\n    y_0_slearner = model.predict(data.drop(['GrossBooking'],axis=1).assign(Treatment=0))\n\n    cate_slearner = y_1_slearner - y_0_slearner\n    return cate_slearner\n</code></pre> <pre><code>sns.kdeplot(cate_slearner,ax=ax[0][1],label='S learner CATE')\nax[0][1].set_title('S Learner CATE')\nplt.tight_layout()\nfig\n</code></pre> <p></p> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#meta-learners-models-t-learners","title":"Meta Learners Models : \\(T-Learners\\)","text":"<p>The difference from S Learner is that T Learner  separately fit model to estimate \\(Y\\) for both treatment and non treatment group </p> <ol> <li>Train each model to predict \\(Y\\) for both treatment and control group</li> </ol> <pre><code>tlearner_1 = AdaBoostRegressor(n_estimators=500)\ntlearner_0 = AdaBoostRegressor(n_estimators=500)\n\nX_treatment = (data_train.loc[data_train['Treatment']==1]\n               .drop(treatment_col,axis=1)\n               .drop(output_col,axis=1))\n\nX_control = (data_train.loc[data_train['Treatment']==0]\n               .drop(treatment_col,axis=1)\n               .drop(output_col,axis=1))\ndisplay(X_treatment)\ny_treatment = data_train.loc[X_treatment.index,output_col]\ny_control = data_train.loc[X_control.index,output_col]\n\n\n\ntlearner_1.fit(X_treatment,y_treatment)\ntlearner_0.fit(X_control,y_control)\n</code></pre> Age NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week female male 24 18 38 1 166 0 30.742873 0.0 1.0 25 29 7 0 269 1 30.109229 0.0 1.0 33 21 33 1 277 0 31.526687 0.0 1.0 56 59 14 0 283 0 30.395622 0.0 1.0 64 45 14 0 132 0 30.900356 0.0 1.0 ... ... ... ... ... ... ... ... ... 2328 47 3 0 260 0 30.093732 1.0 0.0 2329 27 17 0 352 0 31.272954 0.0 1.0 2337 45 6 0 245 0 30.311971 0.0 1.0 2349 48 3 0 98 0 33.666700 1.0 0.0 2385 28 36 0 292 1 30.105533 1.0 0.0 <p>225 rows \u00d7 8 columns</p> <pre>AdaBoostRegressor(n_estimators=500)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostRegressor<pre>AdaBoostRegressor(n_estimators=500)</pre> <pre><code>X_treatment\n</code></pre> Age NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week female male 24 18 38 1 166 0 30.742873 0.0 1.0 25 29 7 0 269 1 30.109229 0.0 1.0 33 21 33 1 277 0 31.526687 0.0 1.0 56 59 14 0 283 0 30.395622 0.0 1.0 64 45 14 0 132 0 30.900356 0.0 1.0 ... ... ... ... ... ... ... ... ... 2328 47 3 0 260 0 30.093732 1.0 0.0 2329 27 17 0 352 0 31.272954 0.0 1.0 2337 45 6 0 245 0 30.311971 0.0 1.0 2349 48 3 0 98 0 33.666700 1.0 0.0 2385 28 36 0 292 1 30.105533 1.0 0.0 <p>225 rows \u00d7 8 columns</p> <p>Estimate the CATE</p> <pre><code>\n</code></pre> <pre><code>tlearner_y1 = tlearner_1.predict(X_train_encoded.drop(treatment_col,axis=1))\ntlearner_y0 = tlearner_0.predict(X_train_encoded.drop(treatment_col,axis=1))\n\ncate_tlearner = tlearner_y1 - tlearner_y0\n</code></pre> <pre><code>def predict_cate_tlearner(data,model1=tlearner_1,model0=tlearner_0) : \n\n    tlearner_y1 = model1.predict(data.drop(['GrossBooking','Treatment'],axis=1))\n    tlearner_y0 = model0.predict(data.drop(['GrossBooking','Treatment'],axis=1))\n\n    cate_tlearner = tlearner_y1 - tlearner_y0\n    return cate_tlearner\n</code></pre> <pre><code>sns.kdeplot(cate_tlearner,ax=ax[1][0],label='T learner CATE')\nax[1][0].set_title('T Learner CATE')\nplt.tight_layout()\nfig\n</code></pre> <p></p> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#meta-learners-models-x-learners","title":"Meta Learners Models : \\(X-Learners\\)","text":"<p>Previously we see progression from s learner to t learner</p> <ol> <li>S - T Learner : Separating Model for Estimating Conterfactual depends on group </li> </ol> <p>However often the case that treatment assignment is not equal, leading to bad estimation of CATE. What should we do next ? </p> <p>Instead of Treating equally the treatment effect, we can utilize some weighting mechanism to weight CATE from treatment group and CATE from non treated group. That is the idea of X- learner </p> <p>Procedure :  1. Train separate model to estimate Y for both treatment and control group </p> <p>\\(Model 1_{\\text{Stage 1}} = E[Y|X,T=1]\\)</p> <p>\\(Model 0_{\\text{Stage 2}} = E[Y|X,T=0]\\) 2. For each group </p> <pre><code>- Estimate CATE from treatment group\n\n  $\\tau_1 = Y_{T=1} - \\text{Model 0}_{\\text{Stage 1}}$\n</code></pre> <ul> <li> <p>Estimate CATE from control group </p> <p>\\(\\tau_0 = \\text{Model 1}_{\\text{Stage 1}} - Y_{T=0}\\) </p> </li> <li> <p>Fit Model to Estimate </p> <p>$\\text{Model 0}_{\\text{Stage 2}} = E[\\tau_0|X,T=0] $</p> <p>$\\text{Model 1}_{\\text{Stage 2}} = E[\\tau_1|X,T=1] $</p> </li> <li> <p>Fit Propensity Score Model </p> <p>$\\text{Propensity Model} = E[T=1|X] $</p> </li> <li> <p>During Prediction     Given Input \\(X_i\\) </p> </li> </ul> <p>$\\tau_i = (E[T=1|X_i]) * E[\\tau_1|X,T=1] + (1 - (E[T=1|X_i])) * E[\\tau_0|X,T=0] $</p> <p>hence \\(\\tau_i\\) is weighted prediction , ensemble of ensemble, yeay!</p> <ol> <li>Train separate model to estimate Y for both treatment and control group </li> </ol> <p>\\(Model 1_{\\text{Stage 1}} = E[Y|X,T=1]\\)</p> <p>\\(Model 0_{\\text{Stage 2}} = E[Y|X,T=0]\\)</p> <pre><code>xlearner_t0_s1 = AdaBoostRegressor(n_estimators=500)\nxlearner_t1_s1 = AdaBoostRegressor(n_estimators=500)\n\n\nX_treatment = (data_train.loc[data_train['Treatment']==1]\n               .drop(treatment_col,axis=1)\n               .drop(output_col,axis=1))\n\nX_control = (data_train.loc[data_train['Treatment']==0]\n               .drop(treatment_col,axis=1)\n               .drop(output_col,axis=1))\n\n\ny_treatment = data_train.loc[X_treatment.index,output_col]\ny_control = data_train.loc[X_control.index,output_col]\n\n\n\nxlearner_t1_s1.fit(X_treatment,y_treatment)\nxlearner_t0_s1.fit(X_control,y_control)\n</code></pre> <pre>AdaBoostRegressor(n_estimators=500)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostRegressor<pre>AdaBoostRegressor(n_estimators=500)</pre> <ol> <li> <p>For each group </p> <ul> <li>Estimate CATE from treatment group </li> </ul> <p>\\(\\tau_1 = Y_{T=1} - \\text{Model 0}_{\\text{Stage 1}}\\)</p> </li> <li> <p>Estimate CATE from control group </p> <p>\\(\\tau_0 = \\text{Model 1}_{\\text{Stage 1}} - Y_{T=0}\\) </p> </li> </ol> <pre><code>def estimate_partial_cate(data,model0,model1,Y,T) :\n    data = data.copy()\n    col_names = [x for x in data.index if x not in [Y,T]]\n\n\n    if data['Treatment'] ==1 :\n\n        counterfactual =  model0.predict(data[col_names].values.reshape(1,-1))\n\n        partial_tau = data[Y] - counterfactual\n        return partial_tau.item()\n\n    elif data['Treatment'] ==0 :\n\n        counterfactual =  model1.predict(data[col_names].values.reshape(1,-1))\n\n        partial_tau = counterfactual - data[Y]\n\n        return partial_tau.item()\n</code></pre> <pre><code>tau = data_train.apply(estimate_partial_cate,model0=xlearner_t0_s1,\n                                     model1=xlearner_t1_s1,Y='GrossBooking',T='Treatment',axis=1)\n</code></pre> <ol> <li> <p>Fit Model to Estimate </p> <p>$\\text{Model 0}_{\\text{Stage 2}} = E[\\tau_0|X,T=0] $</p> <p>$\\text{Model 1}_{\\text{Stage 2}} = E[\\tau_1|X,T=1] $</p> </li> </ol> <pre><code>type(tau)\n</code></pre> <pre><code>pandas.core.series.Series\n</code></pre> <pre><code>xlearner_t1_s2 = AdaBoostRegressor(n_estimators=500)\nxlearner_t0_s2 = AdaBoostRegressor(n_estimators=500)\n\ncol_to_exclude = ['GrossBooking','Treatment']\n\nX_treatment = (data_train.loc[data_train['Treatment']==1]\n               .drop(col_to_exclude,axis=1))\n\nX_control = (data_train.loc[data_train['Treatment']==0]\n               .drop(col_to_exclude,axis=1))\n\n\ny_treatment = tau.loc[X_treatment.index]\ny_control = tau.loc[X_control.index] \n\n\nxlearner_t1_s2.fit(X_treatment,y_treatment)\nxlearner_t0_s2.fit(X_control,y_control)\n</code></pre> <pre>AdaBoostRegressor(n_estimators=500)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostRegressor<pre>AdaBoostRegressor(n_estimators=500)</pre> <ol> <li> <p>Fit Propensity Score Model </p> <p>$\\text{Propensity Model} = E[T=1|X] $</p> </li> </ol> <pre><code>from sklearn.linear_model import LogisticRegression\n\npropensity_model = LogisticRegression()\n\n\npropensity_model.fit(data_train.drop(['Treatment','GrossBooking'],axis=1),data_train['Treatment'])\n</code></pre> <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression<pre>LogisticRegression()</pre> <ol> <li>During Prediction     Given Input \\(X_i\\) </li> </ol> <p>$\\tau_i = (E[T=1|X_i]) * E[\\tau_1|X,T=1] + (1 - (E[T=1|X_i])) * E[\\tau_0|X,T=0] $</p> <p>hence \\(\\tau_i\\) is weighted prediction , ensemble of ensemble, yeay!</p> <pre><code>propensity_score = propensity_model.predict_proba(data_train.drop(['Treatment','GrossBooking'],axis=1))\n\nxlearner_final_cate = (xlearner_t1_s2.predict(data_train.drop(['Treatment','GrossBooking'],axis=1))*propensity_score[:,1]) \\\n                        + (xlearner_t0_s2.predict(data_train.drop(['Treatment','GrossBooking'],axis=1))*propensity_score[:,0])\n</code></pre> <pre><code>def predict_cate_xlearner(data,model1_s2=xlearner_t1_s2,model0_s2=xlearner_t0_s2,propensity_model=propensity_model) : \n    propensity_score = propensity_model.predict_proba(data.drop(['Treatment','GrossBooking'],axis=1))\n\n    xlearner_final_cate = (model1_s2.predict(data.drop(['Treatment','GrossBooking'],axis=1))*propensity_score[:,1]) \\\n                        + (model0_s2.predict(data.drop(['Treatment','GrossBooking'],axis=1))*propensity_score[:,0])\n    return xlearner_final_cate\n</code></pre> <pre><code>sns.kdeplot(xlearner_final_cate,ax=ax[1][1],label='X learner CATE')\nax[1][1].set_title('X Learner CATE')\nplt.tight_layout()\nfig\n</code></pre> <p></p> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#3rd-modelling-using-target-transformation","title":"\\(3^{rd}\\)  Modelling using target transformation","text":"<p>the problem with CATE is that we cannot observe label of true CATE, only the estimate of it, can we a bit hackish so that we could estimate CATE on each individual observation ? Yess (Forcefully  ) we can use Target Transformation </p> <p>Why we need target transformation ? It's because in common machine learning problem , obtaining target variables mean :  1. Easy for evaluation simply like Regression Metrics (MAE,MSE) 2. Optimization Solution (MSE Loss for example)</p> <p>In Matheus Facure Book we can perform target transformation </p> <p>by first taking a look whether the experiment control and group are equal 50% : 50%, if so then : </p> <p>\\(Y_i^* = 2Y_i*T_i - 2Y_i*(1-T_i)\\)</p> <p>\\(2Y_i*T_i\\) = Outcome on Treatment \\(Y_i^1\\)</p> <p>\\(2Y_i*(1-T_i)\\) = Outcome on Treatment \\(Y_i^0\\)</p> <p>to check its validity we can verify  by checking whether </p> <p>\\(E[Y_i^*|X_i=x] = \\tau(x)_i\\)</p> <p>$$  </p> <p>$$</p> <p>However in our condition we are not possible to do so ? It's because we don't have 50 : 50 proportion , we can use </p> <p>\\(Y_i^* = Y_i * \\cfrac{T-e(X_i)}{e(X_i)(1-e(X_i))}\\)</p> <p>with  $e(X_i) $ is propensity model given \\(X_i\\)</p> <pre><code>def transform_y_star(data,propensity_model=propensity_model,treatment_col='Treatment',outcome_col='GrossBooking') : \n\n    # predict propensity score \n    propensity_score = propensity_model.predict_proba(data.drop([treatment_col,outcome_col],axis=1))\n\n    y_star = data[outcome_col] * (data[treatment_col] - propensity_score[:,1]) / (propensity_score[:,0]*propensity_score[:,1])\n    return y_star\n</code></pre> <pre><code>Y_star_train = transform_y_star(data_train)\ndisplay(Y_star_train)\n</code></pre> <pre><code>0      -110.755648\n1      -113.294355\n2      -106.189810\n3      -106.571512\n4      -116.448215\n           ...    \n2395   -101.712382\n2396   -116.399907\n2397   -111.188511\n2398   -119.168422\n2399   -110.466606\nLength: 2400, dtype: float64\n</code></pre> <pre><code>Y_star_val = transform_y_star(data_val)\ndisplay(Y_star_val)\n</code></pre> <pre><code>0      -119.085197\n1      -116.263022\n2       -99.919909\n3      -109.527690\n4      -117.176332\n          ...     \n295    -114.648191\n296     -98.236654\n297    1400.780810\n298    -113.283261\n299    -114.136317\nLength: 300, dtype: float64\n</code></pre> <pre><code>Y_star_test = transform_y_star(data_test)\ndisplay(Y_star_test)\n</code></pre> <pre><code>0     -113.693352\n1     -117.231432\n2     -109.337136\n3     -106.225375\n4     -118.357761\n          ...    \n295   -111.603392\n296   -120.213009\n297   -110.320215\n298    667.131541\n299   -105.251826\nLength: 300, dtype: float64\n</code></pre> <p>After this you can train those using any machine learning model </p> <pre><code>from sklearn.ensemble import RandomForestRegressor\n\nmodel_target_transform = RandomForestRegressor()\n\nmodel_target_transform.fit(data_train.drop(['GrossBooking','Treatment'],axis=1),Y_star_train)\n</code></pre> <pre>RandomForestRegressor()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor<pre>RandomForestRegressor()</pre> <pre><code>y_val_pred = model_target_transform.predict(data_val.drop(['GrossBooking','Treatment'],axis=1))\n</code></pre> <pre><code>from sklearn.metrics import mean_squared_error \n\n\nmean_squared_error(y_true=Y_star_val,y_pred=y_val_pred)\n</code></pre> <pre><code>1079896.9001854945\n</code></pre>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#evaluating-model","title":"Evaluating Model","text":"<p>The   standard of machine learning evaluation : </p> <ol> <li>Split Data into 2 (Training, Test) / 3 (Training, Test, &amp; Validation )</li> <li>Train model on training data </li> <li>Evaluate on Test / Validation Data </li> </ol> <p>The problem is : We don't have label of CATE , above mentioned target transformation still can't reflect the true labels of CATE just like unknown function. </p> <p>In this survey paper highlight some strategies to evaluate uplift modelling </p> <ol> <li>Using Traditional Uplift Metrics</li> </ol> <p>Using Uplift Metrics in forms of bin / groups </p> <p>Steps :     - Predict the uplift or CATE in both treatment and control groups     - Compute the average for each decile in each group     - Calculate the difference in each decile group (gain)</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#gain","title":"Gain","text":"<ol> <li>Predict CATE in both groups </li> </ol> <pre><code>inference_train = data_train.copy()\ninference_test = data_test.copy()\n\n\n# slearner  \n# inference_train['cate_lr'] = predict_cate_lr(model=model_cate_lr,X=inference_train\\\n#                             .drop(['GrossBooking','Treatment'],axis=1))\n# inference_test['cate_lr'] = predict_cate_lr(model=model_cate_lr,X=inference_test\\\n#                             .drop(['GrossBooking','Treatment'],axis=1))\n\n\ninference_train['cate_slearner'] = predict_cate_slearner(data=data_train.copy())\ninference_test['cate_slearner'] = predict_cate_slearner(data=data_test.copy())\n</code></pre> <ol> <li>Compute Average CATE in each groups decile </li> </ol> <pre><code># get groups \ncate_decile_treatment = inference_train.query('Treatment==1').assign(bin=pd.qcut(x=(inference_train.query('Treatment==1').\\\n                        loc[:,'cate_slearner']),q=10,labels=False ))\\\n                        .groupby('bin').agg({'cate_slearner':'mean'})\n\ncate_decile_control = inference_train.query('Treatment==0').assign(bin=pd.qcut(x=(inference_train.query('Treatment==0').\\\n                        loc[:,'cate_slearner']),q=10,labels=False ))\\\n                        .groupby('bin').agg({'cate_slearner':'mean'})\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\nCell In[59], line 2\n      1 # get groups \n----&gt; 2 cate_decile_treatment = inference_train.query('Treatment==1').assign(bin=pd.qcut(x=(inference_train.query('Treatment==1').\\\n      3                         loc[:,'cate_slearner']),q=10,labels=False ))\\\n      4                         .groupby('bin').agg({'cate_slearner':'mean'})\n      6 cate_decile_control = inference_train.query('Treatment==0').assign(bin=pd.qcut(x=(inference_train.query('Treatment==0').\\\n      7                         loc[:,'cate_slearner']),q=10,labels=False ))\\\n      8                         .groupby('bin').agg({'cate_slearner':'mean'})\n\n\nFile c:\\Users\\fakhr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\tile.py:340, in qcut(x, q, labels, retbins, precision, duplicates)\n    336 quantiles = np.linspace(0, 1, q + 1) if is_integer(q) else q\n    338 bins = x_idx.to_series().dropna().quantile(quantiles)\n--&gt; 340 fac, bins = _bins_to_cuts(\n    341     x_idx,\n    342     Index(bins),\n    343     labels=labels,\n    344     precision=precision,\n    345     include_lowest=True,\n    346     duplicates=duplicates,\n    347 )\n    349 return _postprocess_for_cut(fac, bins, retbins, original)\n\n\nFile c:\\Users\\fakhr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\tile.py:443, in _bins_to_cuts(x_idx, bins, right, labels, precision, include_lowest, duplicates, ordered)\n    441 if len(unique_bins) &lt; len(bins) and len(bins) != 2:\n    442     if duplicates == \"raise\":\n--&gt; 443         raise ValueError(\n    444             f\"Bin edges must be unique: {repr(bins)}.\\n\"\n    445             f\"You can drop duplicate edges by setting the 'duplicates' kwarg\"\n    446         )\n    447     bins = unique_bins\n    449 side: Literal[\"left\", \"right\"] = \"left\" if right else \"right\"\n\n\nValueError: Bin edges must be unique: Index([28.364680498458796,   65.0463209597256,  66.05373312568801,\n        66.05373312568801,  69.18811402071225,  85.11635587044222,\n       110.36429339664477, 117.96849741970071, 122.24310318631733,\n        155.1510308498945, 191.09766778653574],\n      dtype='float64', name='cate_slearner').\nYou can drop duplicate edges by setting the 'duplicates' kwarg\n</code></pre> <pre><code>display(cate_decile_treatment)\ndisplay(cate_decile_control)\n</code></pre> cate_slearner bin 0 57.149450 1 65.328915 2 74.068734 3 87.726081 4 90.563524 5 95.141914 6 100.721393 7 107.287273 8 134.848450 9 164.555192 cate_slearner bin 0 53.206568 1 65.003058 2 67.604734 3 86.066899 4 91.947500 5 96.878758 6 103.224171 7 121.331323 8 142.836045 9 172.046919 <ol> <li>Measure the difference (gain) for CATE</li> </ol> <pre><code>gain_ = cate_decile_treatment - cate_decile_control\n</code></pre> <p>Visualize </p> <pre><code>plot_slearner = sns.barplot(x=[x for x in range(10)],y=gain_.values.flatten())\nplt.title('Gain of CATE by decile S Learner')\n\n# Add data labels on the bars\nfor p in plot_slearner.patches:\n    plot_slearner.annotate(format(p.get_height(), '.1f'), \n                (p.get_x() + p.get_width() / 2., p.get_height()), \n                ha = 'center', va = 'center', \n                xytext = (0, 9), \n                textcoords = 'offset points')\n\n# Show the plot\nplt.show()\n</code></pre> <p></p> <pre><code>def evalute_uplift_bin(prediction_result,cate_column,t_col,model_name) : \n    cate_decile_treatment = prediction_result.query(f'{t_col}==1').assign(bin=pd.qcut(x=(prediction_result.query(f'{t_col}==1').\\\n                        loc[:,cate_column]),q=10,labels=False ))\\\n                        .groupby('bin').agg({cate_column:'mean'})\n\n    cate_decile_control = prediction_result.query(f'{t_col}==0').assign(bin=pd.qcut(x=(prediction_result.query(f'{t_col}==0').\\\n                            loc[:,cate_column]),q=10,labels=False ))\\\n                            .groupby('bin').agg({cate_column:'mean'})  \n\n    gain_ = cate_decile_treatment - cate_decile_control\n\n\n    plot_slearner = sns.barplot(x=[x for x in range(10)],y=gain_.values.flatten())\n\n    plt.title(f'Gain of CATE by decile {model_name}')\n\n    # Add data labels on the bars\n    for p in plot_slearner.patches:\n        plot_slearner.annotate(format(p.get_height(), '.1f'), \n                    (p.get_x() + p.get_width() / 2., p.get_height()), \n                    ha = 'center', va = 'center', \n                    xytext = (0, 9), \n                    textcoords = 'offset points')\n\n    # Show the plot\n    plt.show()\n\n    return gain_\n</code></pre> <pre><code>evalute_uplift_bin(prediction_result=inference_test,\n                   cate_column='cate_slearner',\n                   t_col='Treatment',\n                   model_name='Slearner')\n</code></pre> <p></p> cate_slearner bin 0 -6.903539 1 -4.941637 2 -11.728246 3 -22.237055 4 -8.996468 5 -3.817163 6 -4.217261 7 -11.009020 8 5.488645 9 -4.999156 <p>Let's Compare the models</p> <pre><code>inference_train['cate_xlearner'] = predict_cate_xlearner(data=data_train)\ninference_test['cate_xlearner'] = predict_cate_xlearner(data=data_test)\n\n\ninference_train['cate_tlearner'] = predict_cate_tlearner(data=data_train)\ninference_test['cate_tlearner'] = predict_cate_tlearner(data=data_test)\n\n\ninference_train['cate_tlearner'] = predict_cate_tlearner(data=data_train)\ninference_test['cate_tlearner'] = predict_cate_tlearner(data=data_test)\n</code></pre> <pre><code>_ = evalute_uplift_bin(prediction_result=inference_train,\n                   cate_column='cate_xlearner',\n                   t_col='Treatment',\n                   model_name='Xlearner Training')\n\n_ = evalute_uplift_bin(prediction_result=inference_train,\n                   cate_column='cate_tlearner',\n                   t_col='Treatment',\n                   model_name='Tlearner Training')\n</code></pre> <p></p> <p></p> <p>We can also craft it in cumulative way </p> <p>If we can see it's hard to compare both models, the good models should can we use some sort of direct comparative that is similar to AUC in common machine learning method, yes we can </p> <p>Uplift Curve </p> <p>\\(f(t) = \\left (    \\cfrac{Y_t^T}{N_t^T} - \\cfrac{Y_t^C}{N_t^C}    \\right ) \\left (  N_t^T + N_t^C\\right )\\)</p> <p>with :  1. \\(Y_t^T\\) : Cumulative Outcome Variable for Treatment Group up to \\(t^{th}\\) observation  2. \\(Y_t^C\\) : Cumulative Outcome Variable for Control Group up to \\(t^{th}\\) observation 3. \\(N_t^T\\) : Number of Observation for Treatment Group up to \\(t^{th}\\) observation 4. \\(N_t^C\\) : Number of Observation for Control Group up to \\(t^{th}\\) observation</p> <p>Essence : If we have good causal model then it will sort the same as the real outcome data  Procedure  1. Sort the data based on <code>uplift</code> or <code>cate columns</code> 2. Create Running Cumulation</p> <pre><code>def calculate_uplift_curve(data,treatment_col,outcome_col,uplift_col,min_periods=30, steps=100,normalize=True) : \n    size = data.shape[0]\n    data = data.copy()\n\n    # sort based on uplift_col\n    data = data.sort_values(uplift_col,ascending=False)\n    encoded_treatment = pd.get_dummies(data[treatment_col]).astype(int)\n    encoded_treatment.columns = [f'{treatment_col}_0',f'{treatment_col}_1']\n    data = data.drop(treatment_col,axis=1)\n    data = pd.concat([data,encoded_treatment],axis=1)\n\n    # multiply f'{treatment_col}_0',f'{treatment_col}_1' to outcome column \n    data[f'{outcome_col}_{treatment_col}_0'] = data[f'{treatment_col}_0'] * data[outcome_col] # our $Y_t^T$\n    data[f'{outcome_col}_{treatment_col}_1'] = data[f'{treatment_col}_1'] * data[outcome_col] # our $Y_t^C$\n\n    # calculate cumsum \n    data['Y_T'] = data[f'{outcome_col}_{treatment_col}_1'].cumsum()\n    data['Y_C'] = data[f'{outcome_col}_{treatment_col}_0'].cumsum()\n\n\n    data['N_T'] = data[f'{treatment_col}_1'].cumsum()\n    data['N_C'] = data[f'{treatment_col}_0'].cumsum()\n\n\n\n    # calculate f_t \n\n    # uplift =  ( ( (data['Y_T']/data['N_T']) \\\n    #                     - (data['Y_C']/data['N_C']) ) * (data['N_T'] + data['N_C']) ).values\n    # if normalize : \n    #     max_value = uplift.max()\n    #     uplift = uplift/max_value\n\n    # instead of calculating uplift for each row / observation calculate at each step \n    # inspired from https://matheusfacure.github.io/python-causality-handbook/19-Evaluating-Causal-Models.html#references\n    n_rows = list(range(min_periods, size, size // steps)) + [size] \n    n_rows = [x-1 for x in n_rows]\n    print('n rows',n_rows)\n    cumulative_uplifts = np.zeros(len(n_rows))\n    for idx,rows in enumerate(n_rows) : \n        data_at = data.iloc[rows,:]\n\n        uplift =   ( (data_at['Y_T']/data_at['N_T']) \\\n                                - (data_at['Y_C']/data_at['N_C']) ) * (data_at['N_T'] + data_at['N_C']) \n\n        cumulative_uplifts[idx] = uplift\n\n\n    pct = [rows/size for rows in n_rows]\n\n    if normalize : \n        # normalizing the uplift into 0 to 1 scale so that comparable, \n        max_value = cumulative_uplifts.max()\n        cumulative_uplifts = cumulative_uplifts/max_value\n\n\n    return pct, cumulative_uplifts\n</code></pre> <pre><code>pct,slearner_gain = calculate_uplift_curve(data=inference_train,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_slearner',normalize=True)\npct,xlearner_gain = calculate_uplift_curve(data=inference_train,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_xlearner',normalize=True)\npct,tlearner_gain =   calculate_uplift_curve(data=inference_train,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_tlearner',normalize=True)\n</code></pre> <pre><code>n rows [29, 53, 77, 101, 125, 149, 173, 197, 221, 245, 269, 293, 317, 341, 365, 389, 413, 437, 461, 485, 509, 533, 557, 581, 605, 629, 653, 677, 701, 725, 749, 773, 797, 821, 845, 869, 893, 917, 941, 965, 989, 1013, 1037, 1061, 1085, 1109, 1133, 1157, 1181, 1205, 1229, 1253, 1277, 1301, 1325, 1349, 1373, 1397, 1421, 1445, 1469, 1493, 1517, 1541, 1565, 1589, 1613, 1637, 1661, 1685, 1709, 1733, 1757, 1781, 1805, 1829, 1853, 1877, 1901, 1925, 1949, 1973, 1997, 2021, 2045, 2069, 2093, 2117, 2141, 2165, 2189, 2213, 2237, 2261, 2285, 2309, 2333, 2357, 2381, 2399]\nn rows [29, 53, 77, 101, 125, 149, 173, 197, 221, 245, 269, 293, 317, 341, 365, 389, 413, 437, 461, 485, 509, 533, 557, 581, 605, 629, 653, 677, 701, 725, 749, 773, 797, 821, 845, 869, 893, 917, 941, 965, 989, 1013, 1037, 1061, 1085, 1109, 1133, 1157, 1181, 1205, 1229, 1253, 1277, 1301, 1325, 1349, 1373, 1397, 1421, 1445, 1469, 1493, 1517, 1541, 1565, 1589, 1613, 1637, 1661, 1685, 1709, 1733, 1757, 1781, 1805, 1829, 1853, 1877, 1901, 1925, 1949, 1973, 1997, 2021, 2045, 2069, 2093, 2117, 2141, 2165, 2189, 2213, 2237, 2261, 2285, 2309, 2333, 2357, 2381, 2399]\nn rows [29, 53, 77, 101, 125, 149, 173, 197, 221, 245, 269, 293, 317, 341, 365, 389, 413, 437, 461, 485, 509, 533, 557, 581, 605, 629, 653, 677, 701, 725, 749, 773, 797, 821, 845, 869, 893, 917, 941, 965, 989, 1013, 1037, 1061, 1085, 1109, 1133, 1157, 1181, 1205, 1229, 1253, 1277, 1301, 1325, 1349, 1373, 1397, 1421, 1445, 1469, 1493, 1517, 1541, 1565, 1589, 1613, 1637, 1661, 1685, 1709, 1733, 1757, 1781, 1805, 1829, 1853, 1877, 1901, 1925, 1949, 1973, 1997, 2021, 2045, 2069, 2093, 2117, 2141, 2165, 2189, 2213, 2237, 2261, 2285, 2309, 2333, 2357, 2381, 2399]\n</code></pre> <pre><code>\n</code></pre> <pre><code>from sklearn.metrics import auc\n\nsns.lineplot(x =pct,y=xlearner_gain,label=f'X-learner, AUC {auc(pct,xlearner_gain).round(3)}')\nsns.lineplot(x =pct,y=slearner_gain,label=f'S-learner, AUC {auc(pct,slearner_gain).round(3)}')\nsns.lineplot(x =pct,y=tlearner_gain,label=f'T-learner, AUC {auc(pct,tlearner_gain).round(3)}')\nsns.lineplot(x=[0, 1],y= [0, 1],markers='--',label='Random treatment, AUC = 0.5')\nplt.title('Cumulative Gain Curve in Training Data')\n</code></pre> <pre><code>Text(0.5, 1.0, 'Cumulative Gain Curve in Training Data')\n</code></pre> <p></p> <p>In uplift curve still there are limitation, where if not in randomized experiment it fails to make group in cumulative comparable. Hence we need other solution</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#qini-curve","title":"Qini Curve","text":"<p>The next one to eva</p> <p>According to Radcliffe (2007) qini curve can be defined as :  \\(g(t) = \\left (    Y_t^T - \\cfrac{Y_t^C N_t^T} {N_t^C}    \\right )\\)</p> <p>if we dig up little bit </p> <p>\\(g(t) = \\left (   \\cfrac{ Y_t^T N_t^C}{N_t^C} - \\cfrac{Y_t^C N_t^T} {N_t^C}    \\right )\\)</p> <p>to make it comparable outcome for both treatment and control are scaled / multiplied by their counterpart</p> <p>with :  1. \\(Y_t^T\\) : Cumulative Outcome Variable for Treatment Group up to \\(t^{th}\\) observation  2. \\(Y_t^C\\) : Cumulative Outcome Variable for Control Group up to \\(t^{th}\\) observation 3. \\(N_t^T\\) : Number of Observation for Treatment Group up to \\(t^{th}\\) observation 4. \\(N_t^C\\) : Number of Observation for Control Group up to \\(t^{th}\\) observation</p> <pre><code>def calculate_qini_curve(data,treatment_col,outcome_col,uplift_col,min_periods=30, steps=100,normalize=True) : \n    size = data.shape[0]\n    data = data.copy()\n\n    # sort based on uplift_col\n    data = data.sort_values(uplift_col,ascending=False)\n    encoded_treatment = pd.get_dummies(data[treatment_col]).astype(int)\n    encoded_treatment.columns = [f'{treatment_col}_0',f'{treatment_col}_1']\n    data = data.drop(treatment_col,axis=1)\n    data = pd.concat([data,encoded_treatment],axis=1)\n\n    # multiply f'{treatment_col}_0',f'{treatment_col}_1' to outcome column \n    data[f'{outcome_col}_{treatment_col}_0'] = data[f'{treatment_col}_0'] * data[outcome_col] # our $Y_t^T$\n    data[f'{outcome_col}_{treatment_col}_1'] = data[f'{treatment_col}_1'] * data[outcome_col] # our $Y_t^C$\n\n    # calculate cumsum \n    data['Y_T'] = data[f'{outcome_col}_{treatment_col}_1'].cumsum()\n    data['Y_C'] = data[f'{outcome_col}_{treatment_col}_0'].cumsum()\n\n\n    data['N_T'] = data[f'{treatment_col}_1'].cumsum()\n    data['N_C'] = data[f'{treatment_col}_0'].cumsum()\n\n\n\n    # calculate f_t \n\n    # uplift =  ( ( (data['Y_T']/data['N_T']) \\\n    #                     - (data['Y_C']/data['N_C']) ) * (data['N_T'] + data['N_C']) ).values\n    # if normalize : \n    #     max_value = uplift.max()\n    #     uplift = uplift/max_value\n\n    # instead of calculating uplift for each row / observation calculate at each step \n    # inspired from https://matheusfacure.github.io/python-causality-handbook/19-Evaluating-Causal-Models.html#references\n    n_rows = list(range(min_periods, size, size // steps)) + [size] \n    n_rows = [x-1 for x in n_rows]\n    print('n rows',n_rows)\n    cumulative_uplifts = np.zeros(len(n_rows))\n    for idx,rows in enumerate(n_rows) : \n        data_at = data.iloc[rows,:]\n\n        uplift =    (data_at['Y_T']) \\\n                                - ( (data_at['Y_C']*data_at['N_T']) / data_at['N_C'] ) \n\n        cumulative_uplifts[idx] = uplift\n\n\n    pct = [rows/size for rows in n_rows]\n\n    if normalize : \n        # normalizing the uplift into 0 to 1 scale so that comparable, \n        max_value = cumulative_uplifts.max()\n        cumulative_uplifts = cumulative_uplifts/max_value\n\n\n    return pct, cumulative_uplifts\n</code></pre> <pre><code>pct,slearner_qini = calculate_qini_curve(data=inference_train,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_slearner',normalize=True)\npct,xlearner_qini = calculate_qini_curve(data=inference_train,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_xlearner',normalize=True)\npct,tlearner_qini =   calculate_qini_curve(data=inference_train,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_tlearner',normalize=True)\n</code></pre> <pre><code>n rows [29, 53, 77, 101, 125, 149, 173, 197, 221, 245, 269, 293, 317, 341, 365, 389, 413, 437, 461, 485, 509, 533, 557, 581, 605, 629, 653, 677, 701, 725, 749, 773, 797, 821, 845, 869, 893, 917, 941, 965, 989, 1013, 1037, 1061, 1085, 1109, 1133, 1157, 1181, 1205, 1229, 1253, 1277, 1301, 1325, 1349, 1373, 1397, 1421, 1445, 1469, 1493, 1517, 1541, 1565, 1589, 1613, 1637, 1661, 1685, 1709, 1733, 1757, 1781, 1805, 1829, 1853, 1877, 1901, 1925, 1949, 1973, 1997, 2021, 2045, 2069, 2093, 2117, 2141, 2165, 2189, 2213, 2237, 2261, 2285, 2309, 2333, 2357, 2381, 2399]\nn rows [29, 53, 77, 101, 125, 149, 173, 197, 221, 245, 269, 293, 317, 341, 365, 389, 413, 437, 461, 485, 509, 533, 557, 581, 605, 629, 653, 677, 701, 725, 749, 773, 797, 821, 845, 869, 893, 917, 941, 965, 989, 1013, 1037, 1061, 1085, 1109, 1133, 1157, 1181, 1205, 1229, 1253, 1277, 1301, 1325, 1349, 1373, 1397, 1421, 1445, 1469, 1493, 1517, 1541, 1565, 1589, 1613, 1637, 1661, 1685, 1709, 1733, 1757, 1781, 1805, 1829, 1853, 1877, 1901, 1925, 1949, 1973, 1997, 2021, 2045, 2069, 2093, 2117, 2141, 2165, 2189, 2213, 2237, 2261, 2285, 2309, 2333, 2357, 2381, 2399]\nn rows [29, 53, 77, 101, 125, 149, 173, 197, 221, 245, 269, 293, 317, 341, 365, 389, 413, 437, 461, 485, 509, 533, 557, 581, 605, 629, 653, 677, 701, 725, 749, 773, 797, 821, 845, 869, 893, 917, 941, 965, 989, 1013, 1037, 1061, 1085, 1109, 1133, 1157, 1181, 1205, 1229, 1253, 1277, 1301, 1325, 1349, 1373, 1397, 1421, 1445, 1469, 1493, 1517, 1541, 1565, 1589, 1613, 1637, 1661, 1685, 1709, 1733, 1757, 1781, 1805, 1829, 1853, 1877, 1901, 1925, 1949, 1973, 1997, 2021, 2045, 2069, 2093, 2117, 2141, 2165, 2189, 2213, 2237, 2261, 2285, 2309, 2333, 2357, 2381, 2399]\n</code></pre> <pre><code>from sklearn.metrics import auc\n\nsns.lineplot(x =pct,y=xlearner_qini,label=f'X-learner , AUC {auc(pct,xlearner_qini).round(3)}')\nsns.lineplot(x =pct,y=slearner_qini,label=f'S-learner, AUC {auc(pct,slearner_qini).round(3)}')\nsns.lineplot(x =pct,y=tlearner_qini,label=f'T-learner, AUC {auc(pct,tlearner_qini).round(3)}')\nsns.lineplot(x=[0, 1],y= [0, 1],markers='--',label='Random treatment, AUC = 0.5')\nplt.title('Cumulative Qini Curve in Training Data')\n</code></pre> <pre><code>Text(0.5, 1.0, 'Cumulative Qini Curve in Training Data')\n</code></pre> <p></p> <ol> <li>Evaluation Metrics Based on Target Transformation \\((Y^*)\\)</li> </ol> <p>The problem we have when estimating CATE is based on our target transformation \\(Y_i^*\\)</p> <p>our goal is to minimize </p> <p>\\(\\text{min } E[(\\tau_i - \\hat{\\tau_i})^2]\\)</p> <p>\\(\\text{min } E[(Y_i^* - \\hat{\\tau_i})^2]\\)</p> <p>easy task right? you find often in common machine learning </p> <pre><code>train_xlearner_cate = predict_cate_xlearner(data=data_train)\ntrain_slearner_cate = predict_cate_slearner(data=data_train)\ntrain_tlearner_cate = predict_cate_tlearner(data=data_train)\n\ntrain_target_transformation = model_target_transform.predict(data_train.drop(['GrossBooking','Treatment'],axis=1))\n</code></pre> <pre><code>train_target_transformation\n</code></pre> <pre><code>array([-112.40042316,  -58.37587738,  -81.14681886, ..., -108.8789247 ,\n       -103.30859488,  -94.30339274])\n</code></pre> <pre><code>mse_xlearner = mean_squared_error(Y_star_train,train_xlearner_cate)\nmse_slearner = mean_squared_error(Y_star_train,train_slearner_cate)\nmse_tlearner = mean_squared_error(Y_star_train,train_tlearner_cate)\nmse_target_transform = mean_squared_error(Y_star_train,train_target_transformation)\n</code></pre> <pre><code>pd.DataFrame({\n    'model_name' : ['X Learner','S Learner',\n                    'T Learner','Target Transformation'] , \n    'mse' : [mse_xlearner,mse_slearner,mse_tlearner,mse_target_transform]\n})\n</code></pre> model_name mse 0 X Learner 617944.724506 1 S Learner 618012.490589 2 T Learner 617297.753014 3 Target Transformation 374992.073824"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#finding-best-model","title":"Finding Best Model","text":"<p>Common ways to pick best model in machine learning can be done by  1. Cross Validation  2. Finding Model that has the best metrics in Validation Set </p> <p>Both are good, but now we will try second approach as it simple. </p> <p>Steps : </p> <ol> <li> <p>Choose Metrics </p> <p>If we see the that target transformation rely on propensity model, since then when we estimate the error it would be biased to the model fit </p> <p>Hence for now we will use Qini score as metrics </p> </li> <li> <p>Predict on Validation Data </p> </li> </ol> <pre><code>inference_val = data_val.copy()\n</code></pre> <pre><code>inference_val['cate_xlearner'] = predict_cate_xlearner(data=data_val)\ninference_val['cate_slearner'] = predict_cate_slearner(data=data_val)\ninference_val['cate_tlearner'] = predict_cate_tlearner(data=data_val)\n\ninference_val['cate_targettransform'] = model_target_transform.predict(data_val.drop(['GrossBooking','Treatment'],axis=1))\n</code></pre> <ol> <li>Evalute </li> </ol> <pre><code>pct,slearner_qini_val = calculate_qini_curve(data=inference_val,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_slearner',normalize=True)\npct,xlearner_qini_val = calculate_qini_curve(data=inference_val,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_xlearner',normalize=True)\npct,tlearner_qini_val =   calculate_qini_curve(data=inference_val,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_tlearner',normalize=True)\npct,target_transform_qini_val =   calculate_qini_curve(data=inference_val,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_targettransform',normalize=True)\n</code></pre> <pre><code>n rows [29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299]\nn rows [29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299]\nn rows [29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299]\nn rows [29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299]\n</code></pre> <ol> <li>Summarize</li> </ol> <pre><code>from sklearn.metrics import auc\n\nsns.lineplot(x =pct,y=xlearner_qini_val,label=f'X-learner , AUC {auc(pct,xlearner_qini_val).round(3)}')\nsns.lineplot(x =pct,y=slearner_qini_val,label=f'S-learner, AUC {auc(pct,slearner_qini_val).round(3)}')\nsns.lineplot(x =pct,y=tlearner_qini_val,label=f'T-learner, AUC {auc(pct,tlearner_qini_val).round(3)}')\nsns.lineplot(x =pct,y=target_transform_qini_val,label=f'Target Transformation, AUC {auc(pct,target_transform_qini_val).round(3)}')\nsns.lineplot(x=[0, 1],y= [0, 1],markers='--',label='Random treatment, AUC = 0.5')\nplt.title('Cumulative Qini Curve in Validation Data')\n</code></pre> <pre><code>Text(0.5, 1.0, 'Cumulative Qini Curve in Validation Data')\n</code></pre> <p></p> <p>Wee see that the best model is Target Transformation</p> <p>Next we evaluate on test data</p> <pre><code>inference_test = data_test.copy()\n</code></pre> <pre><code>inference_test['cate_xlearner'] = predict_cate_xlearner(data=data_test)\n\npct,xlearner_qini_test =   calculate_qini_curve(data=inference_test,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_xlearner',normalize=True)\n\nsns.lineplot(x =pct,y=xlearner_qini_test,label=f'X-Learner, AUC {auc(pct,xlearner_qini_test).round(3)}')\nsns.lineplot(x=[0, 1],y= [0, 1],markers='--',label='Random treatment, AUC = 0.5')\n</code></pre> <pre><code>n rows [29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299]\n\n\n\n\n\n&lt;Axes: &gt;\n</code></pre> <p></p> <pre><code>inference_test.sort_values('cate_targettransform',ascending=False)\n</code></pre> Age NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week Treatment female male GrossBooking cate_targettransform 86 46 47 1 148 0 31.044854 1 0.0 1.0 360.935582 3885.275750 222 46 48 1 147 0 30.330558 1 0.0 1.0 390.989137 3410.578766 159 67 43 1 108 0 30.298712 1 0.0 1.0 338.279315 3311.544684 256 47 44 1 163 1 31.468509 1 1.0 0.0 302.332050 3279.984190 32 21 33 1 277 0 31.526687 1 0.0 1.0 320.412054 3180.702677 ... ... ... ... ... ... ... ... ... ... ... ... 127 26 9 1 25 0 30.465704 0 1.0 0.0 99.805883 -120.504732 118 36 5 0 330 0 30.113201 0 0.0 1.0 103.206083 -120.914619 135 54 29 1 357 1 30.346237 0 0.0 1.0 101.282853 -121.138202 81 24 45 0 285 0 31.675850 0 1.0 0.0 98.896170 -122.740248 124 33 1 1 224 1 30.281640 0 1.0 0.0 107.618674 -125.771994 <p>300 rows \u00d7 11 columns</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#references","title":"References","text":"<ol> <li>Fascure M. (2020), \"Causal Inference for the Brave and True\", https://matheusfacure.github.io/python-causality-handbook</li> <li>Double Machine Learning for Treatment and Causal Parameters</li> <li>Radcliffe, N. J., 2007a. Using Control Groups to Target on Predicted Lift: Building and Assessing Uplift Models. Direct Marketing Journal, Direct Marketing Asso-ciation Analytics Council (1), pp. 14-21.</li> <li>Houssam Nassif, Finn Kuusisto, Elizabeth S Burnside, and Jude W Shavlik. Uplift modeling with roc: An srl case study. In ILP (Late Breaking Papers), pages 40\u201345, 2013.</li> </ol>"}]}