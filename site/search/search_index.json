{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Latest Blog </p> <ul> <li>Developing Artist Recommendation </li> <li>Measuring Impact of New Initiative with A/B Test</li> </ul>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/","title":"Measuring Effect of Product Recommendation Initiative to decide whether to  launch feature to all user","text":"Photo by Isaac Smith on Unsplash <p>PT Buah Alam , is an online grocery store that are currently growing at an grow stages of startup, to reach profitability they need to make a move. One of the eandeavor by approaching by boosting sales on platform.</p> <p>How to do that ? There are lot of ways. One of them through upselling / cross selling</p> <p>The Data Science Team already developed the recommendation algorithm in checkout pages</p> <p></p> <p>Here's a catch :</p> <ol> <li><p>If we implement new initiative to all of our user , the cost is too big and does not guarantee better marketing performance</p> </li> <li><p>If we implement don't implement / atleast try, we have opportunity cost , \"maybe\" this initiative works well and yield better marketing performance</p> </li> </ol> <p>Solution ?</p> <p>There are several ways to prove causality / causal relationship</p> <p></p> <p>One of the most reliable tool is through Randomized Control Trial (RCT)</p> <p>For example, we want to estimate the effect on changing call to action in payment button</p> <p></p> <p>However there are still other potential cause or what we called as confounding variable.</p> <p></p> <p>To remove the bias , we could add randomization to make other confounder equal , by assignming randomly which user get button change</p> <p></p> <p>That the basic idea of Randomized Control Trial or commonly called as A/B Test</p> <p>So when does the A/B Test works ?</p> <ol> <li><p>When each experiment unit doesnot interfere / influence other experiment unit (Stable Unit Treatment Value Assumption) For example in marketplace where there is network effect</p> </li> <li><p>When assigning experiment is possible (for example it's unethical to assign people to get lower education)</p> </li> <li><p>When period is too long For example experimenting on ads campaign to measure number of deals made for property housing</p> </li> </ol> <p>So is it possible in our case ? Yes ! It's possible because :</p> <ol> <li><p>Each user can be assigned to different feature</p> </li> <li><p>Each user doesnot interfere with other users</p> </li> <li><p>The period to measure causal impact still feasible</p> </li> </ol> <p>So how do we do A/B test? Don't worry here is the steps :</p> <p></p> <p>After that we should decide what metrics as our target of experimentation. Back to our goal this recommender system initiative to boost revenue. Spesifically Revenue from User Transaction. Hence our metrics is Revenue per User</p> <ul> <li>Power is the probability to reject $H_{0}$ given $H_{0}$ is false.</li> </ul> <p>$$ \\text{power} = 1-\\beta = P(\\text{reject } H_{0} \\ | \\ H_{0} \\text{ is false}) $$</p> <ul> <li>Power is dependent to Factor such as<ul> <li>Effect size</li> <li>Data variance</li> <li>Sample size ($n$)</li> </ul> </li> </ul> <p>We will simulate how above mentioned factor related to power</p> In\u00a0[1]: Copied! <pre>import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport math \nimport scipy.stats as stats\n</pre> import pandas as pd  import numpy as np  import matplotlib.pyplot as plt  import seaborn as sns import math  import scipy.stats as stats In\u00a0[2]: Copied! <pre>def generate_data(mu_1,mu_2,std_dev_1,std_dev_2,n_sample_1,n_sample_2, N=10000, n_repeat=1000):\n\n    # generate data \n    data_1 = np.random.normal(mu_1, std_dev_1, N)\n    data_2 = np.random.normal(mu_2, std_dev_2, N)\n\n    sample_1_mean = []\n    sample_2_mean = []\n\n    for i in range(n_repeat):\n        # Generate data\n        sample_1 = np.random.choice(data_1, size=(1, n_sample_1), replace=False)\n        sample_2 = np.random.choice(data_2, size=(1, n_sample_2), replace=False)\n\n        # obtain mean \n        mean_1 = sample_1.mean()\n        mean_2 = sample_2.mean()\n\n        # Append mean to the list\n        sample_1_mean.append(mean_1)\n        sample_2_mean.append(mean_2)\n\n\n    return sample_1_mean,sample_2_mean\n</pre> def generate_data(mu_1,mu_2,std_dev_1,std_dev_2,n_sample_1,n_sample_2, N=10000, n_repeat=1000):      # generate data      data_1 = np.random.normal(mu_1, std_dev_1, N)     data_2 = np.random.normal(mu_2, std_dev_2, N)      sample_1_mean = []     sample_2_mean = []      for i in range(n_repeat):         # Generate data         sample_1 = np.random.choice(data_1, size=(1, n_sample_1), replace=False)         sample_2 = np.random.choice(data_2, size=(1, n_sample_2), replace=False)          # obtain mean          mean_1 = sample_1.mean()         mean_2 = sample_2.mean()          # Append mean to the list         sample_1_mean.append(mean_1)         sample_2_mean.append(mean_2)       return sample_1_mean,sample_2_mean  In\u00a0[3]: Copied! <pre>def generate_ab_viz(mean_1,mean_2) : \n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 7))\n\n    sns.histplot(mean_1,alpha = 1, bins = 30,kde=True,\n                label = r\"Group 1 - $H_0$\",color='red')\n    sns.histplot(mean_2,alpha = 1, bins = 30,kde=True,\n                label = r\"Group 2 - $H_1$\",color='green')\n    plt.axvline(np.mean(mean_1),color = 'b',label='group 1 mean',linestyle='--')\n    plt.axvline(np.mean(mean_2),color = 'yellow',label='group 2 mean',linestyle='--')\n    ax.legend()\n    plt.show()\n</pre> def generate_ab_viz(mean_1,mean_2) :      fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 7))      sns.histplot(mean_1,alpha = 1, bins = 30,kde=True,                 label = r\"Group 1 - $H_0$\",color='red')     sns.histplot(mean_2,alpha = 1, bins = 30,kde=True,                 label = r\"Group 2 - $H_1$\",color='green')     plt.axvline(np.mean(mean_1),color = 'b',label='group 1 mean',linestyle='--')     plt.axvline(np.mean(mean_2),color = 'yellow',label='group 2 mean',linestyle='--')     ax.legend()     plt.show() <p>Let's generate some simulation</p> <p>Bigger</p> In\u00a0[4]: Copied! <pre>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':20 , 'std_dev_2':20, \n    'n_sample_1':100,'n_sample_2':100,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</pre> dict_params = {     'mu_1' : 1 ,'mu_2' : 2 ,     'std_dev_1':20 , 'std_dev_2':20,      'n_sample_1':100,'n_sample_2':100,     'n_repeat':1000 } mean_1,mean_2 = generate_data(**dict_params, N=1000) generate_ab_viz(mean_1,mean_2) <p>The More Variance we get, is getting harder to see the effect size of two groups</p> <p>Smaller</p> In\u00a0[5]: Copied! <pre>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':1 , 'std_dev_2':1, \n    'n_sample_1':100,'n_sample_2':100,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</pre> dict_params = {     'mu_1' : 1 ,'mu_2' : 2 ,     'std_dev_1':1 , 'std_dev_2':1,      'n_sample_1':100,'n_sample_2':100,     'n_repeat':1000 } mean_1,mean_2 = generate_data(**dict_params, N=1000) generate_ab_viz(mean_1,mean_2) <p>The Less Variance we get we can see the difference clearly now</p> <p>Smaller</p> In\u00a0[6]: Copied! <pre>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':100,'n_sample_2':100,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</pre> dict_params = {     'mu_1' : 1 ,'mu_2' : 2 ,     'std_dev_1':5 , 'std_dev_2':5,      'n_sample_1':100,'n_sample_2':100,     'n_repeat':1000 } mean_1,mean_2 = generate_data(**dict_params, N=1000) generate_ab_viz(mean_1,mean_2) <p>Bigger</p> In\u00a0[7]: Copied! <pre>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':500,'n_sample_2':500,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</pre> dict_params = {     'mu_1' : 1 ,'mu_2' : 2 ,     'std_dev_1':5 , 'std_dev_2':5,      'n_sample_1':500,'n_sample_2':500,     'n_repeat':1000 } mean_1,mean_2 = generate_data(**dict_params, N=1000) generate_ab_viz(mean_1,mean_2) <p>Smaller</p> In\u00a0[8]: Copied! <pre>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 1+0.01 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':500,'n_sample_2':500,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</pre> dict_params = {     'mu_1' : 1 ,'mu_2' : 1+0.01 ,     'std_dev_1':5 , 'std_dev_2':5,      'n_sample_1':500,'n_sample_2':500,     'n_repeat':1000 } mean_1,mean_2 = generate_data(**dict_params, N=1000) generate_ab_viz(mean_1,mean_2) <p>Bigger</p> In\u00a0[9]: Copied! <pre>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 1+5 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':500,'n_sample_2':500,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</pre> dict_params = {     'mu_1' : 1 ,'mu_2' : 1+5 ,     'std_dev_1':5 , 'std_dev_2':5,      'n_sample_1':500,'n_sample_2':500,     'n_repeat':1000 } mean_1,mean_2 = generate_data(**dict_params, N=1000) generate_ab_viz(mean_1,mean_2) <p>The more small effect size we want to detect --&gt; more power required</p> In\u00a0[10]: Copied! <pre>def calculate_minimum_sample(effect_size,std,alpha,beta) : \n    # obtain both zscore\n    z_alpha_2 = stats.norm.ppf(1 - alpha/2)\n    z_beta = stats.norm.ppf(1 - beta)\n    \n    upper = 2*(std**2)*(z_alpha_2 + z_beta)**2\n    lower = effect_size**2\n    \n    n_sample = math.ceil(upper/lower)\n    return n_sample\n</pre> def calculate_minimum_sample(effect_size,std,alpha,beta) :      # obtain both zscore     z_alpha_2 = stats.norm.ppf(1 - alpha/2)     z_beta = stats.norm.ppf(1 - beta)          upper = 2*(std**2)*(z_alpha_2 + z_beta)**2     lower = effect_size**2          n_sample = math.ceil(upper/lower)     return n_sample In\u00a0[11]: Copied! <pre>n_sample = calculate_minimum_sample(effect_size=1,std=10,alpha=0.05,beta=0.2)\nn_sample\n</pre> n_sample = calculate_minimum_sample(effect_size=1,std=10,alpha=0.05,beta=0.2) n_sample Out[11]: <pre>1570</pre> <p>For each variant we should assign 1570 randomization unit, in this case is user</p> <p>A good experiment duration &lt; 1 month , since the variation is only 2, we can try to conduct it by 1 week. Starting from February 2nd, 2024 to February 8th, 2024</p> <p>We will generate using synthetic data</p> In\u00a0[12]: Copied! <pre>num_samples = int((100_000/30)*7)\n\ndef generate_data(num_samples,metric_name) : \n    dummy_aa_test = pd.DataFrame({\n        'user_id': np.arange(1, num_samples + 1),\n       f'{metric_name}': stats.halfnorm(loc=1, scale=10).rvs(num_samples) # constraint revenue to positive\n    })\n    return dummy_aa_test\n    \ndummy_aa_test = generate_data(num_samples,metric_name='revenue_per_user')\ndummy_aa_test\n</pre>  num_samples = int((100_000/30)*7)  def generate_data(num_samples,metric_name) :      dummy_aa_test = pd.DataFrame({         'user_id': np.arange(1, num_samples + 1),        f'{metric_name}': stats.halfnorm(loc=1, scale=10).rvs(num_samples) # constraint revenue to positive     })     return dummy_aa_test      dummy_aa_test = generate_data(num_samples,metric_name='revenue_per_user') dummy_aa_test Out[12]: user_id revenue_per_user 0 1 5.264200 1 2 11.863647 2 3 5.423547 3 4 14.484032 4 5 15.570061 ... ... ... 23328 23329 9.137816 23329 23330 6.039538 23330 23331 16.040150 23331 23332 1.436674 23332 23333 5.646788 <p>23333 rows \u00d7 2 columns</p> In\u00a0[13]: Copied! <pre>def simulate_aa_test(n_iter,metric_name,num_samples) : \n    data = generate_data(num_samples,metric_name=metric_name)\n    \n    p_vals_t_test = []\n    for i in range(n_iter) : \n        data['group'] = np.random.choice([1,2 ], size=data.shape[0])\n        metric_g1 = data[data['group'] == 1][metric_name]\n        metric_g2 = data[data['group'] == 2][metric_name]\n        \n        mu_1 = metric_g1.mean()\n        mu_2 = metric_g2.mean()\n\n        t_stat, p_value = stats.ttest_ind(metric_g1, metric_g2)\n\n        p_vals_t_test.append(p_value)\n\n    # perform k-s test\n    ks_stat,p_val_ks_test = stats.kstest(p_vals_t_test,stats.uniform.cdf)    \n    return p_vals_t_test,p_val_ks_test\n</pre> def simulate_aa_test(n_iter,metric_name,num_samples) :      data = generate_data(num_samples,metric_name=metric_name)          p_vals_t_test = []     for i in range(n_iter) :          data['group'] = np.random.choice([1,2 ], size=data.shape[0])         metric_g1 = data[data['group'] == 1][metric_name]         metric_g2 = data[data['group'] == 2][metric_name]                  mu_1 = metric_g1.mean()         mu_2 = metric_g2.mean()          t_stat, p_value = stats.ttest_ind(metric_g1, metric_g2)          p_vals_t_test.append(p_value)      # perform k-s test     ks_stat,p_val_ks_test = stats.kstest(p_vals_t_test,stats.uniform.cdf)         return p_vals_t_test,p_val_ks_test In\u00a0[14]: Copied! <pre>p_value,ks_p_val = simulate_aa_test(n_iter=1000,metric_name='revenue_per_user',num_samples=num_samples)\n</pre> p_value,ks_p_val = simulate_aa_test(n_iter=1000,metric_name='revenue_per_user',num_samples=num_samples)  In\u00a0[15]: Copied! <pre>sns.histplot(p_value,alpha = 1, bins = 30,kde=True,\n            label = r\"p-value\",color='blue')\n</pre> sns.histplot(p_value,alpha = 1, bins = 30,kde=True,             label = r\"p-value\",color='blue') Out[15]: <pre>&lt;Axes: ylabel='Count'&gt;</pre> In\u00a0[16]: Copied! <pre>ks_p_val\n</pre> ks_p_val Out[16]: <pre>0.7919638330879013</pre> <p>The simulated A/A test is from the same distribution (uniform distribution)</p> <p>So what do we do when it comes after experiment. We obtain data from experiment log from both of each group. However as Data Scientist we should be sceptical hence we need to perform Sanity Check</p> In\u00a0[17]: Copied! <pre>experiment_data = pd.read_csv('experiment_collected_data.csv',parse_dates=['datetime'])\nexperiment_data\n</pre> experiment_data = pd.read_csv('experiment_collected_data.csv',parse_dates=['datetime']) experiment_data Out[17]: datetime user_id group revenue_per_user 0 2024-02-02 d5683160-da8e-4027-80eb-f1543a6203e5 control 16.433941 1 2024-02-03 c829d253-d79e-42b8-9fe0-f2f69bd9d7d0 treatment 2.274594 2 2024-02-03 4307a4d6-1125-4191-8aa2-a473c5f46c5c control 6.293043 3 2024-02-07 f23f605a-7a25-45bc-b4bf-3c9ddb4dfacb control 29.030031 4 2024-02-06 fb404923-91df-4c01-956c-3ede39361914 control 2.196385 ... ... ... ... ... 3135 2024-02-04 e56ea29f-a89f-46d0-a81d-1ad6759f26f3 treatment 13.885899 3136 2024-02-04 2597b6c8-71e2-4acc-87a0-8d775e7bf83f treatment 6.362146 3137 2024-02-02 5e758638-e6e2-49c8-81ab-187156b59f97 control 3.460954 3138 2024-02-03 310130df-086e-4c5e-911c-1401979195a5 control 1.898356 3139 2024-02-06 532dc215-a179-4874-9e8b-89d141fe10c9 control 2.944030 <p>3140 rows \u00d7 4 columns</p> <p>Check number of unique user from each group</p> In\u00a0[18]: Copied! <pre>experiment_data.groupby('group')['user_id'].nunique()\n</pre> experiment_data.groupby('group')['user_id'].nunique() Out[18]: <pre>group\ncontrol      1570\ntreatment    1570\nName: user_id, dtype: int64</pre> <p>Number of Unique User is like what we expect</p> <p>Next, we should check the experiment datetime</p> In\u00a0[19]: Copied! <pre>filter_1 = experiment_data['datetime'] &lt;= '2024-08-02'\nfilter_2 = experiment_data['datetime'] &gt;= '2024-02-02'\n\nexperiment_data.loc[~(filter_1&amp;filter_2)]\n</pre> filter_1 = experiment_data['datetime'] &lt;= '2024-08-02' filter_2 = experiment_data['datetime'] &gt;= '2024-02-02'  experiment_data.loc[~(filter_1&amp;filter_2)] Out[19]: datetime user_id group revenue_per_user <p>Our data is valid in terms of datetime as experiment plan</p> <p>Next, we should check if there is duplicate record from user assignment</p> In\u00a0[20]: Copied! <pre>experiment_data.duplicated(subset=['user_id','group']).sum()\n</pre> experiment_data.duplicated(subset=['user_id','group']).sum() Out[20]: <pre>0</pre> <p>It's safe from duplicate user assignment</p> In\u00a0[21]: Copied! <pre>summarize_data = experiment_data.groupby('group').agg({'revenue_per_user':np.mean})\nsummarize_data\n</pre> summarize_data = experiment_data.groupby('group').agg({'revenue_per_user':np.mean}) summarize_data <pre>C:\\Users\\fakhr\\AppData\\Local\\Temp\\ipykernel_21012\\1678331338.py:1: FutureWarning:\n\nThe provided callable &lt;function mean at 0x000001AD8D28E480&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n\n</pre> Out[21]: revenue_per_user group control 8.949105 treatment 9.852804 In\u00a0[22]: Copied! <pre>summarize_data['lift'] = summarize_data['revenue_per_user'].diff(1)\nsummarize_data\n</pre> summarize_data['lift'] = summarize_data['revenue_per_user'].diff(1) summarize_data Out[22]: revenue_per_user lift group control 8.949105 NaN treatment 9.852804 0.903699 In\u00a0[23]: Copied! <pre>control_group = experiment_data.loc[experiment_data['group']=='control','revenue_per_user'].values\ntreatment_group = experiment_data.loc[experiment_data['group']=='treatment','revenue_per_user'].values\n</pre> control_group = experiment_data.loc[experiment_data['group']=='control','revenue_per_user'].values treatment_group = experiment_data.loc[experiment_data['group']=='treatment','revenue_per_user'].values In\u00a0[24]: Copied! <pre>t_stat,p_val_t_test = stats.ttest_ind(treatment_group,control_group, equal_var=True, random_state=45, alternative='greater')\n</pre> t_stat,p_val_t_test = stats.ttest_ind(treatment_group,control_group, equal_var=True, random_state=45, alternative='greater') In\u00a0[25]: Copied! <pre>p_val_t_test\n</pre> p_val_t_test Out[25]: <pre>1.5326415297769846e-05</pre> <p>Proven that we can reject null hypothesis</p> In\u00a0[26]: Copied! <pre># Parameters\nn_samples = 2*1570\nalpha = 0.05\ndf = n_samples-2\ncritical_value = stats.t.ppf(1 - alpha, df)  # one tailed \n\n# t-distribution\nx = np.linspace(-4, 4, 1000) # generate some data \ny = stats.t.pdf(x, df)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label=f't-distribution with {df} degrees of freedom')\nplt.fill_between(x, y, where= (x &gt; critical_value), color='red', alpha=0.5, label='Rejection Region')\n\n# Add critical values\nplt.axvline(critical_value, color='black', linestyle='--', label=f'Critical Value: {critical_value:.3f}')\nplt.axvline(t_stat, color='green', linestyle='--', label=f'T statistic value')\n\n# Labels and title\nplt.title('Two-tailed t-test Rejection Region')\nplt.xlabel('t-value')\nplt.ylabel('pdf')\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre>   # Parameters n_samples = 2*1570 alpha = 0.05 df = n_samples-2 critical_value = stats.t.ppf(1 - alpha, df)  # one tailed   # t-distribution x = np.linspace(-4, 4, 1000) # generate some data  y = stats.t.pdf(x, df)  # Plotting plt.figure(figsize=(10, 6)) plt.plot(x, y, label=f't-distribution with {df} degrees of freedom') plt.fill_between(x, y, where= (x &gt; critical_value), color='red', alpha=0.5, label='Rejection Region')  # Add critical values plt.axvline(critical_value, color='black', linestyle='--', label=f'Critical Value: {critical_value:.3f}') plt.axvline(t_stat, color='green', linestyle='--', label=f'T statistic value')  # Labels and title plt.title('Two-tailed t-test Rejection Region') plt.xlabel('t-value') plt.ylabel('pdf') plt.legend() plt.grid(True) plt.show()  <p>Clearly we   reject the null hypothesis. Next Step we can generate confidence interval for our treatment effect using standard error</p> <p>$$ (\\bar{x}_{\\text{treatment}} - \\bar{x}_{\\text{control}} )  -  t_{\\alpha/2,df} \\cdot SE \\leq \\mu_{\\text{treatment}}- \\mu_{\\text{treatment}} \\leq (\\bar{x}_{\\text{treatment}} - \\bar{x}_{\\text{control}} )  +  t_{\\alpha/2,df} \\cdot SE$$</p> <p>$$SE = \\sqrt{(\\cfrac{s_1^2}{n_1} + \\cfrac{s_2^2}{n_2}})  $$</p> <p>with :</p> <ol> <li>$SE$ : Standard error</li> <li>$s_1^2$ : variance from group 1 , control group</li> <li>$s_2^2$ : variance from group 2 , treatment group</li> <li>$n_1$ : number of sample from group 1 , control group</li> <li>$n_2$ : number of sample from group 2 , treatment group</li> <li>$t_{\\alpha/2 , df}$ : t table value given alpha and degrees of freedom</li> </ol> In\u00a0[27]: Copied! <pre>difference = treatment_group.mean() - control_group.mean() \nn_1 = 1570 \nn_2 = 1570\ns_1 = control_group.var()\ns_2 = treatment_group.var()\n\nse = np.sqrt((s_1/n_1) + (s_2/n_2))\n\nt_alpha = stats.t.ppf(alpha/2, df=n_1+n_2-2)\n\n\nc_i = t_alpha*se\n\n\n\nprint(f'Effect Confidence Interval {difference} +/-{c_i}')\n</pre> difference = treatment_group.mean() - control_group.mean()  n_1 = 1570  n_2 = 1570 s_1 = control_group.var() s_2 = treatment_group.var()  se = np.sqrt((s_1/n_1) + (s_2/n_2))  t_alpha = stats.t.ppf(alpha/2, df=n_1+n_2-2)   c_i = t_alpha*se    print(f'Effect Confidence Interval {difference} +/-{c_i}')    <pre>Effect Confidence Interval 0.903699066036344 +/--0.4243055658869349\n</pre> In\u00a0[28]: Copied! <pre>c_i\n</pre> c_i Out[28]: <pre>-0.4243055658869349</pre> In\u00a0[29]: Copied! <pre>difference\n</pre> difference Out[29]: <pre>0.903699066036344</pre> In\u00a0[30]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nplt.figure(figsize=(8, 6))\nplt.errorbar(y=1, x=difference, xerr=-c_i, fmt='o', ecolor='black', capsize=5, capthick=2, elinewidth=1, label='Revenue per User + CI')\nplt.vlines(x = 0, color = 'red',ymin = 0, ymax = 2, label = 'Statistically not Significant',linestyle='--')\nplt.vlines(x = 1, color = 'green',ymin = 0, ymax = 2, label = 'Practically Not Significant',linestyle='--')\n\nplt.title('Comparing Practically and Statistically Significant')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np    plt.figure(figsize=(8, 6)) plt.errorbar(y=1, x=difference, xerr=-c_i, fmt='o', ecolor='black', capsize=5, capthick=2, elinewidth=1, label='Revenue per User + CI') plt.vlines(x = 0, color = 'red',ymin = 0, ymax = 2, label = 'Statistically not Significant',linestyle='--') plt.vlines(x = 1, color = 'green',ymin = 0, ymax = 2, label = 'Practically Not Significant',linestyle='--')  plt.title('Comparing Practically and Statistically Significant') plt.xlabel('X-axis') plt.ylabel('Y-axis') plt.grid(True) plt.legend()  # Show the plot plt.show()"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#measuring-effect-of-product-recommendation-initiative-to-decide-whether-to-launch-feature-to-all-user","title":"Measuring Effect of Product Recommendation Initiative to decide whether to  launch feature to all user\u00b6","text":""},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#1-setting-up-problem","title":"1. Setting Up Problem\u00b6","text":"<p>Our focus is about increasing revenue ? But which revenue ? There are lot of revenues. The Revenue we are using is from user transaction because we can intervene on it.</p> <p>After that we should have idea what is the current funnel / user journey in buying phase</p> <p></p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#2-designing-experiment","title":"2. Designing Experiment\u00b6","text":""},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#21-experiment-unit","title":"2.1. Experiment Unit\u00b6","text":"<p>Our Experiment unit is user level , but which user since the recommendation system is located checkout process , our user pool is user who proceed into checkout process</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#22-group","title":"2.2. Group\u00b6","text":"<ul> <li>Control Group : Users who dont receive product recommendation during checkout process</li> <li>Treatment Group : User who receive product recommendation during checkout process</li> </ul>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#23-hypothesis","title":"2.3. Hypothesis\u00b6","text":"<p>We want to check if revenue per user for treatment group is &gt; control group</p> <p>$H_0 = \\mu_{control} &gt;= \\mu_{treatment}$</p> <p>$H_1 = \\mu_{control} &lt; \\mu_{treatment}$</p> <p>Our statistical test aim to prove the hypothesis whether it is rejected or fail to reject</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#23-number-of-samples","title":"2.3. Number of Samples\u00b6","text":"<p>As Information , we have 15 Million Active User , with median monthly in-checkout-process user of 20.000 . We use median because number of user in checkout process is vary.</p> <p>So how many number of user we need for both treatment and control ? Well it's depend on Treatment Effect you want to capture , the smaller the effect you want to capture the more sample we need to add because of the experiment precision</p> <p>Decision to role out new feature based on whether new initiative meet certain criteria such as :</p> <ol> <li><p>Statistically Significant We'll talk about it later</p> </li> <li><p>Practically Significant Related to Business Requirement</p> </li> </ol>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#231-business-requirement","title":"2.3.1 Business Requirement\u00b6","text":"<p>Our new initiative by recommending purchase is indeed costly, we have to calculate the cost first</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#2311-cost-of-making-recommendation","title":"2.3.1.1. Cost of Making Recommendation\u00b6","text":"<ol> <li><p>Man Power The Development process assumed in six month , people that are involved :</p> <ul> <li><p>Data Scientist , assume monthly rate $ 8000 to $ 12000 / month x 6 month = $ 48.000 - $ 72.000</p> </li> <li><p>Data Engineer  , assume monthly rate $8000 to $12000 / month x 6 month = $48.000 - $72.000</p> </li> <li><p>MLOps , assume monthly rate $9000 to $10000 / month x 6 month = $45.000 - $60.000</p> </li> <li><p>Software Engineer , assume monthly rate $9000 to $10000 / month x 6 month = $45.000 - $60.000</p> <p>|                   | Development Cost      | Development Cost     | |-------------------|-----------------------|----------------------| | Role              | Lower Bound Cost   | Upper Bound Cost | | Data Scientist    | 48000                 | 72000                | | Data Engineer     | 48000                 | 72000                | | MLOps             | 45000                 | 60000                | | Software Engineer | 45000                 | 60000                | | Total Cost | 186000                 | 264000                |</p> </li> </ul> </li> <li><p>Cloud to handle ~ 100k users per  day x 30 day, another information is that the calculation is in batch manner, we don't need real time recommendation yet. Let say the cost isUSD $1000-2000 per month  x 12 month =$1.000 -$24.000  annualy</p> </li> <li><p>Cost Summarization</p> </li> </ol> <p>| Cost Attribute    | Lower Bound Cost  | Upper Bound Cost | |-------------------|-------------------|------------------| | Development Cost  | 186000            | 264000           | | Annual Infra Cost | 12000             | 24000            | | Total             | 198000            | 288000           |  |</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#2312-reasonable-revenue","title":"2.3.1.2. Reasonable Revenue\u00b6","text":"<p>We have seen that in order to make recommender system is not cheap at this scale, hence at what incremental level should we roll out this feature to all user ? Of Course covering costs + expected increase in user revenue.</p> <p>About cost we have already know let say we take  USD 288,000 , what about expected increase in user revenue. As per now median per user revenue was about USD 1 daily.  Median  of monthly user checkout 100.000 user minimum revenue should be $Revenue &gt;= 288.000$</p> <p>Current Revenue per User/ annual  = 1 x 20.000 x 12  =  USD 240.000 Gaps = Not Yet Covered Cost + Expected Growth USD 48.000 / (12 x 20.000 ) = USD 0.2</p> <p>Revenue / User / Daily &gt;= USD 1.2</p> <p>If we continue to success we can have USD 1 increase from USD 1 to USD 2 We can have Additional Revenue of</p> <p>First Year</p> <p>2-1.2 x 20.000 x 12 = USD 192.000</p> <p>Second Year or more</p> <p>2 x 20.000 x 12 - (Annual Infra Cost)</p> <p>2 x 20.000 x 12 - (24.000) = USD 456.000</p> <p>So in this case we have to able to detect USD 1 Change in revenue per user. This will relate to the next problem which is power Analysis</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#231-power-analysis","title":"2.3.1 Power Analysis\u00b6","text":"<p>In statistical testing there are two possible Errors :</p> <ul> <li>Rejecting when it should failed to be Rejected, or (Type I Error)</li> <li>Fail to Reject when it should be Rejected or (Type II Error)</li> </ul> <p>Typically Type I Error influence what we called as significance level or $\\alpha$. While the second one is influenced by Power $(1-\\beta)$</p> <p>Will be focusing on Power Analysis</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#variance","title":"Variance\u00b6","text":""},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#sample-size","title":"Sample Size\u00b6","text":""},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#effect-size","title":"Effect Size\u00b6","text":"<p>or Change / Increase we want to detect</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#wrapping-up","title":"Wrapping Up\u00b6","text":"<p>In this case we have already know how much effect we wanna test , in this case USD 1 Increase in Revenue oer User</p> <p>to calculate minimum sample size we can use : $$ n = \\cfrac{2 \\sigma^2 (z_{1-\\alpha/2}+z_{1-\\beta})^2}{\\delta^2} $$</p> <p>with :</p> <ol> <li>$\\delta$ : Effect Size</li> <li>$\\sigma$ : variance of Population (estimated) from revenue per user variance</li> <li>$\\alpha/2$ : alpha ,  $1- Confidence Interval$</li> <li>$\\beta$ : beta, 1- Power Level</li> <li>$z$ : z score</li> </ol> <p>Deciding $\\alpha$ and $\\beta$ , in a typical experiment</p> <ol> <li>$\\alpha$ is 5% or Confidence is 95%</li> <li>$\\beta$ is 20%  or Power is 80%</li> </ol>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#24-experiment-durations","title":"2.4. Experiment Durations\u00b6","text":""},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#3-running-experiments","title":"3. Running Experiments\u00b6","text":"<p>Since we don't have a privilege to experimentation platform we will use generated data.</p> <p>But before running the experiments we will have to do A/A test ?</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#31-before-experiment-aa-tests","title":"3.1. Before Experiment : A/A Tests\u00b6","text":"<p>Why ? Because there are maybe still not a ceterus paribus condition no bias , for example user may have experienced delayed response time that cause user to differ in behaviour which eventually will influence our metrics of interest</p> <p>How ? In order to make that not happen, we can compare same variant by having the same metrics.</p> <p>But, it's expensive to run experiment before experiment. There is a hackish way, by using metrics from last observation, for example last week metrics , and then randomly assign user two different group.</p> <p>Workflow :</p> <ol> <li>Obtain Last Week metrics from each user (revenue per user)</li> <li>For n iteration do : - Randomly Assign user into two groups - Obtain means of each metrics from both group - Conduct statistical test, to check differences of mean metrics between two variant using t-test - Collect p-value</li> <li>Test the p-value distribution whether followed uniform distribution using Goodness of Fit test such as Kolmogorov Smirnoff Test</li> <li>If the p-value &gt; $\\alpha$ can be concluded that the samples is the same from referenced distribution ( in this case uniform distribution)</li> <li>Plot the p-value from n_iteration, the p value should be uniform (indicating variant is the same)</li> </ol>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#4-analyzing-data","title":"4. Analyzing Data\u00b6","text":"<p>Let's Recap our Experimentation Plan</p> <p>Metrics of Experimentation : Revenue per User</p> <p>Experiment Unit : User in Checkout Process</p> <p>Variant :</p> <pre><code>1. Control Group : No Product Recommendation during checkout process \n2. Treatment Group : Product Recommendation during checkout process y </code></pre> <p>Number of Samples : 1570 from each variant</p> <p>Experiment Duration : Experiment was conducted for a week , starting from February 2nd, 2024 to February 8th, 2024</p> <p>Our Hypothesis : by adding product recommendation could boost revenue per user</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#41-data-check","title":"4.1. Data Check\u00b6","text":""},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#411-checking-guadrail-metrics","title":"4.1.1. Checking Guadrail Metrics\u00b6","text":"<p>In real experiment platform, we should check , what we called as Guadrail Metrics, which should not be different. Otherwise it will affect the experiment outcome become bias, for example latency</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#42-aggregating-data","title":"4.2. Aggregating Data\u00b6","text":"<p>Next step after we have clean &amp; valid data, we should aggregate data to reflect metrics from both group</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#43-performing-statistical-test","title":"4.3. Performing Statistical Test\u00b6","text":"<p>Condition : We are interested whether by providing product recommendation during checkout will increase revenue per user.</p> <p>Why we need to perform statistical test ? We can just compare the difference between mean Yes! at some point it is right, however we only have small sample, and trying to estimate the population, hence what we can use to leverage the statistical test to add confidence in our result</p> <p>Type of Statistical Test, we want to compare the revenue, in terms of mean difference between two groups (independent sample), we can leverage T-student Test for mean from two sample. Since we are going to check its bigger or not, its one tailed test</p> <p>Hypothesis :</p> <p>$H_0 : \\mu_{\\text{treatment}} &lt;= \\mu_{\\text{control}}$</p> <p>$H_1 : \\mu_{\\text{treatment}} &gt; \\mu_{\\text{control}}$</p> <p>At experiment plan we want Confidence Interval of 95% or $\\alpha=0.05$</p> <p>Assumption : we don;t know variance about population, and both variance assumed to be equal</p> <p>How to conclude ?</p> <ol> <li>If p-value &gt; $\\alpha$ fail to reject the null Hypothesis, otherwise reject the null hypothesis</li> <li>equivalent to if t-test value &lt; t-table fail to reject the null Hypothesis</li> </ol>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#5-inferring-decision","title":"5. Inferring Decision\u00b6","text":"<p>We already reach the end of the experimentation. We have to decided whether to launch product recommendation system during checkout process or not to all users.</p> <p>To rollout this feature , the result should be :</p> <ol> <li>Statistically Significant</li> <li>Practically Significant</li> </ol> <p>From the Result it looks like only Statistical Significance . However the revenue per user still not fullfill our minimum USD 1 Revenue per user increase, so we have to decide not to launch the product.</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/","title":"Measuring Effect of Product Recommendation Initiative to decide whether to  launch feature to all user","text":"<p> Photo by Isaac Smith on Unsplash </p> <p>PT Buah Alam , is an online grocery store that are currently growing at an grow stages of startup, to reach profitability they need to make a move. One of the eandeavor by approaching by boosting sales on platform. </p> <p>How to do that ? There are lot of ways. One of them through upselling / cross selling </p> <p>The Data Science Team already developed the recommendation algorithm in checkout pages </p> <p></p> <p>Here's a catch : </p> <ol> <li> <p>If we implement new initiative to all of our user , the cost is too big and does not guarantee better marketing performance</p> </li> <li> <p>If we implement don't implement / atleast try, we have opportunity cost , \"maybe\" this initiative works well and yield better marketing performance </p> </li> </ol> <p>Solution ? </p> <p>There are several ways to prove causality / causal relationship </p> <p></p> <p>One of the most reliable tool is through Randomized Control Trial (RCT) </p> <p>For example, we want to estimate the effect on changing call to action in payment button</p> <p></p> <p>However there are still other potential cause or what we called as confounding variable. </p> <p></p> <p>To remove the bias , we could add randomization to make other confounder equal , by assignming randomly which user get button change</p> <p></p> <p>That the basic idea of Randomized Control Trial or commonly called as A/B Test</p> <p>So when does the A/B Test works ? </p> <ol> <li> <p>When each experiment unit doesnot interfere / influence other experiment unit (Stable Unit Treatment Value Assumption)    For example in marketplace where there is network effect</p> </li> <li> <p>When assigning experiment is possible (for example it's unethical to assign people to get lower education)</p> </li> <li> <p>When period is too long For example experimenting on ads campaign to measure number of deals made for property housing </p> </li> </ol> <p>So is it possible in our case ? Yes ! It's possible because : </p> <ol> <li> <p>Each user can be assigned to different feature </p> </li> <li> <p>Each user doesnot interfere with other users</p> </li> <li> <p>The period to measure causal impact still feasible </p> </li> </ol> <p>So how do we do A/B test? Don't worry here is the steps : </p> <p></p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#1-setting-up-problem","title":"1. Setting Up Problem","text":"<p>Our focus is about increasing revenue ? But which revenue ? There are lot of revenues. The Revenue we are using is from user transaction because we can intervene on it. </p> <p>After that we should have idea what is the current funnel / user journey in buying phase </p> <p></p> <p>After that we should decide what metrics as our target of experimentation. Back to our goal this recommender system initiative to boost revenue. Spesifically Revenue from User Transaction. Hence our metrics is Revenue per User</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#2-designing-experiment","title":"2. Designing Experiment","text":""},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#21-experiment-unit","title":"2.1. Experiment Unit","text":"<p>Our Experiment unit is user level , but which user since the recommendation system is located checkout process , our user pool is user who proceed into checkout process </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#22-group","title":"2.2. Group","text":"<ul> <li>Control Group : Users who dont receive product recommendation during checkout process </li> <li>Treatment Group : User who receive product recommendation during checkout process </li> </ul>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#23-hypothesis","title":"2.3. Hypothesis","text":"<p>We want to check if revenue per user for treatment group is &gt; control group </p> <p>\\(H_0 = \\mu_{control} &gt;= \\mu_{treatment}\\)</p> <p>\\(H_1 = \\mu_{control} &lt; \\mu_{treatment}\\)</p> <p>Our statistical test aim to prove the hypothesis whether it is rejected or fail to reject </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#23-number-of-samples","title":"2.3. Number of Samples","text":"<p>As Information , we have 15 Million Active User , with median monthly in-checkout-process user of 20.000 . We use median because number of user in checkout process is vary. </p> <p>So how many number of user we need for both treatment and control ? Well it's depend on Treatment Effect you want to capture , the smaller the effect you want to capture the more sample we need to add because of the experiment precision</p> <p>Decision to role out new feature based on whether new initiative meet certain criteria such as :</p> <ol> <li> <p>Statistically Significant     We'll talk about it later</p> </li> <li> <p>Practically Significant    Related to Business Requirement</p> </li> </ol>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#231-business-requirement","title":"2.3.1 Business Requirement","text":"<p>Our new initiative by recommending purchase is indeed costly, we have to calculate the cost first </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#2311-cost-of-making-recommendation","title":"2.3.1.1. Cost of Making Recommendation","text":"<ol> <li> <p>Man Power     The Development process assumed in six month , people that are involved : </p> <ul> <li> <p>Data Scientist , assume monthly rate $ 8000 to $ 12000 / month x 6 month = $ 48.000 - $ 72.000 </p> </li> <li> <p>Data Engineer  , assume monthly rate $8000 to $12000 / month x 6 month = $48.000 - $72.000 </p> </li> <li> <p>MLOps , assume monthly rate $9000 to $10000 / month x 6 month = $45.000 - $60.000 </p> </li> <li> <p>Software Engineer , assume monthly rate $9000 to $10000 / month x 6 month = $45.000 - $60.000 </p> Development Cost Development Cost Role Lower Bound Cost Upper Bound Cost Data Scientist 48000 72000 Data Engineer 48000 72000 MLOps 45000 60000 Software Engineer 45000 60000 Total Cost 186000 264000 </li> </ul> </li> <li> <p>Cloud      to handle ~ 100k users per  day x 30 day, another information is that the calculation is in batch manner, we don't need real time recommendation yet. Let say the cost isUSD \\(1000-2000 per month  x 12 month =\\)1.000 -$24.000  annualy</p> </li> <li> <p>Cost Summarization</p> </li> </ol> Cost Attribute Lower Bound Cost Upper Bound Cost Development Cost 186000 264000 Annual Infra Cost 12000 24000 Total 198000 288000"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#2312-reasonable-revenue","title":"2.3.1.2. Reasonable Revenue","text":"<p>We have seen that in order to make recommender system is not cheap at this scale, hence at what incremental level should we roll out this feature to all user ? Of Course covering costs + expected increase in user revenue. </p> <p>About cost we have already know let say we take  USD 288,000 , what about expected increase in user revenue. As per now median per user revenue was about USD 1 daily.  Median  of monthly user checkout 100.000 user  minimum revenue should be \\(Revenue &gt;= 288.000\\) </p> <p>Current Revenue per User/ annual  = 1 x 20.000 x 12  =  USD 240.000  Gaps = Not Yet Covered Cost + Expected Growth USD 48.000 / (12 x 20.000 ) = USD 0.2 </p> <p>Revenue / User / Daily &gt;= USD 1.2 </p> <p>If we continue to success we can have USD 1 increase from USD 1 to USD 2 We can have Additional Revenue of </p> <p>First Year</p> <p>2-1.2 x 20.000 x 12 = USD 192.000</p> <p>Second Year or more</p> <p>2 x 20.000 x 12 - (Annual Infra Cost) </p> <p>2 x 20.000 x 12 - (24.000) = USD 456.000</p> <p>So in this case we have to able to detect USD 1 Change in revenue per user. This will relate to the next problem which is power Analysis</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#231-power-analysis","title":"2.3.1 Power Analysis","text":"<p>In statistical testing there are two possible Errors : </p> <ul> <li>Rejecting when it should failed to be Rejected, or (Type I Error) </li> <li>Fail to Reject when it should be Rejected or (Type II Error)</li> </ul> <p>Typically Type I Error influence what we called as significance level or \\(\\alpha\\). While the second one is influenced by Power \\((1-\\beta)\\)</p> <p>Will be focusing on Power Analysis </p> <ul> <li>Power is the probability to reject \\(H_{0}\\) given \\(H_{0}\\) is false.</li> </ul> \\[ \\text{power} = 1-\\beta = P(\\text{reject } H_{0} \\ | \\ H_{0} \\text{ is false}) \\] <ul> <li>Power is dependent to Factor such as </li> <li>Effect size</li> <li>Data variance</li> <li>Sample size (\\(n\\))</li> </ul> <p>We will simulate how above mentioned factor related to power </p> <pre><code>import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport math \nimport scipy.stats as stats\n</code></pre> <pre><code>def generate_data(mu_1,mu_2,std_dev_1,std_dev_2,n_sample_1,n_sample_2, N=10000, n_repeat=1000):\n\n    # generate data \n    data_1 = np.random.normal(mu_1, std_dev_1, N)\n    data_2 = np.random.normal(mu_2, std_dev_2, N)\n\n    sample_1_mean = []\n    sample_2_mean = []\n\n    for i in range(n_repeat):\n        # Generate data\n        sample_1 = np.random.choice(data_1, size=(1, n_sample_1), replace=False)\n        sample_2 = np.random.choice(data_2, size=(1, n_sample_2), replace=False)\n\n        # obtain mean \n        mean_1 = sample_1.mean()\n        mean_2 = sample_2.mean()\n\n        # Append mean to the list\n        sample_1_mean.append(mean_1)\n        sample_2_mean.append(mean_2)\n\n\n    return sample_1_mean,sample_2_mean\n</code></pre> <pre><code>def generate_ab_viz(mean_1,mean_2) : \n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 7))\n\n    sns.histplot(mean_1,alpha = 1, bins = 30,kde=True,\n                label = r\"Group 1 - $H_0$\",color='red')\n    sns.histplot(mean_2,alpha = 1, bins = 30,kde=True,\n                label = r\"Group 2 - $H_1$\",color='green')\n    plt.axvline(np.mean(mean_1),color = 'b',label='group 1 mean',linestyle='--')\n    plt.axvline(np.mean(mean_2),color = 'yellow',label='group 2 mean',linestyle='--')\n    ax.legend()\n    plt.show()\n</code></pre> <p>Let's generate some simulation </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#variance","title":"Variance","text":"<p>Bigger</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':20 , 'std_dev_2':20, \n    'n_sample_1':100,'n_sample_2':100,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>The More Variance we get, is getting harder to see the effect size of two groups</p> <p>Smaller</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':1 , 'std_dev_2':1, \n    'n_sample_1':100,'n_sample_2':100,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>The Less Variance we get we can see the difference clearly now </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#sample-size","title":"Sample Size","text":"<p>Smaller</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':100,'n_sample_2':100,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>Bigger</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':500,'n_sample_2':500,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#effect-size","title":"Effect Size","text":"<p>or Change / Increase we want to detect</p> <p>Smaller</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 1+0.01 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':500,'n_sample_2':500,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>Bigger</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 1+5 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':500,'n_sample_2':500,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>The more small effect size we want to detect \u2192 more power required</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#wrapping-up","title":"Wrapping Up","text":"<p>In this case we have already know how much effect we wanna test , in this case USD 1 Increase in Revenue oer User </p> <p>to calculate minimum sample size we can use :   </p> <p>with : </p> <ol> <li>\\(\\delta\\) : Effect Size</li> <li>\\(\\sigma\\) : variance of Population (estimated) from revenue per user variance </li> <li>\\(\\alpha/2\\) : alpha ,  \\(1- Confidence Interval\\)</li> <li>\\(\\beta\\) : beta, 1- Power Level</li> <li>\\(z\\) : z score</li> </ol> <p>Deciding \\(\\alpha\\) and \\(\\beta\\) , in a typical experiment </p> <ol> <li>\\(\\alpha\\) is 5% or Confidence is 95%</li> <li>\\(\\beta\\) is 20%  or Power is 80%</li> </ol> <pre><code>def calculate_minimum_sample(effect_size,std,alpha,beta) : \n    # obtain both zscore\n    z_alpha_2 = stats.norm.ppf(1 - alpha/2)\n    z_beta = stats.norm.ppf(1 - beta)\n\n    upper = 2*(std**2)*(z_alpha_2 + z_beta)**2\n    lower = effect_size**2\n\n    n_sample = math.ceil(upper/lower)\n    return n_sample\n</code></pre> <pre><code>n_sample = calculate_minimum_sample(effect_size=1,std=10,alpha=0.05,beta=0.2)\nn_sample\n</code></pre> <pre><code>1570\n</code></pre> <p>For each variant we should assign 1570 randomization unit, in this case is user </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#24-experiment-durations","title":"2.4. Experiment Durations","text":"<p>A good experiment duration &lt; 1 month , since the variation is only 2, we can try to conduct it by 1 week. Starting from February 2<sup>nd</sup>, 2024 to February 8<sup>th</sup>, 2024</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#3-running-experiments","title":"3. Running Experiments","text":"<p>Since we don't have a privilege to experimentation platform we will use generated data. </p> <p>But before running the experiments we will have to do A/A test ? </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#31-before-experiment-aa-tests","title":"3.1. Before Experiment : A/A Tests","text":"<p>Why ? Because there are maybe still not a ceterus paribus condition no bias , for example user may have experienced delayed response time that cause user to differ in behaviour which eventually will influence our metrics of interest</p> <p>How ? In order to make that not happen, we can compare same variant by having the same metrics. </p> <p>But, it's expensive to run experiment before experiment. There is a hackish way, by using metrics from last observation, for example last week metrics , and then randomly assign user two different group. </p> <p>Workflow : </p> <ol> <li>Obtain Last Week metrics from each user (revenue per user)</li> <li>For n iteration do : <ul> <li>Randomly Assign user into two groups</li> <li>Obtain means of each metrics from both group</li> <li>Conduct statistical test, to check differences of mean metrics between two variant using t-test</li> <li>Collect p-value</li> </ul> </li> <li>Test the p-value distribution whether followed uniform distribution using Goodness of Fit test such as Kolmogorov Smirnoff Test</li> <li>If the p-value &gt; \\(\\alpha\\) can be concluded that the samples is the same from referenced distribution ( in this case uniform distribution) </li> <li>Plot the p-value from n_iteration, the p value should be uniform (indicating variant is the same) </li> </ol> <p>We will generate using synthetic data </p> <pre><code>num_samples = int((100_000/30)*7)\n\ndef generate_data(num_samples,metric_name) : \n    dummy_aa_test = pd.DataFrame({\n        'user_id': np.arange(1, num_samples + 1),\n       f'{metric_name}': stats.halfnorm(loc=1, scale=10).rvs(num_samples) # constraint revenue to positive\n    })\n    return dummy_aa_test\n\ndummy_aa_test = generate_data(num_samples,metric_name='revenue_per_user')\ndummy_aa_test\n</code></pre> user_id revenue_per_user 0 1 5.264200 1 2 11.863647 2 3 5.423547 3 4 14.484032 4 5 15.570061 ... ... ... 23328 23329 9.137816 23329 23330 6.039538 23330 23331 16.040150 23331 23332 1.436674 23332 23333 5.646788 <p>23333 rows \u00d7 2 columns</p> <pre><code>def simulate_aa_test(n_iter,metric_name,num_samples) : \n    data = generate_data(num_samples,metric_name=metric_name)\n\n    p_vals_t_test = []\n    for i in range(n_iter) : \n        data['group'] = np.random.choice([1,2 ], size=data.shape[0])\n        metric_g1 = data[data['group'] == 1][metric_name]\n        metric_g2 = data[data['group'] == 2][metric_name]\n\n        mu_1 = metric_g1.mean()\n        mu_2 = metric_g2.mean()\n\n        t_stat, p_value = stats.ttest_ind(metric_g1, metric_g2)\n\n        p_vals_t_test.append(p_value)\n\n    # perform k-s test\n    ks_stat,p_val_ks_test = stats.kstest(p_vals_t_test,stats.uniform.cdf)    \n    return p_vals_t_test,p_val_ks_test\n</code></pre> <pre><code>p_value,ks_p_val = simulate_aa_test(n_iter=1000,metric_name='revenue_per_user',num_samples=num_samples) \n</code></pre> <pre><code>sns.histplot(p_value,alpha = 1, bins = 30,kde=True,\n            label = r\"p-value\",color='blue')\n</code></pre> <pre><code>&lt;Axes: ylabel='Count'&gt;\n</code></pre> <p></p> <pre><code>ks_p_val\n</code></pre> <pre><code>0.7919638330879013\n</code></pre> <p>The simulated A/A test is from the same distribution (uniform distribution) </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#4-analyzing-data","title":"4. Analyzing Data","text":"<p>Let's Recap our Experimentation Plan </p> <p>Metrics of Experimentation : Revenue per User </p> <p>Experiment Unit : User in Checkout Process </p> <p>Variant : </p> <pre><code>1. Control Group : No Product Recommendation during checkout process \n2. Treatment Group : Product Recommendation during checkout process y\n</code></pre> <p>Number of Samples : 1570 from each variant </p> <p>Experiment Duration : Experiment was conducted for a week , starting from February 2<sup>nd</sup>, 2024 to February 8<sup>th</sup>, 2024</p> <p>Our Hypothesis : by adding product recommendation could boost revenue per user</p> <p>So what do we do when it comes after experiment. We obtain data from experiment log from both of each group. However as Data Scientist we should be sceptical hence we need to perform Sanity Check </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#41-data-check","title":"4.1. Data Check","text":"<pre><code>experiment_data = pd.read_csv('experiment_collected_data.csv',parse_dates=['datetime'])\nexperiment_data\n</code></pre> datetime user_id group revenue_per_user 0 2024-02-02 d5683160-da8e-4027-80eb-f1543a6203e5 control 16.433941 1 2024-02-03 c829d253-d79e-42b8-9fe0-f2f69bd9d7d0 treatment 2.274594 2 2024-02-03 4307a4d6-1125-4191-8aa2-a473c5f46c5c control 6.293043 3 2024-02-07 f23f605a-7a25-45bc-b4bf-3c9ddb4dfacb control 29.030031 4 2024-02-06 fb404923-91df-4c01-956c-3ede39361914 control 2.196385 ... ... ... ... ... 3135 2024-02-04 e56ea29f-a89f-46d0-a81d-1ad6759f26f3 treatment 13.885899 3136 2024-02-04 2597b6c8-71e2-4acc-87a0-8d775e7bf83f treatment 6.362146 3137 2024-02-02 5e758638-e6e2-49c8-81ab-187156b59f97 control 3.460954 3138 2024-02-03 310130df-086e-4c5e-911c-1401979195a5 control 1.898356 3139 2024-02-06 532dc215-a179-4874-9e8b-89d141fe10c9 control 2.944030 <p>3140 rows \u00d7 4 columns</p> <p>Check number of unique user from each group</p> <pre><code>experiment_data.groupby('group')['user_id'].nunique()\n</code></pre> <pre><code>group\ncontrol      1570\ntreatment    1570\nName: user_id, dtype: int64\n</code></pre> <p>Number of Unique User is like what we expect</p> <p>Next, we should check the experiment datetime </p> <pre><code>filter_1 = experiment_data['datetime'] &lt;= '2024-08-02'\nfilter_2 = experiment_data['datetime'] &gt;= '2024-02-02'\n\nexperiment_data.loc[~(filter_1&amp;filter_2)]\n</code></pre> datetime user_id group revenue_per_user <p>Our data is valid in terms of datetime as experiment plan</p> <p>Next, we should check if there is duplicate record from user assignment </p> <pre><code>experiment_data.duplicated(subset=['user_id','group']).sum()\n</code></pre> <pre><code>0\n</code></pre> <p>It's safe from duplicate user assignment</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#411-checking-guadrail-metrics","title":"4.1.1. Checking Guadrail Metrics","text":"<p>In real experiment platform, we should check , what we called as Guadrail Metrics, which should not be different. Otherwise it will affect the experiment outcome become bias, for example latency</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#42-aggregating-data","title":"4.2. Aggregating Data","text":"<p>Next step after we have clean &amp; valid data, we should aggregate data to reflect metrics from both group </p> <pre><code>summarize_data = experiment_data.groupby('group').agg({'revenue_per_user':np.mean})\nsummarize_data\n</code></pre> <pre><code>C:\\Users\\fakhr\\AppData\\Local\\Temp\\ipykernel_21012\\1678331338.py:1: FutureWarning:\n\nThe provided callable &lt;function mean at 0x000001AD8D28E480&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n</code></pre> revenue_per_user group control 8.949105 treatment 9.852804 <pre><code>summarize_data['lift'] = summarize_data['revenue_per_user'].diff(1)\nsummarize_data\n</code></pre> revenue_per_user lift group control 8.949105 NaN treatment 9.852804 0.903699"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#43-performing-statistical-test","title":"4.3. Performing Statistical Test","text":"<p>Condition :  We are interested whether by providing product recommendation during checkout will increase revenue per user. </p> <p>Why we need to perform statistical test ? We can just compare the difference between mean Yes! at some point it is right, however we only have small sample, and trying to estimate the population, hence what we can use to leverage the statistical test to add confidence in our result </p> <p>Type of Statistical Test, we want to compare the revenue, in terms of mean difference between two groups (independent sample), we can leverage T-student Test for mean from two sample.  Since we are going to check its bigger or not, its one tailed test</p> <p>Hypothesis : </p> <p>\\(H_0 : \\mu_{\\text{treatment}} &lt;= \\mu_{\\text{control}}\\)</p> <p>\\(H_1 : \\mu_{\\text{treatment}} &gt; \\mu_{\\text{control}}\\)</p> <p>At experiment plan we want Confidence Interval of 95% or \\(\\alpha=0.05\\)</p> <p>Assumption : we don;t know variance about population, and both variance assumed to be equal </p> <p>How to conclude ?  1. If p-value &gt; \\(\\alpha\\) fail to reject the null Hypothesis, otherwise reject the null hypothesis 2. equivalent to if t-test value &lt; t-table fail to reject the null Hypothesis </p> <pre><code>control_group = experiment_data.loc[experiment_data['group']=='control','revenue_per_user'].values\ntreatment_group = experiment_data.loc[experiment_data['group']=='treatment','revenue_per_user'].values\n</code></pre> <pre><code>t_stat,p_val_t_test = stats.ttest_ind(treatment_group,control_group, equal_var=True, random_state=45, alternative='greater')\n</code></pre> <pre><code>p_val_t_test\n</code></pre> <pre><code>1.5326415297769846e-05\n</code></pre> <p>Proven that we can reject null hypothesis</p> <pre><code># Parameters\nn_samples = 2*1570\nalpha = 0.05\ndf = n_samples-2\ncritical_value = stats.t.ppf(1 - alpha, df)  # one tailed \n\n# t-distribution\nx = np.linspace(-4, 4, 1000) # generate some data \ny = stats.t.pdf(x, df)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label=f't-distribution with {df} degrees of freedom')\nplt.fill_between(x, y, where= (x &gt; critical_value), color='red', alpha=0.5, label='Rejection Region')\n\n# Add critical values\nplt.axvline(critical_value, color='black', linestyle='--', label=f'Critical Value: {critical_value:.3f}')\nplt.axvline(t_stat, color='green', linestyle='--', label=f'T statistic value')\n\n# Labels and title\nplt.title('Two-tailed t-test Rejection Region')\nplt.xlabel('t-value')\nplt.ylabel('pdf')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>Clearly we   reject the null hypothesis. Next Step we can generate confidence interval for our treatment effect using standard error </p> \\[ (\\bar{x}_{\\text{treatment}} - \\bar{x}_{\\text{control}} )  -  t_{\\alpha/2,df} \\cdot SE \\leq \\mu_{\\text{treatment}}- \\mu_{\\text{treatment}} \\leq (\\bar{x}_{\\text{treatment}} - \\bar{x}_{\\text{control}} )  +  t_{\\alpha/2,df} \\cdot SE\\] \\[SE = \\sqrt{(\\cfrac{s_1^2}{n_1} + \\cfrac{s_2^2}{n_2}})  \\] <p>with : </p> <ol> <li>\\(SE\\) : Standard error</li> <li>\\(s_1^2\\) : variance from group 1 , control group</li> <li>\\(s_2^2\\) : variance from group 2 , treatment group</li> <li>\\(n_1\\) : number of sample from group 1 , control group</li> <li>\\(n_2\\) : number of sample from group 2 , treatment group</li> <li>\\(t_{\\alpha/2 , df}\\) : t table value given alpha and degrees of freedom</li> </ol> <pre><code>difference = treatment_group.mean() - control_group.mean() \nn_1 = 1570 \nn_2 = 1570\ns_1 = control_group.var()\ns_2 = treatment_group.var()\n\nse = np.sqrt((s_1/n_1) + (s_2/n_2))\n\nt_alpha = stats.t.ppf(alpha/2, df=n_1+n_2-2)\n\n\nc_i = t_alpha*se\n\n\n\nprint(f'Effect Confidence Interval {difference} +/-{c_i}')\n</code></pre> <pre><code>Effect Confidence Interval 0.903699066036344 +/--0.4243055658869349\n</code></pre> <pre><code>c_i\n</code></pre> <pre><code>-0.4243055658869349\n</code></pre> <pre><code>difference\n</code></pre> <pre><code>0.903699066036344\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nplt.figure(figsize=(8, 6))\nplt.errorbar(y=1, x=difference, xerr=-c_i, fmt='o', ecolor='black', capsize=5, capthick=2, elinewidth=1, label='Revenue per User + CI')\nplt.vlines(x = 0, color = 'red',ymin = 0, ymax = 2, label = 'Statistically not Significant',linestyle='--')\nplt.vlines(x = 1, color = 'green',ymin = 0, ymax = 2, label = 'Practically Not Significant',linestyle='--')\n\nplt.title('Comparing Practically and Statistically Significant')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()\n</code></pre> <p></p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#5-inferring-decision","title":"5. Inferring Decision","text":"<p>We already reach the end of the experimentation. We have to decided whether to launch product recommendation system during checkout process or not to all users. </p> <p>To rollout this feature , the result should be :  1. Statistically Significant 2. Practically Significant</p> <p>From the Result it looks like only Statistical Significance . However the revenue per user still not fullfill our minimum USD 1 Revenue per user increase, so we have to decide not to launch the product.</p>"},{"location":"003-making%20your%20customer%20stay/Untitled/","title":"Untitled","text":"In\u00a0[\u00a0]: Copied!"}]}