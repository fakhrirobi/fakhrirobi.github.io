{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Latest Blog </p> <ul> <li>Learning to Rank on Travel Search Rank</li> <li>Measuring Impact of New Initiative with A/B Test</li> <li>Making Your Customer Stay</li> </ul>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/","title":"Hotel ranking","text":""},{"location":"001-Learning%20to%20Rank/hotel_ranking/#hotel-search-ranking-system","title":"Hotel Search Ranking System","text":"<p>Photo by SumUp on Unsplash</p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#background","title":"Background","text":""},{"location":"001-Learning%20to%20Rank/hotel_ranking/#ota-business-nature","title":"OTA Business Nature","text":"<p>OTA Company has two business models : </p> <ol> <li> <p>Merchant Model </p> <p>The company own what they sell. They have some flexibility in changing price. Their Revenues mostly from profit margin. </p> </li> <li> <p>Agency Model </p> <p>More like broker. Revenue from commision and fees. </p> </li> </ol> <p>Some of OTA Company which apply  Merchant Model : </p> <ul> <li>KAI </li> <li>AirAsia </li> <li>etc </li> </ul> <p>Some of OTA Company which apply Agency Model : </p> <ul> <li>AirBnB</li> <li>Booking.com </li> <li>Expedia</li> <li>Traveloka</li> <li>Tiket.com</li> </ul> <p>Let say there is OTA company , you can name <code>T.LLC</code>, their  problem is that : </p> <ul> <li>The Conversion has dropped 10% from last month </li> </ul> <p>As data scientist we should help business teams to fix the issue. After some analysis for example session replay + interview with product manager they said the customer does not convert when the recommendation show. </p> <p>There are several touchpoint of consumer conversion : </p> <ol> <li>Home Feed Recommendation </li> <li>Search Feature </li> </ol> <p>To provide which one to prioritize we should check the data where most of the conversion take place. Via Home Feed Recommendation or Search Feature. </p> <p>There is distinction between homefeed and search result. People which go to the search bar has already have intent to find hotel. Hypothetically the coversion should be bigger in search. </p> <p>Hence solving bigger portion cover bigger problem. </p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#objectives","title":"Objectives","text":"<p>Our objectives is to build ranking system / services that satisfy both business and ml objectives</p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#business-objective","title":"Business Objective","text":"<p>After all this ML Approach is initiative that aim to solve / optimize business metrics. </p> <p>What type of metrics we are going to optimize ? </p> <p>We need to know what is our business models. Its commision based model. </p> <ul> <li> <p>If number of bookings increase \u2192 company income increase </p> </li> <li> <p>So number of bookings right ? Why dont use conversion. </p> </li> </ul> <p>Of course we can. But we are not using conversion because it can be mislead if we dont mention the numerator </p> <p>Metrics : Total Bookings Increase by 20%</p> <p>\\(\\textbf{Total Booking} = \\text{Number of bookings from search activity}\\)</p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#ml-objective","title":"ML Objective","text":"<p>Its about what you expect from your ML System. </p> <p>Model Wise : </p> <ol> <li>Model Performance : <ol> <li>Can Beat Baseline Model in terms of NDCG (Previously 0.30)</li> </ol> </li> </ol> <p>Model Serving : </p> <ol> <li>Max Latency 50ms </li> <li>Search Experience is time sensitive</li> <li>A company like google already estimated that they apps latency increase by 100ms it affects to their revenue </li> </ol>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#user-flow","title":"User Flow","text":"<p>It's Important to Understand User Flow </p> <pre><code>\nflowchart TD\n    A(Customer Enter Keyword) --&gt; B(Add Query Parameter)\n    B --&gt; C(Customer Sent Search Request)\n    C --&gt; D(Customer Receive Search / Ranking Result)\n    D --&gt; |if interesting|E(Click)\n    D --&gt; | if not interesting |F(Skip / Scroll)\n    E --&gt; | if match |G(Book)\n</code></pre>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#system-design","title":"System Design","text":"<pre><code>\nflowchart TD\n    A[User Query] --&gt; B[Query Understanding Model]\n    B --&gt; C[Retrieval Generation Model]\n    C --&gt; D[Ranker Model] \n    D --&gt; E[Re Ranker Model]\n</code></pre> <p>In Eugene Yan's Blog Post Some Common System Design for Search and Recommendation could be decomposed into two components : </p> <ul> <li> <p>Retrieval / Candidate Generation </p> <p>Job : Filtering Out many items from 10.000 items to 1.000 items for example</p> </li> <li> <p>Ranking Stage </p> <p>Job : Ordering the rank , this is the heavy duty job. </p> </li> </ul> <p>Does all the the system design for search and recommender system should contain both ? No. Its depend on number of item you have in catalogue. The concern on this article is to highlight on Ranker Model Part</p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#model-development","title":"Model Development","text":""},{"location":"001-Learning%20to%20Rank/hotel_ranking/#problem-formulation","title":"Problem Formulation","text":"<p>Given User Query give ordered list of hotels that maximized chance user will book the hotel </p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#solution-1-no-model-approach","title":"Solution 1 : No Model Approach","text":"<p>The first stage is no ML at all. because if we just launch a product / company we dont have enough data, hence we can use simple heuristic such as : </p> <ol> <li>Sort by Name </li> <li>Sort by Price </li> <li>Ranking Randomly</li> </ol> <p>The goal is to obtain enough data to collect data to move on to the next step </p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#solution-2-modelling-approach","title":"Solution 2 : Modelling Approach","text":"<p>Our end task is ranking or to create ordering. </p> <p>Now let's do some first principle breakdown : </p> <p><code>How can we create ranking</code> : </p> <ol> <li>Predict the score of each item</li> <li>Then Order the item based on the score </li> </ol> <p>To produce the score is straight forward, to produce the ordering we want each item to be different in score not in discrete. </p> <p>For example we could use predicted probability or ranking as starter. </p> <p> Source : LucidWorks</p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#data-description","title":"Data Description","text":""},{"location":"001-Learning%20to%20Rank/hotel_ranking/#data-source","title":"Data Source","text":"<p>The Data is obtained from Personalize Expedia Hotel Searches - ICDM 2013 Kaggle Competition</p> <p>So in ml based solution it requires data. What type of Information we show. Air BnB in their Article  can classify the feature as : </p> <ol> <li>Hotels / Listing Feature</li> <li>User / Personalized Features </li> <li>Query Features  </li> </ol> <p>Here is the idea : </p> <ol> <li>If we use Hotel Features Only \u2192 it can produce ranking but we cannot expect variation in user level. If we compare recommendation of two users. Say A and B the recommendation would be the same. </li> <li>To add more personalization we can add feature that unique to user level </li> <li>To what extend its unique, if user characteristics does not change then the recommendation would be the same. However that is not ideal because user intention may be different at different search session </li> </ol>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#listing-hotel-features","title":"Listing / Hotel Features","text":"<ul> <li> <p>It could be feature that describe facility , for example : </p> <ul> <li>Number of rooms available between dates </li> <li>Price </li> <li>Location </li> <li>etc </li> </ul> </li> <li> <p>For now there are lot of companies using embeddings to express the feature of an entity </p> </li> <li> <p>Usually embeddings are resulted from pretrained model (neural nets for example)</p> </li> </ul> <p>There are lot of variables that classify as Listing / Property Features, however several consideration is that : </p> <ul> <li>The feature should survive cold start problem </li> </ul>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#cold-start-problem","title":"Cold Start Problem","text":"<p>Why we care about new item / new user ? </p> <p>New Item / Property </p> <ul> <li>It affects revenue of property, </li> <li>We want our search / recommendation is fair towards property </li> </ul> <p>New User : </p> <ul> <li>Help user build interaction </li> <li>Have great search experience by providing relevant property </li> </ul> <p>To properly handle the cold start problem, we research on how to handle it. To do so , we survey several reference (cited in References Section) We can divide solution into : </p> <ul> <li> <p>Feature Based  Solution  : </p> </li> <li> <p>Add Feature that are robust to Cold Start Feature , for example : </p> <ul> <li>User Demographic Features e.g. Age, Gender , etc</li> <li>Listing Demographic Features e.g. price, lot size , facilities, etc </li> </ul> </li> <li> <p>Fill Default Values from Local / Geographic </p> </li> <li> <p>Estimate Property Features from Similar Property (AirBnB)</p> </li> <li> <p>Estimate Property Engagement Features (AirBnB)</p> </li> <li> <p>Model Based Solution : </p> <ul> <li>Develop 2 Model for User : </li> <li>New User       Need to define what is new user, for example, new user could be user that has not booked any property </li> <li>Old User </li> </ul> </li> </ul> Column Type Description Cold Start Cold Start Strategy <code>prop_id</code> Int Hotel property identifier <code>prop_starrating</code> Float Hotel star rating <code>prop_log_historical_price</code> Float Log Historical Price <code>price_usd</code> Float Display price in USD <code>promotion_flag</code> Bool 1 if the hotel was under promotion <pre><code>PROPERTY_FEATURES = [\n    'prop_starrating', \n    'price_usd',\n    'prop_log_historical_price',\n    'promotion_flag'\n] \n</code></pre>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#user-personal-features","title":"User Personal Features","text":"<p>More Like User Preferences (Could be Implicit or Explicit), Past Bookings(If Available) for examples :  - Past Bookings (1w,1month,3month,etc) - Average Stay  - etc </p> <p>Okay, you might say what would happen if user is new ? AirBnB in their paper build separate model for login user and logout user. We could categorize new user as logout user due to the fact that the data is not available enough </p> Column Type Description <code>visitor_location_country_id</code> Int Customer\u2019s country ID <code>visitor_hist_starrating</code> Float User\u2019s historical average hotel star\u2011rating <code>visitor_hist_adr_usd</code> Float User\u2019s historical average daily rate (USD) <pre><code>\n</code></pre> <pre><code>USER_FEATURES = [\n    'visitor_hist_starrating', \n    'visitor_hist_adr_usd'\n] \n</code></pre>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#search-features","title":"Search Features","text":"<p>What does Query Features Mean ? article Features that describe criteria for hotels we are looking for , for example : </p> <ul> <li>Number of guests </li> <li>Number of Stay </li> <li>etc </li> </ul> <p>The same goes with expedia in their article</p> <p>which include :  - checkin_date  - checkout_date </p> <ul> <li>We could also  represent this as embedding</li> </ul> <p>Here is the feature definition from dataset</p> Column Type Description <code>srch_destination_id</code> Int Destination ID for the search <code>srch_length_of_stay</code> Int Number of nights (check\u2011out minus check\u2011in) <code>srch_adults_count</code> Int Number of adults <code>srch_children_count</code> Int Number of children <code>srch_room_count</code> Int Number of rooms <code>srch_saturday_night_bool</code> Bool 1 if stay includes a Saturday night <code>orig_destination_distance</code> Float Distance between user and hotel (km) <pre><code>SEARCH_FEATURES = [\n    'srch_length_of_stay', \n    'srch_adults_count',\n    'srch_room_count', \n    'srch_saturday_night_bool',\n    'orig_destination_distance',\n    'search_rank' # need to be generated \n] \n</code></pre>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#click-log","title":"Click Log","text":"<p>Last but not least we need the data from search activity the purpose is to train model to maximize the chance of booking happened.  The search process happen in whats called as Session. Each User can have multiple session </p> Column Type Description <code>click_bool</code> Bool 1 if the listing was clicked <code>booking_bool</code> Bool 1 if the listing was booked"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#load-data","title":"Load Data","text":"<pre><code>import pandas as pd \nimport numpy as np\n\nfrom IPython.display import display\n\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\n\nplt.style.use('seaborn-v0_8-pastel')\n</code></pre> <p>Due to the huge size of original data, we decide to sample the data based on  search id. We sampled only 50% of search data resulting 1.3 GB of data. </p> <pre><code>dataset = pd.read_csv('expedia_search_sampled.csv')\ndataset.head()\n</code></pre> Show Code Output <p> srch_id date_time site_id visitor_location_country_id visitor_hist_starrating visitor_hist_adr_usd prop_country_id prop_id prop_starrating prop_review_score ... comp6_rate_percent_diff comp7_rate comp7_inv comp7_rate_percent_diff comp8_rate comp8_inv comp8_rate_percent_diff click_bool gross_bookings_usd booking_bool 0 4 2012-12-31 08:59:22 5 219 NaN NaN 219 3625 4 4.0 ... NaN NaN NaN NaN NaN NaN NaN 0 NaN 0 1 4 2012-12-31 08:59:22 5 219 NaN NaN 219 11622 4 4.0 ... NaN NaN NaN NaN NaN NaN NaN 0 NaN 0 2 4 2012-12-31 08:59:22 5 219 NaN NaN 219 11826 5 4.5 ... NaN NaN NaN NaN NaN NaN NaN 0 NaN 0 3 4 2012-12-31 08:59:22 5 219 NaN NaN 219 22824 3 4.0 ... NaN NaN NaN NaN NaN NaN NaN 0 NaN 0 4 4 2012-12-31 08:59:22 5 219 NaN NaN 219 37581 5 4.5 ... NaN NaN NaN NaN NaN NaN NaN 0 NaN 0 <p>5 rows \u00d7 54 columns</p> </p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#generate-event","title":"Generate Event","text":"<p>What the data should look like:</p> search_session_id listing_id event_time event_type 1 3914 2022-10-01\u00a017:04:19 click 1 3352 2022-10-01\u00a017:04:30 click 1 2575 2022-10-01\u00a017:04:36 click 1 4257 2022-10-01\u00a017:04:46 click 1 3071 2022-10-01\u00a017:05:03 click 1 1757 2022-10-01\u00a017:05:22 click 1 1828 2022-10-01\u00a017:05:34 scroll 1 3492 2022-10-01\u00a017:05:53 click <p>Each unique session : </p> <ol> <li>occur at a particular time </li> <li>has n interactions </li> <li>the interactions should be ordered chronologically </li> </ol> <p>We should order the data first , based on : </p> <ol> <li>search session </li> <li>event time </li> </ol> <pre><code>dataset = dataset.sort_values( \n    ['srch_id','date_time']\n)\ndataset.head(2)\n</code></pre> Show Code Output <p> srch_id date_time site_id visitor_location_country_id visitor_hist_starrating visitor_hist_adr_usd prop_country_id prop_id prop_starrating prop_review_score ... comp6_rate_percent_diff comp7_rate comp7_inv comp7_rate_percent_diff comp8_rate comp8_inv comp8_rate_percent_diff click_bool gross_bookings_usd booking_bool 0 4 2012-12-31 08:59:22 5 219 NaN NaN 219 3625 4 4.0 ... NaN NaN NaN NaN NaN NaN NaN 0 NaN 0 1 4 2012-12-31 08:59:22 5 219 NaN NaN 219 11622 4 4.0 ... NaN NaN NaN NaN NaN NaN NaN 0 NaN 0 <p>2 rows \u00d7 54 columns</p> </p> <pre><code>session_id = 4\n\nsample_session = dataset.loc[\n    dataset['srch_id']==session_id,\n    ['date_time','prop_id','click_bool','booking_bool']\n]\n\nsample_session\n</code></pre> Show Code Output <p> date_time prop_id click_bool booking_bool 0 2012-12-31 08:59:22 3625 0 0 1 2012-12-31 08:59:22 11622 0 0 2 2012-12-31 08:59:22 11826 0 0 3 2012-12-31 08:59:22 22824 0 0 4 2012-12-31 08:59:22 37581 0 0 5 2012-12-31 08:59:22 39993 0 0 6 2012-12-31 08:59:22 46162 0 0 7 2012-12-31 08:59:22 49152 0 0 8 2012-12-31 08:59:22 56063 0 0 9 2012-12-31 08:59:22 56472 0 0 10 2012-12-31 08:59:22 58696 0 0 11 2012-12-31 08:59:22 64344 0 0 12 2012-12-31 08:59:22 65984 0 0 13 2012-12-31 08:59:22 71258 0 0 14 2012-12-31 08:59:22 75491 0 0 15 2012-12-31 08:59:22 81172 0 0 16 2012-12-31 08:59:22 83045 0 0 17 2012-12-31 08:59:22 83430 0 0 18 2012-12-31 08:59:22 83806 0 0 19 2012-12-31 08:59:22 85567 0 0 20 2012-12-31 08:59:22 85742 0 0 21 2012-12-31 08:59:22 89119 0 0 22 2012-12-31 08:59:22 97099 0 0 23 2012-12-31 08:59:22 109185 0 0 24 2012-12-31 08:59:22 110813 0 0 25 2012-12-31 08:59:22 116696 0 0 26 2012-12-31 08:59:22 125069 0 0 27 2012-12-31 08:59:22 127808 0 0 28 2012-12-31 08:59:22 129278 0 0 29 2012-12-31 08:59:22 134162 0 0 30 2012-12-31 08:59:22 137826 0 0 31 2012-12-31 08:59:22 139893 1 0 </p> <p>What we can infer or how to read the dataset ? </p> <ul> <li>a user in a search session scrolled from <code>prop_id=3625</code> and clicked at <code>prop_id=139893</code>. However the user did not end up booking the property.</li> </ul> <p>Our dataset is not look like what we are expected its because the click and bookings are still in different columns hence we need to make single column responsible for managing event_type</p> <pre><code>def generate_event(dataset:pd.DataFrame) -&gt; pd.DataFrame : \n    # if booking = 1 --&gt; event_type --&gt; 'booked' \n    # if click == 1 --&gt; event_type --&gt; 'clicked' \n    # else : --&gt; 'scrolled'\n\n    if dataset['booking_bool'] == 1 : \n        return 'booked'\n    elif dataset['click_bool'] == 1 : \n        return 'clicked'\n    else : \n        return 'scrolled'\n</code></pre> <pre><code>dataset['event'] = dataset.apply(generate_event,axis=1)\n</code></pre> <p>Okay, next sanity check </p> <pre><code>dataset['event'].unique()\n</code></pre> <pre><code>array(['scrolled', 'clicked', 'booked'], dtype=object)\n</code></pre> <p>no missing values, meaning successfully mapped</p> <pre><code>sample_session = dataset.loc[\n    dataset['srch_id']==session_id,\n    ['date_time','prop_id','click_bool','booking_bool','event']\n]\n\nsample_session\n</code></pre> Show Code Output <p> date_time prop_id click_bool booking_bool event 0 2012-12-31 08:59:22 3625 0 0 scrolled 1 2012-12-31 08:59:22 11622 0 0 scrolled 2 2012-12-31 08:59:22 11826 0 0 scrolled 3 2012-12-31 08:59:22 22824 0 0 scrolled 4 2012-12-31 08:59:22 37581 0 0 scrolled 5 2012-12-31 08:59:22 39993 0 0 scrolled 6 2012-12-31 08:59:22 46162 0 0 scrolled 7 2012-12-31 08:59:22 49152 0 0 scrolled 8 2012-12-31 08:59:22 56063 0 0 scrolled 9 2012-12-31 08:59:22 56472 0 0 scrolled 10 2012-12-31 08:59:22 58696 0 0 scrolled 11 2012-12-31 08:59:22 64344 0 0 scrolled 12 2012-12-31 08:59:22 65984 0 0 scrolled 13 2012-12-31 08:59:22 71258 0 0 scrolled 14 2012-12-31 08:59:22 75491 0 0 scrolled 15 2012-12-31 08:59:22 81172 0 0 scrolled 16 2012-12-31 08:59:22 83045 0 0 scrolled 17 2012-12-31 08:59:22 83430 0 0 scrolled 18 2012-12-31 08:59:22 83806 0 0 scrolled 19 2012-12-31 08:59:22 85567 0 0 scrolled 20 2012-12-31 08:59:22 85742 0 0 scrolled 21 2012-12-31 08:59:22 89119 0 0 scrolled 22 2012-12-31 08:59:22 97099 0 0 scrolled 23 2012-12-31 08:59:22 109185 0 0 scrolled 24 2012-12-31 08:59:22 110813 0 0 scrolled 25 2012-12-31 08:59:22 116696 0 0 scrolled 26 2012-12-31 08:59:22 125069 0 0 scrolled 27 2012-12-31 08:59:22 127808 0 0 scrolled 28 2012-12-31 08:59:22 129278 0 0 scrolled 29 2012-12-31 08:59:22 134162 0 0 scrolled 30 2012-12-31 08:59:22 137826 0 0 scrolled 31 2012-12-31 08:59:22 139893 1 0 clicked </p> <p>Okay the result correct, next we should check on the session that is in booking</p> <pre><code>booked_session_id = dataset.loc[ \n    dataset['booking_bool'] == 1 ,'srch_id'\n].unique()[0]\nprint('Sample of Booked Session, srch_id = ',booked_session_id)\n</code></pre> <pre><code>Sample of Booked Session, srch_id =  8\n</code></pre> <pre><code>booked_session = dataset.loc[\n    dataset['srch_id']==booked_session_id,\n    ['date_time','prop_id','click_bool','booking_bool','event']\n]\n\nbooked_session\n</code></pre> Show Code Output <p> date_time prop_id click_bool booking_bool event 32 2013-03-20 17:50:44 10250 0 0 scrolled 33 2013-03-20 17:50:44 13252 0 0 scrolled 34 2013-03-20 17:50:44 22756 0 0 scrolled 35 2013-03-20 17:50:44 27669 1 1 booked 36 2013-03-20 17:50:44 30630 0 0 scrolled 37 2013-03-20 17:50:44 32491 0 0 scrolled 38 2013-03-20 17:50:44 33805 0 0 scrolled 39 2013-03-20 17:50:44 35397 0 0 scrolled 40 2013-03-20 17:50:44 39137 0 0 scrolled 41 2013-03-20 17:50:44 41497 0 0 scrolled 42 2013-03-20 17:50:44 54403 0 0 scrolled 43 2013-03-20 17:50:44 58176 0 0 scrolled 44 2013-03-20 17:50:44 63511 0 0 scrolled 45 2013-03-20 17:50:44 66444 0 0 scrolled 46 2013-03-20 17:50:44 67150 0 0 scrolled 47 2013-03-20 17:50:44 73738 0 0 scrolled 48 2013-03-20 17:50:44 83293 0 0 scrolled 49 2013-03-20 17:50:44 90151 0 0 scrolled 50 2013-03-20 17:50:44 119302 0 0 scrolled 51 2013-03-20 17:50:44 125655 0 0 scrolled 52 2013-03-20 17:50:44 134607 0 0 scrolled </p> <p>Okay, it correctly annotate the <code>booked</code> event . Something interesting is that event after the booked happen the result still appear. We should check if the data also display recommended item but not clicked</p> <pre><code>dataset.groupby('srch_id').agg({'prop_id':'count'}).sort_values('prop_id')\n</code></pre> Show Code Output <p> prop_id srch_id 144590 5 327375 5 159295 5 474323 5 66599 5 ... ... 560306 37 60367 37 394061 38 524634 38 597375 38 <p>157267 rows \u00d7 1 columns</p> </p> <pre><code>display(dataset.loc[ \n    dataset['srch_id'] ==144590,['date_time','prop_id','click_bool','booking_bool','event']\n])\ndisplay(dataset.loc[ \n    dataset['srch_id'] ==597375,['date_time','prop_id','click_bool','booking_bool','event']\n])\n</code></pre> Show Code Output <p> date_time prop_id click_bool booking_bool event 847859 2013-03-24 15:34:50 34327 0 0 scrolled 847860 2013-03-24 15:34:50 45505 0 0 scrolled 847861 2013-03-24 15:34:50 49597 1 1 booked 847862 2013-03-24 15:34:50 61770 1 0 clicked 847863 2013-03-24 15:34:50 79426 1 0 clicked </p> <p> date_time prop_id click_bool booking_bool event 3508390 2013-06-26 22:33:27 499 0 0 scrolled 3508391 2013-06-26 22:33:27 2998 0 0 scrolled 3508392 2013-06-26 22:33:27 6268 0 0 scrolled 3508393 2013-06-26 22:33:27 21838 1 1 booked 3508394 2013-06-26 22:33:27 26195 0 0 scrolled 3508395 2013-06-26 22:33:27 29233 0 0 scrolled 3508396 2013-06-26 22:33:27 38587 0 0 scrolled 3508397 2013-06-26 22:33:27 40117 0 0 scrolled 3508398 2013-06-26 22:33:27 45094 0 0 scrolled 3508399 2013-06-26 22:33:27 45834 0 0 scrolled 3508400 2013-06-26 22:33:27 62095 0 0 scrolled 3508401 2013-06-26 22:33:27 67553 0 0 scrolled 3508402 2013-06-26 22:33:27 68293 0 0 scrolled 3508403 2013-06-26 22:33:27 71357 0 0 scrolled 3508404 2013-06-26 22:33:27 75017 0 0 scrolled 3508405 2013-06-26 22:33:27 79474 0 0 scrolled 3508406 2013-06-26 22:33:27 82547 0 0 scrolled 3508407 2013-06-26 22:33:27 83018 0 0 scrolled 3508408 2013-06-26 22:33:27 85294 0 0 scrolled 3508409 2013-06-26 22:33:27 86224 0 0 scrolled 3508410 2013-06-26 22:33:27 86511 0 0 scrolled 3508411 2013-06-26 22:33:27 88144 1 0 clicked 3508412 2013-06-26 22:33:27 91550 0 0 scrolled 3508413 2013-06-26 22:33:27 94975 0 0 scrolled 3508414 2013-06-26 22:33:27 102111 0 0 scrolled 3508415 2013-06-26 22:33:27 102135 0 0 scrolled 3508416 2013-06-26 22:33:27 102943 0 0 scrolled 3508417 2013-06-26 22:33:27 103972 0 0 scrolled 3508418 2013-06-26 22:33:27 104198 0 0 scrolled 3508419 2013-06-26 22:33:27 106449 0 0 scrolled 3508420 2013-06-26 22:33:27 110143 0 0 scrolled 3508421 2013-06-26 22:33:27 117992 0 0 scrolled 3508422 2013-06-26 22:33:27 123354 0 0 scrolled 3508423 2013-06-26 22:33:27 123978 0 0 scrolled 3508424 2013-06-26 22:33:27 124413 0 0 scrolled 3508425 2013-06-26 22:33:27 131611 0 0 scrolled 3508426 2013-06-26 22:33:27 132136 0 0 scrolled 3508427 2013-06-26 22:33:27 139340 0 0 scrolled </p> <p>We can see number of property in each session is different meaning it fully represent user behaviour in recommendation. If the records only 5 property means user only interact (scroll , click , book ) up to 5<sup>th</sup> position.  Another case is that after booking user still can possibly click item. </p> <p>Next, what we should do is to calculate Generate Search Rank</p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#adding-search-rank","title":"Adding Search Rank","text":"<p>Next step is to create search ranking feature from recommendation to mitigae positional bias </p> <p>we will add new column <code>search_rank</code> to define ranking result from recommendation. </p> <p>However we have to sure our data really reflect the ordering from current search system. when outputing search result the rank also should be logged. </p> <p>The assumption is that :  - In Company <code>T.LLC</code> System if we jump from 1<sup>st</sup> search result  to 5<sup>th</sup> position the rank 1 to 4 regarded as <code>scroll</code> </p> listing_id event_time event_type search rank 3914 2022-10-01 17:04:19 click 1 3352 2022-10-01 17:04:30 click 2 2575 2022-10-01 17:04:36 click 3 4257 2022-10-01 17:04:46 click 4 3071 2022-10-01 17:05:03 click 5 1757 2022-10-01 17:05:22 click 6 1828 2022-10-01 17:05:34 scroll 7 3494 2022-10-01 17:05:53 click 8 2341 2022-10-01 17:06:05 click 9 438 2022-10-01 17:06:11 scroll 10 1757 2022-10-01 17:06:27 book 11 <p>Next, we need to create those label </p> <pre><code>dataset['search_rank'] = dataset.groupby('srch_id').cumcount() + 1\n</code></pre> <pre><code>display(dataset.loc[ \n    dataset['srch_id'] ==144590,['date_time','prop_id','click_bool','booking_bool','event','search_rank']\n])\n</code></pre> Show Code Output <p> date_time prop_id click_bool booking_bool event search_rank 847859 2013-03-24 15:34:50 34327 0 0 scrolled 1 847860 2013-03-24 15:34:50 45505 0 0 scrolled 2 847861 2013-03-24 15:34:50 49597 1 1 booked 3 847862 2013-03-24 15:34:50 61770 1 0 clicked 4 847863 2013-03-24 15:34:50 79426 1 0 clicked 5 </p> <p>After that we need to create / calculate Relevance Score</p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#calculate-relevance-score","title":"Calculate Relevance Score","text":"<p>Purpose \u2192 To evaluate how well our search model produce the ordering </p> <p>The score could be created in a way such that some interaction scored higher. According to original kaggle competition the score for each event type is : </p> <ul> <li><code>Book</code> : <code>5</code></li> <li><code>Click</code> : <code>1</code> </li> <li><code>Scroll</code> : <code>0</code> </li> </ul> <pre><code>dataset['relevance_score'] = dataset['event'].map({\n    'scrolled' : 0 , \n    'clicked' : 1 , \n    'booked'  : 5 \n})\n</code></pre> <p>Sanity Check </p> <pre><code>dataset['relevance_score'].value_counts()\n</code></pre> <pre><code>relevance_score\n0    3732736\n5     108884\n1      65565\nName: count, dtype: int64\n</code></pre>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#crafting-training-data","title":"Crafting Training Data","text":"<p>How to construct training pair :  - We need to ensure there is book in a query to make sure the model learn the labels / relevance score  - We need to substract the training pair which does not contain <code>event_type = book</code>.  - Why ? Some of the learning models work in query level. </p> <p>Here is a glimpse of how our data would look like</p> srch_id X (features) y (relevance score) <p>There are variety of design type to construct dataset. Choosing this method also will affect how model learn : </p> <ul> <li>Filter only search id contain at least 1 booking </li> <li>Not Filtering search id </li> </ul> <pre><code># get query id which contain `event==book`\nbook_session_id = dataset.loc[ \n    dataset['event'] == 'booked','srch_id'\n].unique()\n\nprint('Before filtering search_id with bookings, Num of search id = ',dataset.srch_id.nunique())\nprint('After filtering search_id with bookings, Num of search id = ',len(book_session_id))\n</code></pre> <pre><code>Before filtering search_id with bookings, Num of search id =  157267\nAfter filtering search_id with bookings, Num of search id =  108884\n</code></pre> <pre><code>prev_shape = dataset.shape \ndataset = dataset.loc[ \n    dataset['srch_id'].isin(book_session_id)\n]\nprint('Before filtering search_id with bookings, Num of Records = ',prev_shape[0])\nprint('After filtering search_id with bookings, Num of Records = ',dataset.shape[0])\n</code></pre> <pre><code>Before filtering search_id with bookings, Num of Records =  3907185\nAfter filtering search_id with bookings, Num of Records =  2727065\n</code></pre> <p>To create training data, we should list all feature combination </p> <pre><code>ALL_FEATURES = USER_FEATURES + PROPERTY_FEATURES + SEARCH_FEATURES \n\nprint('Number of Features = ',len(ALL_FEATURES) )\n</code></pre> <pre><code>Number of Features =  11\n</code></pre>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#split-data","title":"Split Data","text":"<p>How to split data ? </p> <p>It depends on ranking metrics. Our ranking metrics compare relevance score in actual vs predicted (from our model). </p> <p>to obtain actual ranking , we need to obtain to each query. Hence our split process is quite different from random subsetting where we pick random for example 80% of data. </p> <p>Our split unit is search id </p> <pre><code>book_session_id\n</code></pre> <pre><code>array([     8,     25,     28, ..., 665554, 665562, 665574],\n      shape=(108884,))\n</code></pre> <pre><code># Split training data into Train and Test Data\ntrain_idx = int(0.8 * (len(book_session_id)))\nvalid_idx = int(0.9 * (len(book_session_id)))\n\nsearch_id_train = book_session_id[:train_idx]\nsearch_id_val = book_session_id[train_idx:valid_idx]\nsearch_id_test = book_session_id[valid_idx:]\n\nprint('Number of Training search_id : ',len(search_id_train))\nprint('Number of Validation search_id : ',len(search_id_val))\nprint('Number of Test search_id : ',len(search_id_test))\n</code></pre> <pre><code>Number of Training search_id :  87107\nNumber of Validation search_id :  10888\nNumber of Test search_id :  10889\n</code></pre> <pre><code># split X &amp; y \nX_train_relevance = dataset.loc[dataset['srch_id'].isin(search_id_train)]\nqid_train = dataset.loc[dataset['srch_id'].isin(search_id_train),\n                          'srch_id' ]\n\n\ny_train_relevance = dataset.loc[dataset['srch_id'].isin(search_id_train),\n                          'relevance_score']\n\nX_val_relevance = dataset.loc[dataset['srch_id'].isin(search_id_val)]\nqid_val = dataset.loc[dataset['srch_id'].isin(search_id_val),\n                          'srch_id' ]\ny_val_relevance = dataset.loc[dataset['srch_id'].isin(search_id_val),\n                          'relevance_score']\n\nX_test_relevance = dataset.loc[dataset['srch_id'].isin(search_id_test)]\nqid_test = dataset.loc[dataset['srch_id'].isin(search_id_test),\n                          'srch_id'  ]\ny_test_relevance = dataset.loc[dataset['srch_id'].isin(search_id_test),\n                          'relevance_score']\n</code></pre> <pre><code>def split_data(dataset,\n                train_frac,\n                unique_query_id,\n                query_col,\n                relevance_col) :\n\n    \"\"\"\n    Function to split data \n\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    # obtain index \n    train_idx = int(train_frac * (len(unique_query_id)))\n    val_pos = (1 - train_frac)/2\n    valid_idx = int( val_pos* (len(unique_query_id))) + train_idx\n\n\n    # obtain unique query \n    query_id_train = unique_query_id[:train_idx]\n    query_id_val = unique_query_id[train_idx:valid_idx]\n    query_id_test = unique_query_id[valid_idx:]\n\n\n\n    X_train_relevance = dataset.loc[dataset[query_col].isin(query_id_train), :  ]\n    query_train = dataset.loc[dataset[query_col].isin(query_id_train),\n    query_col ]\n\n    y_train_relevance = dataset.loc[dataset[query_col].isin(query_id_train),\n    relevance_col]\n\n    X_val_relevance = dataset.loc[dataset[query_col].isin(query_id_val),\n    :]\n    query_val = dataset.loc[dataset[query_col].isin(query_id_val),\n    query_col ]\n    y_val_relevance = dataset.loc[dataset[query_col].isin(query_id_val),\n    relevance_col]\n\n    X_test_relevance = dataset.loc[dataset[query_col].isin(query_id_test),\n    :]\n\n    query_test = dataset.loc[dataset[query_col].isin(query_id_test),\n    query_col ]\n    y_test_relevance = dataset.loc[dataset[query_col].isin(query_id_test),\n    relevance_col]\n\n    split = {\n    'train' : (X_train_relevance,y_train_relevance,query_train),\n    'val' : (X_val_relevance,y_val_relevance,query_val),\n    'test' : (X_test_relevance,y_test_relevance,query_test)\n    }\n\n    return split\n</code></pre>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#eda","title":"EDA","text":"<p>Some EDA wont hurt , right ? </p> <p>Question / Focus of EDA : </p> <ul> <li>Number of event per search activity</li> <li>Check Position that most booking occur </li> <li>Check whether there is missing value in each feature </li> </ul>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#number-of-missing-values","title":"Number of Missing Values","text":"<pre><code>display((X_train_relevance.isnull().sum() / X_train_relevance.shape[0] ).sort_values(ascending=False))\n</code></pre> Show Code Output <ul> <li>comp1_rate_percent_diff        0.981806</li> <li>comp6_rate_percent_diff        0.979394</li> <li>comp1_rate                     0.976894</li> <li>comp1_inv                      0.975163</li> <li>comp4_rate_percent_diff        0.973210</li> <li>comp7_rate_percent_diff        0.970401</li> <li>gross_bookings_usd             0.960052</li> <li>comp6_rate                     0.948188</li> <li>comp6_inv                      0.943810</li> <li>comp4_rate                     0.937723</li> <li>visitor_hist_starrating        0.936747</li> <li>visitor_hist_adr_usd           0.936399</li> <li>comp7_rate                     0.932321</li> <li>comp4_inv                      0.930634</li> <li>comp7_inv                      0.923850</li> <li>srch_query_affinity_score      0.921880</li> <li>comp3_rate_percent_diff        0.896652</li> <li>comp2_rate_percent_diff        0.880073</li> <li>comp8_rate_percent_diff        0.865494</li> <li>comp5_rate_percent_diff        0.820446</li> <li>comp3_rate                     0.669075</li> <li>comp3_inv                      0.644406</li> <li>comp8_rate                     0.584797</li> <li>comp8_inv                      0.569799</li> <li>comp2_rate                     0.568846</li> <li>comp2_inv                      0.546673</li> <li>comp5_rate                     0.529495</li> <li>comp5_inv                      0.501489</li> <li>orig_destination_distance      0.314939</li> <li>prop_location_score2           0.217862</li> <li>prop_review_score              0.001281</li> <li>event                          0.000000</li> <li>booking_bool                   0.000000</li> <li>search_rank                    0.000000</li> <li>click_bool                     0.000000</li> <li>srch_id                        0.000000</li> <li>date_time                      0.000000</li> <li>position                       0.000000</li> <li>site_id                        0.000000</li> <li>visitor_location_country_id    0.000000</li> <li>prop_country_id                0.000000</li> <li>prop_id                        0.000000</li> <li>prop_starrating                0.000000</li> <li>prop_brand_bool                0.000000</li> <li>prop_location_score1           0.000000</li> <li>prop_log_historical_price      0.000000</li> <li>price_usd                      0.000000</li> <li>random_bool                    0.000000</li> <li>promotion_flag                 0.000000</li> <li>srch_destination_id            0.000000</li> <li>srch_length_of_stay            0.000000</li> <li>srch_booking_window            0.000000</li> <li>srch_adults_count              0.000000</li> <li>srch_children_count            0.000000</li> <li>srch_room_count                0.000000</li> <li>srch_saturday_night_bool       0.000000</li> <li>relevance_score                0.000000 dtype: float64</li> </ul> <p>We can see that most of the user feature is not available, hence we can update the decision not to include the feature. For now we are not interested in imputation technique. you can check other blogs. This article point out the wholistic pipeline of learning to rank in application to OTA search particularly in hotel / property ranking</p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#position-that-most-booking-occur","title":"Position that most booking occur","text":"<pre><code>bookings_event_train = X_train_relevance.loc[ \n    X_train_relevance['event'] == 'booked'\n]\n</code></pre> <pre><code>sns.histplot(data = bookings_event_train,\n             x = 'search_rank')\nplt.xlabel('search rank position')\nplt.title('Search Position Distribution Where Bookings Occur')\n</code></pre> <pre><code>Text(0.5, 1.0, 'Search Position Distribution Where Bookings Occur')\n</code></pre> <p>There is bias in bookings, where most of bookings occur at top position, hence we need to add position as feature </p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#number-of-activity-occur-within-search","title":"Number of Activity Occur Within Search","text":"<pre><code>search_activity_count = X_train_relevance.groupby('srch_id',as_index=False).agg({\n    'prop_id' : 'count'\n})\nsearch_activity_count = search_activity_count.rename(columns={'prop_id':'event_count'})\nsearch_activity_count\n\n\nsns.histplot(data = search_activity_count,\n             x = 'event_count')\nplt.xlabel('Number of Event')\nplt.title('Distibution of Number of Event from each search')\n</code></pre> <pre><code>Text(0.5, 1.0, 'Distibution of Number of Event from each search')\n</code></pre>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#training-model","title":"Training Model","text":"<p>Previously we checked some features and we decide to remove some of them. </p> <pre><code>display((X_train_relevance.isnull().sum() / X_train_relevance.shape[0] ).sort_values(ascending=False))\n</code></pre> Show Code Output <ul> <li>comp1_rate_percent_diff        0.981806</li> <li>comp6_rate_percent_diff        0.979394</li> <li>comp1_rate                     0.976894</li> <li>comp1_inv                      0.975163</li> <li>comp4_rate_percent_diff        0.973210</li> <li>comp7_rate_percent_diff        0.970401</li> <li>gross_bookings_usd             0.960052</li> <li>comp6_rate                     0.948188</li> <li>comp6_inv                      0.943810</li> <li>comp4_rate                     0.937723</li> <li>visitor_hist_starrating        0.936747</li> <li>visitor_hist_adr_usd           0.936399</li> <li>comp7_rate                     0.932321</li> <li>comp4_inv                      0.930634</li> <li>comp7_inv                      0.923850</li> <li>srch_query_affinity_score      0.921880</li> <li>comp3_rate_percent_diff        0.896652</li> <li>comp2_rate_percent_diff        0.880073</li> <li>comp8_rate_percent_diff        0.865494</li> <li>comp5_rate_percent_diff        0.820446</li> <li>comp3_rate                     0.669075</li> <li>comp3_inv                      0.644406</li> <li>comp8_rate                     0.584797</li> <li>comp8_inv                      0.569799</li> <li>comp2_rate                     0.568846</li> <li>comp2_inv                      0.546673</li> <li>comp5_rate                     0.529495</li> <li>comp5_inv                      0.501489</li> <li>orig_destination_distance      0.314939</li> <li>prop_location_score2           0.217862</li> <li>prop_review_score              0.001281</li> <li>event                          0.000000</li> <li>booking_bool                   0.000000</li> <li>search_rank                    0.000000</li> <li>click_bool                     0.000000</li> <li>srch_id                        0.000000</li> <li>date_time                      0.000000</li> <li>position                       0.000000</li> <li>site_id                        0.000000</li> <li>visitor_location_country_id    0.000000</li> <li>prop_country_id                0.000000</li> <li>prop_id                        0.000000</li> <li>prop_starrating                0.000000</li> <li>prop_brand_bool                0.000000</li> <li>prop_location_score1           0.000000</li> <li>prop_log_historical_price      0.000000</li> <li>price_usd                      0.000000</li> <li>random_bool                    0.000000</li> <li>promotion_flag                 0.000000</li> <li>srch_destination_id            0.000000</li> <li>srch_length_of_stay            0.000000</li> <li>srch_booking_window            0.000000</li> <li>srch_adults_count              0.000000</li> <li>srch_children_count            0.000000</li> <li>srch_room_count                0.000000</li> <li>srch_saturday_night_bool       0.000000</li> <li>relevance_score                0.000000 dtype: float64</li> </ul> <pre><code>PROPERTY_FEATURES = [\n    'prop_starrating', \n    'price_usd',\n    'prop_log_historical_price',\n    'promotion_flag'\n] \n\nSEARCH_FEATURES = [\n    'srch_length_of_stay', \n    'srch_adults_count',\n    'srch_booking_window',\n    'srch_room_count', \n    'srch_saturday_night_bool',\n] \n\nPOSITIONAL_FEATURES = [ \n    'search_rank'\n]\n\nALL_FEATURES = PROPERTY_FEATURES + SEARCH_FEATURES + POSITIONAL_FEATURES\n</code></pre>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#pointwise-model","title":"Pointwise Model","text":"<p>We can train model to predict directly relevance score, but only minimizing error on relevance score prediction. We dont need the query id information since we are not performing swapping. </p> <p>For example we could use Regression as LinearRegression and RandomForest</p> <pre><code>from sklearn.linear_model import LinearRegression \nfrom sklearn.ensemble import RandomForestRegressor \n\n\nlr_pointwise = LinearRegression(n_jobs=-1)\nrf_pointwise = RandomForestRegressor(n_estimators=100,\n                                     n_jobs=-1,verbose=3,\n                                     random_state=2) # random state control the bootstrapping process \n\n\n# train model \nprint('Training Linear Regression')\nlr_pointwise.fit(X_train_relevance[ALL_FEATURES],y_train_relevance)\nprint('Training Random Forest')\nrf_pointwise.fit(X_train_relevance[ALL_FEATURES],y_train_relevance)\n</code></pre> Show Code Output <ul> <li>Training Linear Regression</li> <li>Training Random Forest</li> <li>building tree 1 of 100</li> <li>building tree 2 of 100</li> <li>building tree 3 of 100</li> <li>building tree 4 of 100</li> <li>building tree 5 of 100</li> <li>building tree 6 of 100</li> <li>building tree 7 of 100</li> <li>building tree 8 of 100</li> <li>[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.</li> <li>building tree 9 of 100</li> <li>building tree 10 of 100</li> <li>building tree 11 of 100</li> <li>building tree 12 of 100</li> <li>building tree 13 of 100</li> <li>building tree 14 of 100</li> <li>building tree 15 of 100</li> <li>building tree 16 of 100</li> <li>building tree 17 of 100</li> <li>building tree 18 of 100</li> <li>building tree 19 of 100</li> <li>building tree 20 of 100</li> <li>building tree 21 of 100</li> <li>building tree 22 of 100</li> <li>building tree 23 of 100</li> <li>building tree 24 of 100</li> <li>[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   32.3s</li> <li>building tree 25 of 100</li> <li>building tree 26 of 100</li> <li>building tree 27 of 100</li> <li>building tree 28 of 100</li> <li>building tree 29 of 100</li> <li>building tree 30 of 100</li> <li>building tree 31 of 100</li> <li>building tree 32 of 100</li> <li>building tree 33 of 100</li> <li>building tree 34 of 100</li> <li>building tree 35 of 100</li> <li>building tree 36 of 100</li> <li>building tree 37 of 100</li> <li>building tree 38 of 100</li> <li>building tree 39 of 100</li> <li>building tree 40 of 100</li> <li>building tree 41 of 100</li> <li>building tree 42 of 100</li> <li>building tree 43 of 100</li> <li>building tree 44 of 100</li> <li>building tree 45 of 100</li> <li>building tree 46 of 100</li> <li>building tree 47 of 100</li> <li>building tree 48 of 100</li> <li>building tree 49 of 100</li> <li>building tree 50 of 100</li> <li>building tree 51 of 100</li> <li>building tree 52 of 100</li> <li>building tree 53 of 100</li> <li>building tree 54 of 100</li> <li>building tree 55 of 100</li> <li>building tree 56 of 100</li> <li>building tree 57 of 100</li> <li>building tree 58 of 100</li> <li>building tree 59 of 100</li> <li>building tree 60 of 100</li> <li>building tree 61 of 100</li> <li>building tree 62 of 100</li> <li>building tree 63 of 100</li> <li>building tree 64 of 100</li> <li>building tree 65 of 100</li> <li>building tree 66 of 100</li> <li>building tree 67 of 100</li> <li>building tree 68 of 100</li> <li>building tree 69 of 100</li> <li>building tree 70 of 100</li> <li>building tree 71 of 100</li> <li>building tree 72 of 100</li> <li>building tree 73 of 100</li> <li>building tree 74 of 100</li> <li>building tree 75 of 100</li> <li>building tree 76 of 100</li> <li>building tree 77 of 100</li> <li>building tree 78 of 100</li> <li>building tree 79 of 100</li> <li>building tree 80 of 100</li> <li>building tree 81 of 100</li> <li>building tree 82 of 100</li> <li>building tree 83 of 100</li> <li>building tree 84 of 100</li> <li>building tree 85 of 100</li> <li>building tree 86 of 100</li> <li>building tree 87 of 100</li> <li>building tree 88 of 100</li> <li>building tree 89 of 100</li> <li>building tree 90 of 100</li> <li>building tree 91 of 100</li> <li>building tree 92 of 100</li> <li>building tree 93 of 100</li> <li>building tree 94 of 100</li> <li>building tree 95 of 100</li> <li>building tree 96 of 100</li> <li>building tree 97 of 100</li> <li>building tree 98 of 100</li> <li>building tree 99 of 100</li> <li> <p>building tree 100 of 100</p> </li> <li> <p>[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  3.4min finished</p> </li> </ul> <pre>RandomForestRegressor(n_jobs=-1, random_state=2, verbose=3)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFitted<pre>RandomForestRegressor(n_jobs=-1, random_state=2, verbose=3)</pre>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#pairwise-model","title":"Pairwise Model","text":"<p>There are lot of model that are used objective to maximize pairwise learning. </p> <p>for example let say user A has search in e commerce \"shiny bag\" and it returns 3 items </p> <ol> <li>Item A (click)</li> <li>Item B (click)</li> <li>Item C (bought)</li> </ol> <p>the idea is that our model should optimize maximizing pairwise objective Item C &gt; Item A and Item C &gt; B </p> <p>Some of them model that have pairwise objective :  - RankNet  - LambdaMART (Logistic Loss / RankNet Loss)</p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#ranknet","title":"RankNet","text":"<ul> <li> <p>Training Data Construction : </p> </li> <li> <p>Objective : </p> <ul> <li> <p>\\(P_{i,j} = P(U_i &gt; U_j) = \\cfrac{1}{1 + e^{-\\sigma(s_i-s_j)}}\\)</p> </li> <li> <p>Where : </p> <ul> <li>$P_{i,j} $ : Probability that item i is scored higher than item j</li> </ul> </li> <li> <p>In plain words we want our model giving higher score to item that are having higher score. </p> </li> <li> <p>the \\(s_i\\) comes from model prediction / output : </p> </li> <li> <p>\\(s_i = f(x_i|\\theta)\\)</p> </li> <li> <p>s_i - s_j : </p> <ul> <li>\\(s_i - s_j = 1 if s_i &gt; s_j\\) </li> <li>\\(s_i - s_j = 0 if s_i = s_j\\) </li> <li>\\(s_i - s_j = -1 s_i &lt; s_j\\)</li> </ul> </li> <li> <p>To obtain optimal parameter \\(\\theta\\) : </p> <ul> <li>Minimizing negative log likelihood from all training data (Similar to Classification)  : <ul> <li>$$ \\mathcal{L} = - \\sum_{(i,j) \\in \\mathcal{P}} \\left[     y_{ij}\\,\\log \\left(\\frac{1}{1+\\exp\\left(-\\sigma\\,(s_i-s_j)\\right)}\\right)<ul> <li>(1-y_{ij})\\,\\log \\left(1 - \\frac{1}{1+\\exp\\left(-\\sigma\\,(s_i-s_j)\\right)}\\right) \\right] $$</li> </ul> </li> <li>\\(y_{ij}\\) : Pairwise label , 1 if item i &gt; item j </li> </ul> </li> </ul> </li> <li>The objective could be solved through gradient descent </li> </ul> </li> <li> <p>The goal is to learn relevance scoring function. </p> </li> </ul>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#-given-query-result-each-item-in-result-has-relevance-score-depends-on-how-user-interact-for-example-book-should-have-bigger-relevance-score","title":"- Given query result , each item in result has relevance score depends on how user interact (for example book should have bigger relevance score)","text":""},{"location":"001-Learning%20to%20Rank/hotel_ranking/#lambda-rank","title":"Lambda Rank","text":"<ul> <li>Why  : </li> <li>Previous Approach does not directly optimize information retrieval metrics such as : NDCG</li> <li> <p>NDCG is not convex function. </p> </li> <li> <p>Solution : </p> </li> <li>We can optimize the NDCG but with sneaky trick , we can optimize the NDCG by seeing if we switch position between two item (i,j) how the NDCG changes (that similar  to gradient). </li> </ul> <p>\\(\\lambda_{i,j} = \\Delta NDCG_{\\text{swap}_{i,j}}\\)</p> <ul> <li>After swapping position we can find which item if we swap the position has the biggest change in NDCG, after swapping each query result combination </li> </ul> <p>now our task is predicting lambda </p> query_id X (features) y (lambda)"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#lambdamart","title":"LambdaMART","text":"<ul> <li>Basically LambdaMART ~ Work the same way as Lambda Rank only that the model in Lambda MART use Decision Tree (Multiple Additive Regression Trees)</li> </ul> <pre><code>import xgboost as xgb\nranker_pairwise = xgb.XGBRanker(n_estimators = 500,\n                                n_jobs = -1, random_state = 2, \n                                verbose = 2,\n                                tree_method=\"hist\", \n                                objective=\"rank:pairwise\")\nranker_pairwise.fit(X_train_relevance[ALL_FEATURES], y_train_relevance,qid=qid_train)\n</code></pre> <pre><code>/Users/sintamulyawati/Documents/rovos/venv/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [23:38:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \nParameters: { \"verbose\" } are not used.\n\n  bst.update(dtrain, iteration=i, fobj=obj)\n</code></pre> <pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n          colsample_bynode=None, colsample_bytree=None, device=None,\n          early_stopping_rounds=None, enable_categorical=False,\n          eval_metric=None, feature_types=None, feature_weights=None,\n          gamma=None, grow_policy=None, importance_type=None,\n          interaction_constraints=None, learning_rate=None, max_bin=None,\n          max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=None,\n          max_depth=None, max_leaves=None, min_child_weight=None, missing=nan,\n          monotone_constraints=None, multi_strategy=None, n_estimators=500,\n          n_jobs=-1, num_parallel_tree=None, ...)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRanker?Documentation for XGBRankeriFitted<pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n          colsample_bynode=None, colsample_bytree=None, device=None,\n          early_stopping_rounds=None, enable_categorical=False,\n          eval_metric=None, feature_types=None, feature_weights=None,\n          gamma=None, grow_policy=None, importance_type=None,\n          interaction_constraints=None, learning_rate=None, max_bin=None,\n          max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=None,\n          max_depth=None, max_leaves=None, min_child_weight=None, missing=nan,\n          monotone_constraints=None, multi_strategy=None, n_estimators=500,\n          n_jobs=-1, num_parallel_tree=None, ...)</pre>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#listwise-model","title":"Listwise Model","text":"<p>Some of them model that have pairwise objective :  - RankNet  - LambdaMART (Logistic Loss / RankNet Loss)</p> <pre><code>ranker_listwise = xgb.XGBRanker(n_estimators = 500,\n                                n_jobs = -1, random_state = 2, \n                                tree_method=\"hist\", \n                                objective=\"rank:ndcg\")\nranker_listwise.fit(X_train_relevance[ALL_FEATURES], y_train_relevance,qid=qid_train,\n        verbose=True)\n</code></pre> <pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n          colsample_bynode=None, colsample_bytree=None, device=None,\n          early_stopping_rounds=None, enable_categorical=False,\n          eval_metric=None, feature_types=None, feature_weights=None,\n          gamma=None, grow_policy=None, importance_type=None,\n          interaction_constraints=None, learning_rate=None, max_bin=None,\n          max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=None,\n          max_depth=None, max_leaves=None, min_child_weight=None, missing=nan,\n          monotone_constraints=None, multi_strategy=None, n_estimators=500,\n          n_jobs=-1, num_parallel_tree=None, ...)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRanker?Documentation for XGBRankeriFitted<pre>XGBRanker(base_score=None, booster=None, callbacks=None, colsample_bylevel=None,\n          colsample_bynode=None, colsample_bytree=None, device=None,\n          early_stopping_rounds=None, enable_categorical=False,\n          eval_metric=None, feature_types=None, feature_weights=None,\n          gamma=None, grow_policy=None, importance_type=None,\n          interaction_constraints=None, learning_rate=None, max_bin=None,\n          max_cat_threshold=None, max_cat_to_onehot=None, max_delta_step=None,\n          max_depth=None, max_leaves=None, min_child_weight=None, missing=nan,\n          monotone_constraints=None, multi_strategy=None, n_estimators=500,\n          n_jobs=-1, num_parallel_tree=None, ...)</pre>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#holdout-set-evaluation","title":"Holdout Set Evaluation","text":"<p>The goal is to check whether model learn or not </p> <p>There are lot of Ranking / Information Retrieval Metrics. we can divide into : </p> <ul> <li> <p>Binary Relevance Metrics </p> <ul> <li>Precision @ K </li> <li>Mean Average Precision @ K </li> <li>Mean Reciprocal Rank </li> </ul> </li> <li> <p>Rank Aware Metrics </p> <ul> <li>Normalized Discounted Cumulative Gain (NDCG)</li> </ul> </li> </ul> <p>What is Ideal Ranking Metrics ?</p> <ul> <li>If we predict correctly the relevance score at top position is better than correctly predicting relevance score in lower position. </li> </ul>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#metrics-ndcg","title":"Metrics : NDCG","text":"\\[ \\begin{align*}\\\\ \\begin{split}\\\\ \\text{DCG}= \\sum_{i=1}^{k} \\frac{ 2^{\\text{rel}[i]}-1}{\\log_{2}([i]+2)} \\\\ \\end{split}\\\\ \\end{align*}\\\\ \\] <p>Where : </p> <ul> <li>\\(\\text{DCG}\\) = Discounted Cumulative Gain </li> <li>\\(k= \\text{number of items in recommendation}\\)\\</li> <li>\\(\\text{rel}[i]\\) = relevance score at item in position i th</li> <li>Relevance score itself usually relevance score based on ground truth </li> </ul> <p>So what is the NDCG ? Why do we even need NDCG ? the analogy similar to correlation and covariance matrix. We can measure covariance matrix to asses comovement between two Random Variables. However the value is incomparable hence we need to scale to some finite space such as probability score, ranging from 0 to 1. </p> <p>The Idea of DCG is that position does matter, if our model can predict the highest relevance score at top position is better compared to lower position</p> <p>In NDCG we can scale the DCG by maximum value of DCG or we call it as  Ideal DCG</p> \\[   \\begin{align*}\\\\   \\begin{split}\\\\   \\text{NDCG}= \\frac{\\text{DCG}}{\\text{Ideal DCG}}\\\\   \\end{split}\\\\   \\end{align*}\\\\ \\] <p>So what is the ideal DCG in this case ? </p> <p>There are some flavor, citing Doug TurnBull in His Blogpost </p> <ul> <li>Local Ideal \u2192 Sorted based on ground truth relevance \u2192 </li> <li>Global Ideal \u2192 using whole score from judgement list as ground truth , the previous one only use the response from search only </li> <li>Max Label \u2192 Considering each position has maximum score, for example in our case the book event type has relevance of 2. </li> </ul> <p>The fact is that if we use Max Label the NDCG Score will decrease, its because the view is pessimistic.</p> <p>for example we will use validation to measure ndcg. </p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#validation-set","title":"Validation Set","text":"<p>The purpose on this set mainly selecting model : </p> <ul> <li>Selecting best hyperparameter </li> <li>Selecting approach </li> <li>pipeline etc </li> </ul> <p>We could do through Cross Validation which have better estimate of model performance.  In this article we only implement the Validation Set approach . We  wont cover Hyperparameter Tuning in this article.</p> <p>Next, we should add our model prediction. So, how can we predict ?First this model focus on ranking, which means, does not focus on candidate generation step. Hence the model predict all item in given search id. the model doesnot predict relevance score for all item. </p> <p>Before we perform prediction we have our positional feature needs to be adjusted , because we dont know the positional features in validation dataset, we can set to the first rank.</p> <pre><code>X_val_relevance['search_rank'] = 1\n</code></pre> <p>Pointwise</p> <pre><code># pointwise score \ny_val_pred_pointwise = rf_pointwise.predict(X_val_relevance[ALL_FEATURES])\n\n# save as separate df which contain \nval_prediction_pointwise = dataset.loc[dataset['srch_id'].isin(search_id_val),\n                         ['prop_id' , 'srch_id']]\nval_prediction_pointwise['y_true'] = y_val_relevance\nval_prediction_pointwise['pred_lambda'] = y_val_pred_pointwise\n\n\n# sort based on query id and pred_lambda \nval_prediction_pointwise = val_prediction_pointwise.sort_values(['srch_id',\n                                                               'pred_lambda'],ascending=[\n                                                                   True,False\n                                                               ])\nval_prediction_pointwise['rank'] = val_prediction_pointwise.groupby('srch_id').cumcount() + 1\nval_prediction_pointwise\n</code></pre> Show Code Output <p>[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers. [Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.3s [Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    1.7s finished</p> <p> prop_id srch_id y_true pred_lambda rank 3123447 105305 532288 5 1.275 1 3123444 90139 532288 0 0.650 2 3123455 136621 532288 0 0.530 3 3123433 25017 532288 0 0.500 4 3123436 35962 532288 0 0.400 5 ... ... ... ... ... ... 3516328 53300 598662 0 0.100 28 3516344 107244 598662 0 0.100 29 3516327 48186 598662 0 0.050 30 3516341 101066 598662 0 0.050 31 3516340 96306 598662 0 0.000 32 <p>273242 rows \u00d7 5 columns</p> </p> <p>Pairwise</p> <pre><code># pairwise score \ny_val_pred_pairwise = ranker_pairwise.predict(X_val_relevance[ALL_FEATURES])\n\n# save as separate df which contain \nval_prediction_pairwise = dataset.loc[dataset['srch_id'].isin(search_id_val),\n                         ['prop_id' , 'srch_id']]\nval_prediction_pairwise['y_true'] = y_val_relevance\nval_prediction_pairwise['pred_lambda'] = y_val_pred_pairwise\n\n\n# sort based on query id and pred_lambda \nval_prediction_pairwise = val_prediction_pairwise.sort_values(['srch_id',\n                                                               'pred_lambda'],ascending=[\n                                                                   True,False\n                                                               ])\nval_prediction_pairwise['rank'] = val_prediction_pairwise.groupby('srch_id').cumcount() + 1\nval_prediction_pairwise\n</code></pre> Show Code Output <p> prop_id srch_id y_true pred_lambda rank 3123433 25017 532288 0 0.170013 1 3123448 105787 532288 0 0.161488 2 3123436 35962 532288 0 0.082060 3 3123454 136377 532288 0 -0.105323 4 3123447 105305 532288 5 -0.230895 5 ... ... ... ... ... ... 3516343 104950 598662 0 -0.857398 28 3516339 95810 598662 0 -1.003210 29 3516342 102352 598662 0 -1.123185 30 3516341 101066 598662 0 -1.618251 31 3516328 53300 598662 0 -1.923931 32 <p>273242 rows \u00d7 5 columns</p> </p> <p>Listwise</p> <pre><code># listwise score \nval_prediction_listwise = dataset.loc[dataset['srch_id'].isin(search_id_val),\n                          ['prop_id' , 'srch_id']]\n\ny_val_pred_listwise = ranker_listwise.predict(X_val_relevance[ALL_FEATURES])\nval_prediction_listwise['pred_lambda'] = y_val_pred_listwise\nval_prediction_listwise['y_true'] = y_val_relevance\n# sort based on query id and pred_lambda \nval_prediction_listwise = val_prediction_listwise.sort_values(['srch_id',\n                                                               'pred_lambda'],ascending=[\n                                                                   True,False\n                                                               ])\n\nval_prediction_listwise['rank'] = val_prediction_listwise.groupby('srch_id').cumcount() + 1\nval_prediction_listwise\n</code></pre> Show Code Output <p> prop_id srch_id pred_lambda y_true rank 3123433 25017 532288 0.144549 0 1 3123436 35962 532288 0.136666 0 2 3123447 105305 532288 -0.069895 5 3 3123451 111825 532288 -0.170109 0 4 3123439 49185 532288 -0.170845 0 5 ... ... ... ... ... ... 3516349 120070 598662 -0.440107 0 28 3516331 62444 598662 -0.501732 0 29 3516339 95810 598662 -0.638550 0 30 3516328 53300 598662 -0.875726 0 31 3516341 101066 598662 -1.246449 0 32 <p>273242 rows \u00d7 5 columns</p> </p> <p>lets make a function to prepare the prediction on ground truth </p> <pre><code>def prepare_for_eval(dataset,model,X_test,y_test,query_id,search_col='srch_id',prop_col = 'prop_id') : \n    pred_df = dataset.loc[dataset[search_col].isin(query_id),\n                            [prop_col , search_col]]\n\n    y_pred = model.predict(X_test)\n    pred_df['pred_lambda'] = y_pred\n    pred_df['y_true'] = y_test\n    # sort based on query id and pred_lambda \n    pred_df = pred_df.sort_values([search_col,\n                                   'pred_lambda'],ascending=[ True,False])\n    pred_df['rank'] = pred_df.groupby(search_col).cumcount() + 1\n    return pred_df\n</code></pre> <p>Simple NDCG Calculation on a query </p> <pre><code>sample_dcg = val_prediction_listwise.loc[ \n    val_prediction_listwise['srch_id'] == np.random.choice(val_prediction_listwise['srch_id']) \n]\n</code></pre> \\[ \\text{DCG}= \\sum_{i=1}^{k} \\frac{ 2^{\\text{rel}[i]}-1}{\\log_{2}([i]+2)} \\] <pre><code># \nsample_dcg.loc[:,'gain'] = 2 ** sample_dcg.loc[:,'y_true'] - 1\n# \nsample_dcg.loc[:,'dcg'] = sample_dcg.loc[:,'gain'] / np.log2(sample_dcg.loc[:,'rank'] + 1)\n</code></pre> <pre><code>/var/folders/zt/dkkszv0n5bd8nq9dywcdj7pc0000gn/T/ipykernel_743/404199756.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sample_dcg.loc[:,'gain'] = 2 ** sample_dcg.loc[:,'y_true'] - 1\n/var/folders/zt/dkkszv0n5bd8nq9dywcdj7pc0000gn/T/ipykernel_743/404199756.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sample_dcg.loc[:,'dcg'] = sample_dcg.loc[:,'gain'] / np.log2(sample_dcg.loc[:,'rank'] + 1)\n</code></pre> <pre><code>dcg = sample_dcg.groupby('srch_id').agg({'dcg':'sum'})\ndcg.head(5)\n</code></pre> dcg srch_id 587980 10.333333 <pre><code># calculate ideal dcg , ideal sorting based on y_true \nideal_dcg = sample_dcg.copy()\nideal_dcg = sample_dcg.sort_values(['srch_id',\n                                                               'y_true'],ascending=[\n                                                                   True,False\n                                                               ])\nideal_dcg['rank'] = ideal_dcg.groupby('srch_id').cumcount() + 1\nideal_dcg\n</code></pre> Show Code Output <p> prop_id srch_id pred_lambda y_true rank gain dcg 3453954 109147 587980 -0.388572 5 1 31 10.333333 3453941 3904 587980 0.085224 0 2 0 0.000000 3453951 82171 587980 0.080693 0 3 0 0.000000 3453948 57499 587980 -0.161591 0 4 0 0.000000 3453955 131812 587980 -0.281207 0 5 0 0.000000 3453953 101045 587980 -0.313309 0 6 0 0.000000 3453946 33678 587980 -0.374939 0 7 0 0.000000 3453947 55787 587980 -0.388951 0 8 0 0.000000 3453949 68081 587980 -0.528888 0 9 0 0.000000 3453950 68414 587980 -0.581832 0 10 0 0.000000 3453943 10085 587980 -0.616705 0 11 0 0.000000 3453944 24104 587980 -0.616705 0 12 0 0.000000 3453942 3910 587980 -0.698352 0 13 0 0.000000 3453952 92278 587980 -0.813638 0 14 0 0.000000 3453945 24966 587980 -0.872794 0 15 0 0.000000 </p> <pre><code># \nideal_dcg.loc[:,'gain'] = 2 ** ideal_dcg.loc[:,'y_true'] - 1\n# \nideal_dcg.loc[:,'dcg'] = ideal_dcg.loc[:,'gain'] / np.log2(ideal_dcg.loc[:,'rank'] + 1)\nideal_dcg = ideal_dcg.groupby('srch_id').agg({'dcg':'sum'})\n</code></pre> <pre><code>dcg / ideal_dcg\n</code></pre> Show Code Output <p> dcg srch_id 587980 0.333333 </p> <pre><code>def calculate_dcg(prediction_result,\n                  target_col,\n                  sorting_col,\n                  search_col='srch_id') :\n     dcg = prediction_result.copy()\n\n     # order based on relevance score \n     dcg = dcg.sort_values(\n          [search_col,sorting_col],ascending=[True,False]\n     )\n\n\n     # calculate ranking\n     dcg['rank'] = dcg.groupby(search_col).cumcount() + 1\n\n\n     # calculate gain \n     dcg['gain'] = 2 ** dcg[target_col] - 1\n\n     # calculate dcg \n     dcg['dcg'] = dcg['gain'] / np.log2(dcg['rank'] + 1)\n\n     # sum \n     dcg = dcg.groupby(search_col).agg({'dcg':'sum'})\n\n     return dcg\n</code></pre> <p>we usually calculate using Top N Position of ranking, not full item. </p> <pre><code>def calculate_dcg_topk(prediction_result,\n                  target_col,\n                  sorting_col,\n                  search_col,k=None) :\n     dcg = prediction_result.copy()\n\n     # order based on relevance score \n     dcg = dcg.sort_values(\n          [search_col,sorting_col],ascending=[True,False]\n     )\n\n\n     # calculate ranking\n     dcg['rank'] = dcg.groupby(search_col).cumcount() + 1\n\n     # filter only up to k \n\n     if k is not None : \n          dcg = dcg.loc[ \n               dcg['rank']&lt;= k\n          ]\n\n     # calculate gain \n     dcg['gain'] = 2 ** dcg[target_col] - 1\n\n     # calculate dcg \n     dcg['dcg'] = dcg['gain'] / np.log2(dcg['rank'] + 1)\n\n     # sum \n     dcg = dcg.groupby(search_col).agg({'dcg':'sum'})\n\n     return dcg\n</code></pre> <pre><code>dcg_listwise = calculate_dcg(prediction_result=val_prediction_listwise,\n                             target_col='y_true',\n                             sorting_col='pred_lambda',\n                             search_col='srch_id')\nideal_dcg_listwise = calculate_dcg(prediction_result=val_prediction_listwise,\n                             target_col='y_true',\n                             sorting_col='y_true',\n                             search_col='srch_id')\n\nndcg = dcg_listwise / ideal_dcg_listwise\n\nndcg.mean()\n</code></pre> <pre><code>dcg    0.439791\ndtype: float64\n</code></pre> <p>Top 5</p> <pre><code>dcg_listwise_top5 = calculate_dcg_topk(prediction_result=val_prediction_listwise,\n                             target_col='y_true',\n                             sorting_col='pred_lambda',\n                             search_col='srch_id',k=5)\nideal_dcg_listwise_top5 = calculate_dcg_topk(prediction_result=val_prediction_listwise,\n                             target_col='y_true',\n                             sorting_col='y_true',\n                             search_col='srch_id',k=5)\n\nndcg5 = dcg_listwise_top5 / ideal_dcg_listwise_top5\n\nndcg5.mean()\n</code></pre> <pre><code>dcg    0.289173\ndtype: float64\n</code></pre> <p>Top 10 </p> <pre><code>dcg_listwise_top10 = calculate_dcg_topk(prediction_result=val_prediction_listwise,\n                             target_col='y_true',\n                             sorting_col='pred_lambda',\n                             search_col='srch_id',k=10)\nideal_dcg_listwise_top10 = calculate_dcg_topk(prediction_result=val_prediction_listwise,\n                             target_col='y_true',\n                             sorting_col='y_true',\n                             search_col='srch_id',k=10)\n\nndcg10 = dcg_listwise_top10 / ideal_dcg_listwise_top10\n\nndcg10.mean()\n</code></pre> <pre><code>dcg    0.357547\ndtype: float64\n</code></pre> <pre><code>dcg_listwise_top50 = calculate_dcg_topk(prediction_result=val_prediction_listwise,\n                             target_col='y_true',\n                             sorting_col='pred_lambda',\n                             search_col='srch_id',k=50)\nideal_dcg_listwise_top50 = calculate_dcg_topk(prediction_result=val_prediction_listwise,\n                             target_col='y_true',\n                             sorting_col='y_true',\n                             search_col='srch_id',k=50)\n\nndcg50 = dcg_listwise_top50 / ideal_dcg_listwise_top50\n\nndcg50.mean()\n</code></pre> <pre><code>dcg    0.439791\ndtype: float64\n</code></pre> <p>We can see as number of Top N increase NDCG gets bigger. </p> <p>Wrapping Up into a function  to calculate ndcg</p> <pre><code>def calculate_ndcg_ideal(prediction_result,\n                         target_col,sorting_col,\n                         search_col,k=None) : \n    dcg = calculate_dcg_topk(prediction_result=prediction_result,\n                             target_col='y_true',\n                             sorting_col=sorting_col,\n                             search_col=search_col,k=k)\n    ideal_dcg = calculate_dcg_topk(prediction_result=prediction_result,\n                                target_col=target_col,\n                                sorting_col=target_col,\n                                search_col=search_col,k=k)\n\n    ndcg = dcg['dcg'].values / ideal_dcg['dcg'].values\n\n    return ndcg\n</code></pre> <p>Comparing Model Performance on Validation Set</p> <p>NDCG@5</p> <pre><code># pointwise \npointwise_ndcg5 = calculate_ndcg_ideal(prediction_result = val_prediction_pointwise ,\n                         target_col = 'y_true',\n                         sorting_col = 'pred_lambda',\n                         search_col='srch_id',\n                         k=5).mean()\n\n# pairwise\npairwise_ndcg5 = calculate_ndcg_ideal(prediction_result = val_prediction_pairwise ,\n                         target_col = 'y_true',\n                         sorting_col = 'pred_lambda',\n                         search_col= 'srch_id',\n                         k=5).mean()\n# listwise\nlistwise_ndcg5 = calculate_ndcg_ideal(prediction_result = val_prediction_listwise ,\n                         target_col = 'y_true',\n                         sorting_col = 'pred_lambda',\n                         search_col= 'srch_id',\n                         k=5).mean()\n\nval_ndcg5 = pd.DataFrame({\n    'NDCG@5' : [pointwise_ndcg5,pairwise_ndcg5,listwise_ndcg5]\n},index=['pointwise','pairwise','listwise'])\nval_ndcg5\n</code></pre> NDCG@5 pointwise 0.191748 pairwise 0.295687 listwise 0.289173 <p>NDCG@10</p> <pre><code># pointwise \npointwise_ndcg10 = calculate_ndcg_ideal(prediction_result = val_prediction_pointwise ,\n                         target_col = 'y_true',\n                         sorting_col = 'pred_lambda',\n                         search_col= 'srch_id',\n                         k=10).mean()\n\n# pairwise\npairwise_ndcg10 = calculate_ndcg_ideal(prediction_result = val_prediction_pairwise ,\n                         target_col = 'y_true',\n                         sorting_col = 'pred_lambda',\n                         search_col= 'srch_id',\n                         k=10).mean()\n# listwise\nlistwise_ndcg10 = calculate_ndcg_ideal(prediction_result = val_prediction_listwise ,\n                         target_col = 'y_true',\n                         sorting_col = 'pred_lambda',\n                         search_col= 'srch_id',\n                         k=10).mean()\n\nval_ndcg10 = pd.DataFrame({\n    'NDCG@10' : [pointwise_ndcg10,pairwise_ndcg10,listwise_ndcg10]\n},index=['pointwise','pairwise','listwise'])\nval_ndcg10\n</code></pre> NDCG@10 pointwise 0.261939 pairwise 0.364634 listwise 0.357547 <p>NDCG@50</p> <pre><code># pointwise \npointwise_ndcg50 = calculate_ndcg_ideal(prediction_result = val_prediction_pointwise ,\n                         target_col = 'y_true',\n                         sorting_col = 'pred_lambda',\n                         search_col= 'srch_id',\n                         k=50).mean()\n\n# pairwise\npairwise_ndcg50 = calculate_ndcg_ideal(prediction_result = val_prediction_pairwise ,\n                         target_col = 'y_true',\n                         sorting_col = 'pred_lambda',\n                         search_col= 'srch_id',\n                         k=50).mean()\n# listwise\nlistwise_ndcg50 = calculate_ndcg_ideal(prediction_result = val_prediction_listwise ,\n                         target_col = 'y_true',\n                         sorting_col = 'pred_lambda',\n                         search_col= 'srch_id',\n                         k=50).mean()\n\nval_ndcg50 = pd.DataFrame({\n    'NDCG@50' : [pointwise_ndcg50,pairwise_ndcg50,listwise_ndcg50]\n},index=['pointwise','pairwise','listwise'])\nval_ndcg50\n</code></pre> NDCG@50 pointwise 0.372516 pairwise 0.445406 listwise 0.439791 <pre><code>val_ndcg = val_ndcg5.join(val_ndcg10).join(val_ndcg50)\nval_ndcg.sort_values('NDCG@10',ascending=False)\n</code></pre> NDCG@5 NDCG@10 NDCG@50 pairwise 0.295687 0.364634 0.445406 listwise 0.289173 0.357547 0.439791 pointwise 0.191748 0.261939 0.372516 <p>From the result above we can see that most of the NDCG measure pairwise models perform better by huge margin. </p> <p>Next, we predict on the test set. </p>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#test-set","title":"Test Set","text":"<pre><code># set the positional features first \nX_test_relevance['search_rank'] = 1\n</code></pre> <pre><code>/var/folders/zt/dkkszv0n5bd8nq9dywcdj7pc0000gn/T/ipykernel_743/1429340643.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_test_relevance['search_rank'] = 1\n</code></pre> <pre><code>test_prediction_pairwise = prepare_for_eval(dataset = dataset,\n                 model = ranker_pairwise,\n                 X_test = X_test_relevance[ALL_FEATURES],\n                 y_test = y_test_relevance,\n                 query_id = search_id_test,\n                 search_col='srch_id',\n                 prop_col = 'prop_id')\n\ntest_ndcg5 = calculate_ndcg_ideal(prediction_result = test_prediction_pairwise ,\n                         target_col = 'y_true',\n                         sorting_col = 'pred_lambda',\n                         search_col= 'srch_id',\n                         k=5).mean()\n\ntest_ndcg10 = calculate_ndcg_ideal(prediction_result = test_prediction_pairwise ,\n                         target_col = 'y_true',\n                         sorting_col = 'pred_lambda',\n                         search_col= 'srch_id',\n                         k=10).mean()\ntest_ndcg50 = calculate_ndcg_ideal(prediction_result = test_prediction_pairwise ,\n                         target_col = 'y_true',\n                         sorting_col = 'pred_lambda',\n                         search_col= 'srch_id',\n                         k=50).mean()\n\n\ntest_ndcg_pairwise = pd.DataFrame({\n    'NDCG@5' : test_ndcg5,\n    'NDCG@10' : test_ndcg10,\n    'NDCG@50' : test_ndcg50\n},index=['pairwise in test set'])\ntest_ndcg_pairwise\n</code></pre> NDCG@5 NDCG@10 NDCG@50 pairwise in test set 0.297005 0.36653 0.446281 <pre><code>val_ndcg_pairwise = val_ndcg.loc[ \n    val_ndcg.index=='pairwise'\n]\nval_ndcg_pairwise\n</code></pre> NDCG@5 NDCG@10 NDCG@50 pairwise 0.295687 0.364634 0.445406 <pre><code>pd.concat(\n    [val_ndcg_pairwise,\n    test_ndcg_pairwise],axis=0\n)\n</code></pre> NDCG@5 NDCG@10 NDCG@50 pairwise 0.295687 0.364634 0.445406 pairwise in test set 0.297005 0.366530 0.446281"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#dump-model-artifact","title":"Dump Model Artifact","text":"<pre><code>import joblib \n\n\njoblib.dump(ranker_pairwise,'ltr_pairwise.pkl')\njoblib.dump(ranker_listwise,'ltr_listwise.pkl')\njoblib.dump(rf_pointwise,'rf_pointwise.pkl')\n</code></pre> <pre><code>['rf_pointwise.pkl']\n</code></pre>"},{"location":"001-Learning%20to%20Rank/hotel_ranking/#references","title":"References :","text":"<ul> <li> <p>Expedia's Channel-Smart Property Search: How Expedia Tailors Rankings for You</p> </li> <li> <p>Embedding-Based Retrieval for Airbnb Search</p> </li> <li> <p>Doug TurnBull : Flavors of NDCG</p> </li> <li> <p>Stanford University CS 276 : Information Retrieval</p> </li> <li> <p>AI Powered Search (Book)</p> </li> <li> <p>Machine Learning Powered Search Ranking of AirBnB Experiences</p> </li> <li> <p>Learning to Rank with Non-Smooth Cost Functions (Lambda Rank) </p> </li> <li> <p>From RankNet to LambdaRank to LambdaMART: An Overview</p> </li> <li> <p>Eugene Yan's System Design for Recommendations and Search</p> </li> </ul>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/","title":"Measuring Effect of Product Recommendation Initiative to decide whether to  launch feature to all user","text":"Photo by Isaac Smith on Unsplash <p>PT Buah Alam , is an online grocery store that are currently growing at an grow stages of startup, to reach profitability they need to make a move. One of the eandeavor by approaching by boosting sales on platform. </p> <p>How to do that ? There are lot of ways. One of them through upselling / cross selling </p> <p>The Data Science Team already developed the recommendation algorithm in checkout pages </p> <p></p> <p>Here's a catch : </p> <ol> <li> <p>If we implement new initiative to all of our user , the cost is too big and does not guarantee better marketing performance</p> </li> <li> <p>If we implement don't implement / atleast try, we have opportunity cost , \"maybe\" this initiative works well and yield better marketing performance </p> </li> </ol> <p>Solution ? </p> <p>There are several ways to prove causality / causal relationship </p> <p></p> <p>One of the most reliable tool is through Randomized Control Trial (RCT) </p> <p>For example, we want to estimate the effect on changing call to action in payment button</p> <p></p> <p>However there are still other potential cause or what we called as confounding variable. </p> <p></p> <p>To remove the bias , we could add randomization to make other confounder equal , by assignming randomly which user get button change</p> <p></p> <p>That the basic idea of Randomized Control Trial or commonly called as A/B Test</p> <p>So when does the A/B Test works ? </p> <ol> <li> <p>When each experiment unit doesnot interfere / influence other experiment unit (Stable Unit Treatment Value Assumption)    For example in marketplace where there is network effect</p> </li> <li> <p>When assigning experiment is possible (for example it's unethical to assign people to get lower education)</p> </li> <li> <p>When period is too long For example experimenting on ads campaign to measure number of deals made for property housing </p> </li> </ol> <p>So is it possible in our case ? Yes ! It's possible because : </p> <ol> <li> <p>Each user can be assigned to different feature </p> </li> <li> <p>Each user doesnot interfere with other users</p> </li> <li> <p>The period to measure causal impact still feasible </p> </li> </ol> <p>So how do we do A/B test? Don't worry here is the steps : </p> <p></p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#1-setting-up-problem","title":"1. Setting Up Problem","text":"<p>Our focus is about increasing revenue ? But which revenue ? There are lot of revenues. The Revenue we are using is from user transaction because we can intervene on it. </p> <p>After that we should have idea what is the current funnel / user journey in buying phase </p> <p></p> <p>After that we should decide what metrics as our target of experimentation. Back to our goal this recommender system initiative to boost revenue. Spesifically Revenue from User Transaction. Hence our metrics is Revenue per User</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#2-designing-experiment","title":"2. Designing Experiment","text":""},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#21-experiment-unit","title":"2.1. Experiment Unit","text":"<p>Our Experiment unit is user level , but which user since the recommendation system is located checkout process , our user pool is user who proceed into checkout process </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#22-group","title":"2.2. Group","text":"<ul> <li>Control Group : Users who dont receive product recommendation during checkout process </li> <li>Treatment Group : User who receive product recommendation during checkout process </li> </ul>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#23-hypothesis","title":"2.3. Hypothesis","text":"<p>We want to check if revenue per user for treatment group is &gt; control group </p> <p>\\(H_0 = \\mu_{control} &gt;= \\mu_{treatment}\\)</p> <p>\\(H_1 = \\mu_{control} &lt; \\mu_{treatment}\\)</p> <p>Our statistical test aim to prove the hypothesis whether it is rejected or fail to reject </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#23-number-of-samples","title":"2.3. Number of Samples","text":"<p>As Information , we have 15 Million Active User , with median monthly in-checkout-process user of 20.000 . We use median because number of user in checkout process is vary. </p> <p>So how many number of user we need for both treatment and control ? Well it's depend on Treatment Effect you want to capture , the smaller the effect you want to capture the more sample we need to add because of the experiment precision</p> <p>Decision to role out new feature based on whether new initiative meet certain criteria such as :</p> <ol> <li> <p>Statistically Significant     We'll talk about it later</p> </li> <li> <p>Practically Significant    Related to Business Requirement</p> </li> </ol>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#231-business-requirement","title":"2.3.1 Business Requirement","text":"<p>Our new initiative by recommending purchase is indeed costly, we have to calculate the cost first </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#2311-cost-of-making-recommendation","title":"2.3.1.1. Cost of Making Recommendation","text":"<ol> <li> <p>Man Power     The Development process assumed in six month , people that are involved : </p> <ul> <li> <p>Data Scientist , assume monthly rate $ 8000 to $ 12000 / month x 6 month = $ 48.000 - $ 72.000 </p> </li> <li> <p>Data Engineer  , assume monthly rate $8000 to $12000 / month x 6 month = $48.000 - $72.000 </p> </li> <li> <p>MLOps , assume monthly rate $9000 to $10000 / month x 6 month = $45.000 - $60.000 </p> </li> <li> <p>Software Engineer , assume monthly rate $9000 to $10000 / month x 6 month = $45.000 - $60.000 </p> Development Cost Development Cost Role Lower Bound Cost Upper Bound Cost Data Scientist 48000 72000 Data Engineer 48000 72000 MLOps 45000 60000 Software Engineer 45000 60000 Total Cost 186000 264000 </li> </ul> </li> <li> <p>Cloud      to handle ~ 100k users per  day x 30 day, another information is that the calculation is in batch manner, we don't need real time recommendation yet. Let say the cost isUSD \\(1000-2000 per month  x 12 month =\\)1.000 -$24.000  annualy</p> </li> <li> <p>Cost Summarization</p> </li> </ol> Cost Attribute Lower Bound Cost Upper Bound Cost Development Cost 186000 264000 Annual Infra Cost 12000 24000 Total 198000 288000"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#2312-reasonable-revenue","title":"2.3.1.2. Reasonable Revenue","text":"<p>We have seen that in order to make recommender system is not cheap at this scale, hence at what incremental level should we roll out this feature to all user ? Of Course covering costs + expected increase in user revenue. </p> <p>About cost we have already know let say we take  USD 288,000 , what about expected increase in user revenue. As per now median per user revenue was about USD 1 daily.  Median  of monthly user checkout 100.000 user  minimum revenue should be \\(Revenue &gt;= 288.000\\) </p> <p>Current Revenue per User/ annual  = 1 x 20.000 x 12  =  USD 240.000  Gaps = Not Yet Covered Cost + Expected Growth USD 48.000 / (12 x 20.000 ) = USD 0.2 </p> <p>Revenue / User / Daily &gt;= USD 1.2 </p> <p>If we continue to success we can have USD 1 increase from USD 1 to USD 2 We can have Additional Revenue of </p> <p>First Year</p> <p>2-1.2 x 20.000 x 12 = USD 192.000</p> <p>Second Year or more</p> <p>2 x 20.000 x 12 - (Annual Infra Cost) </p> <p>2 x 20.000 x 12 - (24.000) = USD 456.000</p> <p>So in this case we have to able to detect USD 1 Change in revenue per user. This will relate to the next problem which is power Analysis</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#231-power-analysis","title":"2.3.1 Power Analysis","text":"<p>In statistical testing there are two possible Errors : </p> <ul> <li>Rejecting when it should failed to be Rejected, or (Type I Error) </li> <li>Fail to Reject when it should be Rejected or (Type II Error)</li> </ul> <p>Typically Type I Error influence what we called as significance level or \\(\\alpha\\). While the second one is influenced by Power \\((1-\\beta)\\)</p> <p>Will be focusing on Power Analysis </p> <ul> <li>Power is the probability to reject \\(H_{0}\\) given \\(H_{0}\\) is false.</li> </ul> \\[ \\text{power} = 1-\\beta = P(\\text{reject } H_{0} \\ | \\ H_{0} \\text{ is false}) \\] <ul> <li>Power is dependent to Factor such as </li> <li>Effect size</li> <li>Data variance</li> <li>Sample size (\\(n\\))</li> </ul> <p>We will simulate how above mentioned factor related to power </p> <pre><code>import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport math \nimport scipy.stats as stats\n</code></pre> <pre><code>def generate_data(mu_1,mu_2,std_dev_1,std_dev_2,n_sample_1,n_sample_2, N=10000, n_repeat=1000):\n\n    # generate data \n    data_1 = np.random.normal(mu_1, std_dev_1, N)\n    data_2 = np.random.normal(mu_2, std_dev_2, N)\n\n    sample_1_mean = []\n    sample_2_mean = []\n\n    for i in range(n_repeat):\n        # Generate data\n        sample_1 = np.random.choice(data_1, size=(1, n_sample_1), replace=False)\n        sample_2 = np.random.choice(data_2, size=(1, n_sample_2), replace=False)\n\n        # obtain mean \n        mean_1 = sample_1.mean()\n        mean_2 = sample_2.mean()\n\n        # Append mean to the list\n        sample_1_mean.append(mean_1)\n        sample_2_mean.append(mean_2)\n\n\n    return sample_1_mean,sample_2_mean\n</code></pre> <pre><code>def generate_ab_viz(mean_1,mean_2) : \n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 7))\n\n    sns.histplot(mean_1,alpha = 1, bins = 30,kde=True,\n                label = r\"Group 1 - $H_0$\",color='red')\n    sns.histplot(mean_2,alpha = 1, bins = 30,kde=True,\n                label = r\"Group 2 - $H_1$\",color='green')\n    plt.axvline(np.mean(mean_1),color = 'b',label='group 1 mean',linestyle='--')\n    plt.axvline(np.mean(mean_2),color = 'yellow',label='group 2 mean',linestyle='--')\n    ax.legend()\n    plt.show()\n</code></pre> <p>Let's generate some simulation </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#variance","title":"Variance","text":"<p>Bigger</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':20 , 'std_dev_2':20, \n    'n_sample_1':100,'n_sample_2':100,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>The More Variance we get, is getting harder to see the effect size of two groups</p> <p>Smaller</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':1 , 'std_dev_2':1, \n    'n_sample_1':100,'n_sample_2':100,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>The Less Variance we get we can see the difference clearly now </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#sample-size","title":"Sample Size","text":"<p>Smaller</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':100,'n_sample_2':100,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>Bigger</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 2 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':500,'n_sample_2':500,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#effect-size","title":"Effect Size","text":"<p>or Change / Increase we want to detect</p> <p>Smaller</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 1+0.01 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':500,'n_sample_2':500,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>Bigger</p> <pre><code>dict_params = {\n    'mu_1' : 1 ,'mu_2' : 1+5 ,\n    'std_dev_1':5 , 'std_dev_2':5, \n    'n_sample_1':500,'n_sample_2':500,\n    'n_repeat':1000\n}\nmean_1,mean_2 = generate_data(**dict_params, N=1000)\ngenerate_ab_viz(mean_1,mean_2)\n</code></pre> <p></p> <p>The more small effect size we want to detect \u2192 more power required</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#wrapping-up","title":"Wrapping Up","text":"<p>In this case we have already know how much effect we wanna test , in this case USD 1 Increase in Revenue oer User </p> <p>to calculate minimum sample size we can use :   </p> <p>with : </p> <ol> <li>\\(\\delta\\) : Effect Size</li> <li>\\(\\sigma\\) : variance of Population (estimated) from revenue per user variance </li> <li>\\(\\alpha/2\\) : alpha ,  \\(1- Confidence Interval\\)</li> <li>\\(\\beta\\) : beta, 1- Power Level</li> <li>\\(z\\) : z score</li> </ol> <p>Deciding \\(\\alpha\\) and \\(\\beta\\) , in a typical experiment </p> <ol> <li>\\(\\alpha\\) is 5% or Confidence is 95%</li> <li>\\(\\beta\\) is 20%  or Power is 80%</li> </ol> <pre><code>def calculate_minimum_sample(effect_size,std,alpha,beta) : \n    # obtain both zscore\n    z_alpha_2 = stats.norm.ppf(1 - alpha/2)\n    z_beta = stats.norm.ppf(1 - beta)\n\n    upper = 2*(std**2)*(z_alpha_2 + z_beta)**2\n    lower = effect_size**2\n\n    n_sample = math.ceil(upper/lower)\n    return n_sample\n</code></pre> <pre><code>n_sample = calculate_minimum_sample(effect_size=1,std=10,alpha=0.05,beta=0.2)\nn_sample\n</code></pre> <pre><code>1570\n</code></pre> <p>For each variant we should assign 1570 randomization unit, in this case is user </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#24-experiment-durations","title":"2.4. Experiment Durations","text":"<p>A good experiment duration &lt; 1 month , since the variation is only 2, we can try to conduct it by 1 week. Starting from February 2<sup>nd</sup>, 2024 to February 8<sup>th</sup>, 2024</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#3-running-experiments","title":"3. Running Experiments","text":"<p>Since we don't have a privilege to experimentation platform we will use generated data. </p> <p>But before running the experiments we will have to do A/A test ? </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#31-before-experiment-aa-tests","title":"3.1. Before Experiment : A/A Tests","text":"<p>Why ? Because there are maybe still not a ceterus paribus condition no bias , for example user may have experienced delayed response time that cause user to differ in behaviour which eventually will influence our metrics of interest</p> <p>How ? In order to make that not happen, we can compare same variant by having the same metrics. </p> <p>But, it's expensive to run experiment before experiment. There is a hackish way, by using metrics from last observation, for example last week metrics , and then randomly assign user two different group. </p> <p>Workflow : </p> <ol> <li>Obtain Last Week metrics from each user (revenue per user)</li> <li>For n iteration do : <ul> <li>Randomly Assign user into two groups</li> <li>Obtain means of each metrics from both group</li> <li>Conduct statistical test, to check differences of mean metrics between two variant using t-test</li> <li>Collect p-value</li> </ul> </li> <li>Test the p-value distribution whether followed uniform distribution using Goodness of Fit test such as Kolmogorov Smirnoff Test</li> <li>If the p-value &gt; \\(\\alpha\\) can be concluded that the samples is the same from referenced distribution ( in this case uniform distribution) </li> <li>Plot the p-value from n_iteration, the p value should be uniform (indicating variant is the same) </li> </ol> <p>We will generate using synthetic data </p> <pre><code>num_samples = int((100_000/30)*7)\n\ndef generate_data(num_samples,metric_name) : \n    dummy_aa_test = pd.DataFrame({\n        'user_id': np.arange(1, num_samples + 1),\n       f'{metric_name}': stats.halfnorm(loc=1, scale=10).rvs(num_samples) # constraint revenue to positive\n    })\n    return dummy_aa_test\n\ndummy_aa_test = generate_data(num_samples,metric_name='revenue_per_user')\ndummy_aa_test\n</code></pre> user_id revenue_per_user 0 1 5.264200 1 2 11.863647 2 3 5.423547 3 4 14.484032 4 5 15.570061 ... ... ... 23328 23329 9.137816 23329 23330 6.039538 23330 23331 16.040150 23331 23332 1.436674 23332 23333 5.646788 <p>23333 rows \u00d7 2 columns</p> <pre><code>def simulate_aa_test(n_iter,metric_name,num_samples) : \n    data = generate_data(num_samples,metric_name=metric_name)\n\n    p_vals_t_test = []\n    for i in range(n_iter) : \n        data['group'] = np.random.choice([1,2 ], size=data.shape[0])\n        metric_g1 = data[data['group'] == 1][metric_name]\n        metric_g2 = data[data['group'] == 2][metric_name]\n\n        mu_1 = metric_g1.mean()\n        mu_2 = metric_g2.mean()\n\n        t_stat, p_value = stats.ttest_ind(metric_g1, metric_g2)\n\n        p_vals_t_test.append(p_value)\n\n    # perform k-s test\n    ks_stat,p_val_ks_test = stats.kstest(p_vals_t_test,stats.uniform.cdf)    \n    return p_vals_t_test,p_val_ks_test\n</code></pre> <pre><code>p_value,ks_p_val = simulate_aa_test(n_iter=1000,metric_name='revenue_per_user',num_samples=num_samples) \n</code></pre> <pre><code>sns.histplot(p_value,alpha = 1, bins = 30,kde=True,\n            label = r\"p-value\",color='blue')\n</code></pre> <pre><code>&lt;Axes: ylabel='Count'&gt;\n</code></pre> <p></p> <pre><code>ks_p_val\n</code></pre> <pre><code>0.7919638330879013\n</code></pre> <p>The simulated A/A test is from the same distribution (uniform distribution) </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#4-analyzing-data","title":"4. Analyzing Data","text":"<p>Let's Recap our Experimentation Plan </p> <p>Metrics of Experimentation : Revenue per User </p> <p>Experiment Unit : User in Checkout Process </p> <p>Variant : </p> <pre><code>1. Control Group : No Product Recommendation during checkout process \n2. Treatment Group : Product Recommendation during checkout process y\n</code></pre> <p>Number of Samples : 1570 from each variant </p> <p>Experiment Duration : Experiment was conducted for a week , starting from February 2<sup>nd</sup>, 2024 to February 8<sup>th</sup>, 2024</p> <p>Our Hypothesis : by adding product recommendation could boost revenue per user</p> <p>So what do we do when it comes after experiment. We obtain data from experiment log from both of each group. However as Data Scientist we should be sceptical hence we need to perform Sanity Check </p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#41-data-check","title":"4.1. Data Check","text":"<pre><code>experiment_data = pd.read_csv('experiment_collected_data.csv',parse_dates=['datetime'])\nexperiment_data\n</code></pre> datetime user_id group revenue_per_user 0 2024-02-02 d5683160-da8e-4027-80eb-f1543a6203e5 control 16.433941 1 2024-02-03 c829d253-d79e-42b8-9fe0-f2f69bd9d7d0 treatment 2.274594 2 2024-02-03 4307a4d6-1125-4191-8aa2-a473c5f46c5c control 6.293043 3 2024-02-07 f23f605a-7a25-45bc-b4bf-3c9ddb4dfacb control 29.030031 4 2024-02-06 fb404923-91df-4c01-956c-3ede39361914 control 2.196385 ... ... ... ... ... 3135 2024-02-04 e56ea29f-a89f-46d0-a81d-1ad6759f26f3 treatment 13.885899 3136 2024-02-04 2597b6c8-71e2-4acc-87a0-8d775e7bf83f treatment 6.362146 3137 2024-02-02 5e758638-e6e2-49c8-81ab-187156b59f97 control 3.460954 3138 2024-02-03 310130df-086e-4c5e-911c-1401979195a5 control 1.898356 3139 2024-02-06 532dc215-a179-4874-9e8b-89d141fe10c9 control 2.944030 <p>3140 rows \u00d7 4 columns</p> <p>Check number of unique user from each group</p> <pre><code>experiment_data.groupby('group')['user_id'].nunique()\n</code></pre> <pre><code>group\ncontrol      1570\ntreatment    1570\nName: user_id, dtype: int64\n</code></pre> <p>Number of Unique User is like what we expect</p> <p>Next, we should check the experiment datetime </p> <pre><code>filter_1 = experiment_data['datetime'] &lt;= '2024-08-02'\nfilter_2 = experiment_data['datetime'] &gt;= '2024-02-02'\n\nexperiment_data.loc[~(filter_1&amp;filter_2)]\n</code></pre> datetime user_id group revenue_per_user <p>Our data is valid in terms of datetime as experiment plan</p> <p>Next, we should check if there is duplicate record from user assignment </p> <pre><code>experiment_data.duplicated(subset=['user_id','group']).sum()\n</code></pre> <pre><code>0\n</code></pre> <p>It's safe from duplicate user assignment</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#411-checking-guadrail-metrics","title":"4.1.1. Checking Guadrail Metrics","text":"<p>In real experiment platform, we should check , what we called as Guadrail Metrics, which should not be different. Otherwise it will affect the experiment outcome become bias, for example latency</p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#42-aggregating-data","title":"4.2. Aggregating Data","text":"<p>Next step after we have clean &amp; valid data, we should aggregate data to reflect metrics from both group </p> <pre><code>summarize_data = experiment_data.groupby('group').agg({'revenue_per_user':np.mean})\nsummarize_data\n</code></pre> <pre><code>C:\\Users\\fakhr\\AppData\\Local\\Temp\\ipykernel_21012\\1678331338.py:1: FutureWarning:\n\nThe provided callable &lt;function mean at 0x000001AD8D28E480&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n</code></pre> revenue_per_user group control 8.949105 treatment 9.852804 <pre><code>summarize_data['lift'] = summarize_data['revenue_per_user'].diff(1)\nsummarize_data\n</code></pre> revenue_per_user lift group control 8.949105 NaN treatment 9.852804 0.903699"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#43-performing-statistical-test","title":"4.3. Performing Statistical Test","text":"<p>Condition :  We are interested whether by providing product recommendation during checkout will increase revenue per user. </p> <p>Why we need to perform statistical test ? We can just compare the difference between mean Yes! at some point it is right, however we only have small sample, and trying to estimate the population, hence what we can use to leverage the statistical test to add confidence in our result </p> <p>Type of Statistical Test, we want to compare the revenue, in terms of mean difference between two groups (independent sample), we can leverage T-student Test for mean from two sample.  Since we are going to check its bigger or not, its one tailed test</p> <p>Hypothesis : </p> <p>\\(H_0 : \\mu_{\\text{treatment}} &lt;= \\mu_{\\text{control}}\\)</p> <p>\\(H_1 : \\mu_{\\text{treatment}} &gt; \\mu_{\\text{control}}\\)</p> <p>At experiment plan we want Confidence Interval of 95% or \\(\\alpha=0.05\\)</p> <p>Assumption : we don;t know variance about population, and both variance assumed to be equal </p> <p>How to conclude ?  1. If p-value &gt; \\(\\alpha\\) fail to reject the null Hypothesis, otherwise reject the null hypothesis 2. equivalent to if t-test value &lt; t-table fail to reject the null Hypothesis </p> <pre><code>control_group = experiment_data.loc[experiment_data['group']=='control','revenue_per_user'].values\ntreatment_group = experiment_data.loc[experiment_data['group']=='treatment','revenue_per_user'].values\n</code></pre> <pre><code>t_stat,p_val_t_test = stats.ttest_ind(treatment_group,control_group, equal_var=True, random_state=45, alternative='greater')\n</code></pre> <pre><code>p_val_t_test\n</code></pre> <pre><code>1.5326415297769846e-05\n</code></pre> <p>Proven that we can reject null hypothesis</p> <pre><code># Parameters\nn_samples = 2*1570\nalpha = 0.05\ndf = n_samples-2\ncritical_value = stats.t.ppf(1 - alpha, df)  # one tailed \n\n# t-distribution\nx = np.linspace(-4, 4, 1000) # generate some data \ny = stats.t.pdf(x, df)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label=f't-distribution with {df} degrees of freedom')\nplt.fill_between(x, y, where= (x &gt; critical_value), color='red', alpha=0.5, label='Rejection Region')\n\n# Add critical values\nplt.axvline(critical_value, color='black', linestyle='--', label=f'Critical Value: {critical_value:.3f}')\nplt.axvline(t_stat, color='green', linestyle='--', label=f'T statistic value')\n\n# Labels and title\nplt.title('Two-tailed t-test Rejection Region')\nplt.xlabel('t-value')\nplt.ylabel('pdf')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>Clearly we   reject the null hypothesis. Next Step we can generate confidence interval for our treatment effect using standard error </p> \\[ (\\bar{x}_{\\text{treatment}} - \\bar{x}_{\\text{control}} )  -  t_{\\alpha/2,df} \\cdot SE \\leq \\mu_{\\text{treatment}}- \\mu_{\\text{treatment}} \\leq (\\bar{x}_{\\text{treatment}} - \\bar{x}_{\\text{control}} )  +  t_{\\alpha/2,df} \\cdot SE\\] \\[SE = \\sqrt{(\\cfrac{s_1^2}{n_1} + \\cfrac{s_2^2}{n_2}})  \\] <p>with : </p> <ol> <li>\\(SE\\) : Standard error</li> <li>\\(s_1^2\\) : variance from group 1 , control group</li> <li>\\(s_2^2\\) : variance from group 2 , treatment group</li> <li>\\(n_1\\) : number of sample from group 1 , control group</li> <li>\\(n_2\\) : number of sample from group 2 , treatment group</li> <li>\\(t_{\\alpha/2 , df}\\) : t table value given alpha and degrees of freedom</li> </ol> <pre><code>difference = treatment_group.mean() - control_group.mean() \nn_1 = 1570 \nn_2 = 1570\ns_1 = control_group.var()\ns_2 = treatment_group.var()\n\nse = np.sqrt((s_1/n_1) + (s_2/n_2))\n\nt_alpha = stats.t.ppf(alpha/2, df=n_1+n_2-2)\n\n\nc_i = t_alpha*se\n\n\n\nprint(f'Effect Confidence Interval {difference} +/-{c_i}')\n</code></pre> <pre><code>Effect Confidence Interval 0.903699066036344 +/--0.4243055658869349\n</code></pre> <pre><code>c_i\n</code></pre> <pre><code>-0.4243055658869349\n</code></pre> <pre><code>difference\n</code></pre> <pre><code>0.903699066036344\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nplt.figure(figsize=(8, 6))\nplt.errorbar(y=1, x=difference, xerr=-c_i, fmt='o', ecolor='black', capsize=5, capthick=2, elinewidth=1, label='Revenue per User + CI')\nplt.vlines(x = 0, color = 'red',ymin = 0, ymax = 2, label = 'Statistically not Significant',linestyle='--')\nplt.vlines(x = 1, color = 'green',ymin = 0, ymax = 2, label = 'Practically Not Significant',linestyle='--')\n\nplt.title('Comparing Practically and Statistically Significant')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()\n</code></pre> <p></p>"},{"location":"002-measuring%20impact%20with%20AB%20Test/ab_test/#5-inferring-decision","title":"5. Inferring Decision","text":"<p>We already reach the end of the experimentation. We have to decided whether to launch product recommendation system during checkout process or not to all users. </p> <p>To rollout this feature , the result should be :  1. Statistically Significant 2. Practically Significant</p> <p>From the Result it looks like only Statistical Significance . However the revenue per user still not fullfill our minimum USD 1 Revenue per user increase, so we have to decide not to launch the product.</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/","title":"Targeting problem","text":"<p> Photo by engin akyurt on Unsplash</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#giving-offer-to-right-user","title":"Giving Offer to Right User","text":""},{"location":"003-making%20your%20customer%20stay/targeting_problem/#business-context","title":"Business Context","text":"<p>In a ride-hailing company , keeping both customer and driver active is crucial important, because both form equilibrium. </p> <ol> <li>Lack of Driver Availability lead to scarcity and ride service cannot be provided, also can lead to extreme price, due to dynamic pricing mechanism </li> <li>Lack of Customer of course hurt a lot, driver, company, etc. </li> </ol> <p>To balance those several ways can be approached, for example by giving a promo or coupon </p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#business-metrics","title":"Business Metrics","text":"<p>What is the suitable metrics for this? To help answer the question, we should answer  : </p> <ol> <li>What is the goal of promo ? One of the sign that user is active can be derive from Gross Booking </li> </ol> <p>Here is the relation with Revenue </p> <p>$$</p> <p>\\begin{align} \\text{Revenue from Service} &amp;= \\text{Average Revenue Per User}  \\cdot  \\text{Number of Active User} \\ \\text{Revenue from Service} &amp;= \\text{Average Gross Booking Per User } \\cdot \\text{Take Rate} \\cdot  (1-P(\\text{Churn Rate})) \\cdot  \\text{Total User} \\</p> <p>\\text{Revenue from Service} &amp;= \\cfrac{\\text{ Gross Booking  }}{\\text{Number of User }} \\cdot \\text{Take Rate} \\cdot  (1-P(\\text{Churn Rate})) \\cdot\\text{Total User} \\  \\text{Revenue from Service} &amp;= \\cfrac{\\text{ Gross Booking  }}{((1-P(\\text{Churn Rate})) \\cdot\\text{Total User})} \\cdot \\text{Take Rate} \\cdot  (1-P(\\text{Churn Rate})) \\cdot\\text{Total User} \\  \\end{align}</p> <p>$$</p> <ul> <li>Take Rate : Similar to Net Profit Margin </li> </ul>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#business-process","title":"Business Process","text":"<p>To better understand the problem, we should understand the business process first  </p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#problems","title":"Problems","text":"<p>The problem is we have limited budget constraint, let say called it B we cannot surpass. How can we maximize the benefit from giving promo and still within the constraint</p> <p>$$  </p> <p>$$</p> <p>$$  </p> <p>$$</p> <ol> <li>GB :  Gross Booking </li> <li>w_i : Binary 0/1 indicating whether user i receive promo or not</li> <li>IGB_i : Incremental Gross Booking from promo </li> <li>cost_i : Cost of Promo (Homogen) from user i</li> </ol> <p>So, how do we approach this problem ? </p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#candidate-solutions","title":"Candidate Solutions","text":"<p>There are several ways to approach a problem , we can do some literature review, on how industry approach this </p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#solution-1-using-machine-learning","title":"Solution 1 : Using Machine Learning","text":"<p>The first approach we can predict the outcome variable given treatment assign or not using any machine learning model. </p> <p>Data Looks : </p> userID X_1 .. X_n Treatment Outcome (GB) 1 <p>Pros : </p> <p>Easy to do just like machine learning case with regression or classification</p> <p>Cons : </p> <p>We cannot adding incrementality / the effect of treatment, hence it's biased to user which Outcome is the highest (for example user who are member )</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#solution-2-enter-causality","title":"Solution 2 : Enter Causality","text":"<p>Instead of just using machine learning to predict outcome variable, we can leverage causality to measure the incrementality of each user if user receive treatment (promo)</p> <p>Pros : </p> <p>Account for causality / incrementality factor</p> <p>Cons : </p> <p>Each user only can be observed one potential outcome, receiving treatment or not. Don't worry we still can estimate the counterfactual. </p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#choosen-solution","title":"Choosen Solution :","text":"<p>Considering our need to measure incrementality we choose the Solution 2. </p> <p>Step by Step Solution : </p> <p>1. Running Experiment</p> <p>First we need to design experiment by giving randomly coupon to user assign  , however we should be aware that people who offered coupon can either accept or do not accept their coupon </p> <p><pre><code>\ngraph LR\n    A(Coupon Assignment) --&gt;B(Coupon Claims)\n    B --&gt; C(Gross Booking)\n    D(Number of Bookings) --&gt; B\n    E(Income) --&gt; D\n    F(Membership) --&gt; B\n    E --&gt; F\n    F --&gt; C \n    D --&gt; C\n\n\n</code></pre> The variable we can observe :  1. Number of Bookings  2. Membership  3. Coupon Assignment </p> <p>Planning Experiment</p> <p>The truth is this is that the assignment is random and the Coupon Claims is not random hence, this is suitable for Causal Inference for Observational Data</p> <ul> <li>Experiment Duration : 1 Week only </li> <li>Randomization Unit : User Level </li> <li>Chance of getting Treatment : 30% </li> <li>Which User is candidate : User who still active in past week </li> <li>Coupon Budget : $1 </li> <li>Number of Users Active in Past Week : 2000</li> <li>Hence, the cost of experiment : 2000 x $ 1 = $2000</li> </ul> <p>2. Estimating Treatment Effect </p> <p>After we have obtained the experiment data we can train model to estimate the treatment effect </p> <p>3. Choosing Whose to be Treated </p> <p>After we have estimated the treatment effect on individual level, we can observe that treating all user is not good way, hence we need to choose what fraction of user to be treated</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#introduction-to-causal-inference","title":"Introduction to Causal Inference","text":"<p>Sometimes we are interested in question such as \"Does Implementing Minimum 9 Years Education impact GDP\" and so on. That is study of Causal Inference </p> <p>The Gold Standard of Measuring Causality is by Randomized Control Trial / Experiment, however sometimes treatment assignment imposible to be random or it's unethical. </p> <p>There is another method , called Causal Inference with Observational Data. </p> <p>Before diving more, we often heard , Correlation Does not Imply Causation</p> <p>But How do we can claim causality, here is several condition  to fullfil causality</p> <ol> <li>Temporal Sequencing </li> </ol> <p>If we have treatment (T) and Outcome Variable (Y). It means we do some treatment first then we observe the outcome variable. For example we want to measure the effect of rising price of commodity we want to measure the Demand. </p> <pre><code>\n   graph LR\n      A(Commodity Price) --&gt;B(Demand)\n\n\n\n</code></pre> <ol> <li>Non Spurious Relationship</li> </ol> <p>One meme about this </p> <p></p> <ol> <li>Removing alternate Causes (Confounding)</li> </ol> <p>Previously we want to measure the effect of rising price of commodity on demand, however there are many factors affecting demand,such as seasonality </p> <pre><code>\n   graph LR\n      A(Commodity Price) --&gt;B(Demand)\n      C(Seasonality Confounding) --&gt; B</code></pre> <p>How to remove this bias ? </p> <ol> <li>We could use randomization / random experiment such as A/B Test </li> <li>Controlling the Confounder for example by adding the confounding variable to model </li> </ol> <p>Potential Outcome</p> <p>The one who developed causality theory is Donald B Rubin, or called as Rubin Causality framework of potential outcome framework. </p> <p>So what is potential outcome, let say we have  a headache, how can we cure the headache, for example : we can take medicine. </p> <p>The outcome or result after taking medicine </p> <pre><code>\n   graph LR\n      A[Headache] --&gt;B[Take Pills]\n      B --&gt; C[Headache Gone]\n      B --&gt; D[Still Headache]\n\n      A --&gt; E[Doesnot Take Pills]\n      E --&gt; F[Headache Gone]\n      E --&gt; G[Still Headache]\n\n\n\n</code></pre> <p>Taking Pills or Not Taking Pills is a form of treatment say we denote as T , each person only can take one treatment at a time (take pills or not ) and only observe one outcome say we denote Y (still headache or not)</p> <p>those outcome, called potential outcome. </p> <p>Hence if we want to denote the outcome of each person, say \\(i\\) after taking pills is \\(Y_{1i}\\) </p> <p>The Treatment effect on individuals can be define as </p> <p>\\(\\text{Individual Treatment Effect}_i = Y_{1i} - Y_{0i}\\)</p> <p>However, again we can observe both at the same time then usually we come up with Average Treatment Effect </p> <p>which can be define as </p> <p>\\(\\text{Average Treatment Effect} = \\text{Average Outcome on Treated individual} - \\text{Average Outcome on Not Treated individual}\\)</p> <p>or </p> <p>\\(\\text{Average Treatment Effect} = E[Y_1 - Y_0 ]\\)</p> <p>with \\(E\\) as expected value or simply average </p> <p>Individual Treatment Effect </p> <p>You said that estimating treatment effect is quite impossible, how could you over with this? Don't worry we'll talk about industry implementation</p> <p>So why would you use individual treatment effects instead of Average Treatment Effect. </p> <p>Typical use of Average Treatment Effect :  - Program Evaluation  - Research  - Decision </p> <p>The reason about using Individual Treatment Effect is Personalization , why ? </p> <p>Individuals may react differently in different degree , for example in ride hailing case : </p> <ul> <li>We give discount offer to individual A \u2192 one week after the discount the user keep using  / ordering to ride hailing apps </li> <li>But that could be different in user Z \u2192 offered discount but not reacting at all </li> </ul> <p>Again what's the company interest ? Gaining Maximum Shareholder value , How ?  - Could from rising the revenue  - Or from reducing the cost </p> <p>In case we choose the revenue, how could we give treatment such as offer so that we could maximize the revenue given budget constraint : </p> <ul> <li>We select individual if we treat \u2192 gain highest treatment effect / incremental value </li> </ul> <p>\\(\\text{Individual Treatment Effect}_i = Y_{1i} - Y_{0i}\\)</p> <p>We only have one outcome at a time, hence we need to estimate the other one using model </p> <p>Let's dive in </p> <p>Modelling Workflow </p> <pre><code>\n   graph LR\n      A[Data Preparation] --&gt;B[Exploratory Data Analysis]\n      B --&gt; C[Data Preprocessing]\n      C --&gt; D[Modelling Phase]\n      D --&gt; E[Finding Best Model]\n\n\n\n</code></pre>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#data-preparation","title":"Data Preparation","text":"<pre><code>import pandas as pd \nimport numpy as np \nimport scipy.stats as st \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nfrom IPython.display import display\n\nimport warnings\n\n# Suppress all warnings\nwarnings.filterwarnings('ignore')\n</code></pre> <p>Here is the dataset definition</p> Variable Name Definition Data Type PromoID ID of Promos INT PromoType Type of Promos Offered  to Treatment Effect String PromoDate Date of Promos Given to Treated User String UserID UserID String Treatment Binary Value (0/1) indicating user receive treatment or not String GrossBooking Outcome Variable , Gross Booking in 7 Days String Age Customer Age String Gender Customer Gender String NumberOfBooking Count of user Booking in Last 7 Days String ResponseToPastPromo Binary Value (0/1) indicating whether user responded / claim last offered promo String LastBooking(Days) Number of Days  from last booking, given experiment date cutoff String isMember Binary Value (0/1) indicating whether user is a member String average_apps_open_week Number of user app open in 7 days String <pre><code>data = pd.read_csv('dataset/promo_continous_outcome.csv')\n</code></pre> <pre><code>data\n</code></pre> PromoID PromoType PromoDate UserID Treatment GrossBooking Age Gender NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week 0 1 NaN NaN 1 0 98.346167 56 male 16 1 148 0 30.940793 1 1 NaN NaN 2 0 99.150442 69 male 49 1 339 0 32.466263 2 1 NaN NaN 3 0 93.619220 46 male 39 0 115 0 30.170032 3 1 NaN NaN 4 0 87.382590 32 female 44 0 213 0 31.393513 4 1 NaN NaN 5 0 100.478985 60 male 31 1 293 0 30.839952 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2995 3 NaN 2024-01-07 996 1 107.387370 60 female 17 0 87 0 30.010098 2996 3 NaN NaN 997 0 97.674515 64 male 36 0 302 0 30.216715 2997 3 NaN NaN 998 0 90.007692 62 female 44 1 167 0 30.675957 2998 3 NaN NaN 999 0 108.604006 35 male 31 0 169 0 33.005855 2999 3 NaN NaN 1000 0 94.680165 55 male 35 0 134 0 30.483874 <p>3000 rows \u00d7 13 columns</p> <pre><code>input_col = ['Age', 'Gender', 'NumberOfBooking','ResponseToPastPromo','LastBooking(Days)', \n            'isMember','average_apps_open_week']\ntreatment_col = ['Treatment']\noutput_col = ['GrossBooking']\n</code></pre> <pre><code>X = data[input_col + treatment_col]\ny = data[output_col]\n</code></pre> <pre><code>X.head()\n</code></pre> Age Gender NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week Treatment 0 56 male 16 1 148 0 30.940793 0 1 69 male 49 1 339 0 32.466263 0 2 46 male 39 0 115 0 30.170032 0 3 32 female 44 0 213 0 31.393513 0 4 60 male 31 1 293 0 30.839952 0 <pre><code>y.head()\n</code></pre> GrossBooking 0 98.346167 1 99.150442 2 93.619220 3 87.382590 4 100.478985 <p>Split data into training and testing </p> <pre><code>from sklearn.model_selection import train_test_split\n\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\nX_val,X_test,y_val,y_test = train_test_split(X_test,y_test,test_size=0.5)\n</code></pre> <pre><code>X_train\n</code></pre> Age Gender NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week Treatment 970 53 male 26 0 45 0 30.041950 0 159 66 male 23 1 258 0 31.328888 0 2513 31 female 4 1 103 0 30.189209 0 1604 54 male 43 0 170 0 30.413598 0 436 33 female 1 1 224 1 30.281640 0 ... ... ... ... ... ... ... ... ... 826 23 female 15 0 178 0 30.325069 0 2839 36 male 1 1 338 1 30.525431 0 1112 34 male 31 1 25 1 30.312170 0 130 41 male 31 0 162 0 30.799777 0 744 54 male 45 0 16 0 30.807659 0 <p>2400 rows \u00d7 8 columns</p> <pre><code>y_train\n</code></pre> GrossBooking 970 101.384184 159 103.983185 2513 96.753668 1604 97.215699 436 103.875343 ... ... 826 90.591134 2839 101.575794 1112 101.633420 130 107.497184 744 102.238127 <p>2400 rows \u00d7 1 columns</p> <p>Data Preprocessing</p> <ul> <li>Encoding Categorical Variable</li> <li>Creating Feature : Interaction between Treatment and other covariates</li> </ul> <p>Encoding Categorical Variable</p> <pre><code>categorical_columns = ['Gender']\n</code></pre> <pre><code>from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse_output=False)\n\nencoder.fit(X_train[categorical_columns].values.reshape(-1,1))\n</code></pre> <pre>OneHotEncoder(sparse_output=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.OneHotEncoder<pre>OneHotEncoder(sparse_output=False)</pre> <pre><code>from IPython.display import display\n</code></pre> <pre><code>def encode_categorical(X,categorical_columns, train=True,encoder=None) : \n    X = X.copy()\n    X.index = [x for x in range(X.shape[0])]\n    if train : \n        encoder = OneHotEncoder(sparse_output=False)\n\n        encoder.fit(X[categorical_columns].values.reshape(-1,1))\n        feature_names = encoder.categories_[0]\n        encoded_values = encoder.transform(X[categorical_columns].values.reshape(-1,1))\n        encoded_df  = pd.DataFrame(encoded_values, columns=feature_names)\n\n\n        X.drop(categorical_columns,axis=1,inplace=True)\n\n        X_encoded = pd.concat([X,encoded_df],axis=1)\n\n        return X_encoded,encoder\n    else : \n        if encoder is None : \n            raise ValueError('you should add fitted encoder for other than training data')\n        feature_names = encoder.categories_[0]\n        encoded_values = encoder.transform(X[categorical_columns].values.reshape(-1,1))\n        encoded_df  = pd.DataFrame(encoded_values, columns=feature_names)\n\n        X.drop(categorical_columns,axis=1,inplace=True)\n\n        X_encoded = pd.concat([X,encoded_df],axis=1,join='inner')\n        return X_encoded,_\n</code></pre> <pre><code>X_train_encoded,encoder = encode_categorical(X=X_train,categorical_columns=categorical_columns,\n                                             train=True)\nX_test_encoded,_ = encode_categorical(X=X_test,categorical_columns=categorical_columns,\n                                             train=False,encoder=encoder)\nX_val_encoded,_ = encode_categorical(X=X_val,categorical_columns=categorical_columns,\n                                             train=False,encoder=encoder)\n</code></pre> <pre><code>X_train_encoded.shape\n</code></pre> <pre><code>(2400, 9)\n</code></pre> <pre><code>X_test_encoded.shape\n</code></pre> <pre><code>(300, 9)\n</code></pre> <pre><code>def generate_interaction(X,input_col,treatment_col) : \n    X = X.copy()\n    for col in input_col : \n        for t in treatment_col : \n            X[f'{col}_{t}_interaction'] = X[col] * X[t]\n    return X\n</code></pre> <pre><code>input_col = ['Age', 'female','male', 'NumberOfBooking','ResponseToPastPromo','LastBooking(Days)', \n            'isMember','average_apps_open_week']\ntreatment_col = ['Treatment']\noutput_col = ['GrossBooking']\n</code></pre> <pre><code>X_train_interaction = generate_interaction(X=X_train_encoded,input_col=input_col,treatment_col=treatment_col)\nX_test_interaction = generate_interaction(X=X_test_encoded,input_col=input_col,treatment_col=treatment_col)\nX_val_interaction = generate_interaction(X=X_val_encoded,input_col=input_col,treatment_col=treatment_col)\n</code></pre> <p>dropping column of non interaction column</p> <pre><code>X_train_interaction = X_train_interaction.drop(input_col,axis=1)\nX_test_interaction = X_test_interaction.drop(input_col,axis=1)\nX_val_interaction = X_val_interaction.drop(input_col,axis=1)\n</code></pre> <pre><code>X_train_interaction.shape\n</code></pre> <pre><code>(2400, 9)\n</code></pre> <pre><code>X_test_interaction.shape\n</code></pre> <pre><code>(300, 9)\n</code></pre> <pre><code>data_train = pd.concat([X_train_encoded.reset_index(drop=True),y_train.reset_index(drop=True)],axis=1,ignore_index=False)\ndata_test = pd.concat([X_test_encoded.reset_index(drop=True),y_test.reset_index(drop=True)],axis=1,ignore_index=False)\ndata_val = pd.concat([X_val_encoded.reset_index(drop=True),y_val.reset_index(drop=True)],axis=1,ignore_index=False)\n</code></pre> <p>hence  1. \\(X\\) or Covariate : <code>'GrossBookingValue', 'Age', 'Gender', 'NumberOfBooking','ResponseToPastPromo','LastBooking(Days)',              'isMember','average_apps_open_week'</code> 2. \\(T\\) or Treatment Variable  : <code>Treatment</code> 3. \\(Y\\) or Outcome Variable : <code>GrossBookingValue</code></p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#modelling-phase","title":"Modelling Phase","text":""},{"location":"003-making%20your%20customer%20stay/targeting_problem/#1st-linear-regression","title":"\\(1^{st}\\) Linear Regression","text":"<p>$$ \\begin{align} Y = \\sum^{K-1}{i=1} \\beta \\cdot X_i + \\beta_K \\cdot T \\</p> <p>\\dfrac{\\partial Y}{\\partial T} = \\beta_K \\ </p> <p>\\beta_K = \\textbf{ATE}</p> <p>\\end{align}</p> <p>$$</p> <p>It produces the ATE however what we need is is persnalized effect, hence we can add interaction term with covariate $$ \\begin{align} Y = \\sum^{K-1}{i=1} \\beta} \\cdot X_i + \\beta_K \\cdot T +  \\sum^{K-1{i=1} \\beta \\cdot X_i \\cdot T \\</p> <p>\\dfrac{\\partial Y}{\\partial T} = \\sum^{K-1}{i=1} \\beta \\cdot X_i  \\ </p> <p>\\sum^{K-1}{i=1} \\beta} \\cdot X_i = \\textbf{CATE</p> <p>\\end{align}</p> <p>$$</p> <p>However Linear Regression of course has limitation such as :  - Linearity Assumption </p> <p>Can we use any estimator that can produce non linearity ? </p> <p>Of Course : We could take a look at another estimator</p> <pre><code>from sklearn.linear_model import LinearRegression\n</code></pre> <pre><code>model_cate_lr = LinearRegression()\n\nmodel_cate_lr.fit(X_train_interaction,y_train)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre> <pre><code>ate_prediction = model_cate_lr.coef_ @ X_train_interaction.to_numpy().T\n</code></pre> <pre><code>model_cate_lr.predict(X_train_interaction)\n</code></pre> <pre><code>array([[99.80225631],\n       [99.80225631],\n       [99.80225631],\n       ...,\n       [99.80225631],\n       [99.80225631],\n       [99.80225631]])\n</code></pre> <pre><code>def predict_cate_lr(model,X) : \n    ate_prediction = model.coef_ @ X.to_numpy().T\n    return ate_prediction\n</code></pre> <pre><code>cate = X_train_interaction.copy()\ncate['predicted_cate'] = predict_cate_lr(model=model_cate_lr,X=X_train_interaction).flatten()\ncate = cate.sort_values('predicted_cate',ascending=False)\ncate\n</code></pre> Treatment Age_Treatment_interaction female_Treatment_interaction male_Treatment_interaction NumberOfBooking_Treatment_interaction ResponseToPastPromo_Treatment_interaction LastBooking(Days)_Treatment_interaction isMember_Treatment_interaction average_apps_open_week_Treatment_interaction predicted_cate 1822 1 46 0.0 1.0 47 1 148 0 31.044854 202.942291 576 1 47 0.0 1.0 46 1 188 0 31.266090 202.225657 418 1 25 0.0 1.0 40 1 194 1 31.692579 200.104725 513 1 25 0.0 1.0 40 1 194 1 31.692579 200.104725 2039 1 59 0.0 1.0 43 1 59 0 31.798532 200.035397 ... ... ... ... ... ... ... ... ... ... ... 840 0 0 0.0 0.0 0 0 0 0 0.000000 0.000000 841 0 0 0.0 0.0 0 0 0 0 0.000000 0.000000 842 0 0 0.0 0.0 0 0 0 0 0.000000 0.000000 2328 1 47 1.0 0.0 3 0 260 0 30.093732 -0.619641 2323 1 48 1.0 0.0 1 0 272 0 30.298811 -4.556660 <p>2400 rows \u00d7 10 columns</p> <pre><code>fig,ax = plt.subplots(nrows=2,ncols=2)\nsns.kdeplot(cate['predicted_cate'].values,ax=ax[0][0],label='Linear Regression CATE')\nax[0][0].set_title('LinReg CATE')\nplt.tight_layout()\nfig\n</code></pre> <p></p> <p></p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#2nd-meta-learners-models","title":"\\(2^{nd}\\) Meta Learners Models","text":"<p>Previously, we learn about we can extract the interaction term and use it to extract CATE, however the relationship is linear , what if we can implement non linear model such as Tree Based Method </p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#meta-learners-models-s-learners","title":"Meta Learners Models : \\(S-Learners\\)","text":"<p>The approach is quite simple </p> <ol> <li> <p>Train any machine learning model to all data  <pre><code>\n   graph LR\n      A(Covariate) --&gt;B(Y)\n      C(Treatment Variable) --&gt; B\n\n\n\n</code></pre></p> </li> <li> <p>During prediction phase : </p> </li> <li>Predict \\(Y_1\\) by input  Covariate + <code>Treatment=1</code> </li> <li>Predict \\(Y_0\\) Covariate + <code>Treatment=0</code></li> <li>Measure difference of  \\(Y_1\\) and \\(Y_0\\), That is our CATE</li> </ol> <p>Let's use boosting model such as AdaBoost </p> <pre><code>X_train_encoded\n</code></pre> Age NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week Treatment female male 0 53 26 0 45 0 30.041950 0 0.0 1.0 1 66 23 1 258 0 31.328888 0 0.0 1.0 2 31 4 1 103 0 30.189209 0 1.0 0.0 3 54 43 0 170 0 30.413598 0 0.0 1.0 4 33 1 1 224 1 30.281640 0 1.0 0.0 ... ... ... ... ... ... ... ... ... ... 2395 23 15 0 178 0 30.325069 0 1.0 0.0 2396 36 1 1 338 1 30.525431 0 0.0 1.0 2397 34 31 1 25 1 30.312170 0 0.0 1.0 2398 41 31 0 162 0 30.799777 0 0.0 1.0 2399 54 45 0 16 0 30.807659 0 0.0 1.0 <p>2400 rows \u00d7 9 columns</p> <pre><code>from sklearn.ensemble import AdaBoostRegressor\n\nslearner = AdaBoostRegressor(n_estimators=500)\n\nslearner.fit(data_train.drop(['GrossBooking'],axis=1),\n             data_train['GrossBooking'])\n</code></pre> <pre>AdaBoostRegressor(n_estimators=500)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostRegressor<pre>AdaBoostRegressor(n_estimators=500)</pre> <p>Time to estimate the cate </p> <pre><code>y_1_slearner = slearner.predict(data_train.drop(['GrossBooking'],axis=1).assign(Treatment=1))\ny_0_slearner = slearner.predict(data_train.drop(['GrossBooking'],axis=1).assign(Treatment=0))\n\ncate_slearner = y_1_slearner - y_0_slearner\ncate_slearner\n</code></pre> <pre><code>array([110.78624859, 104.76189742,  66.05373313, ...,  70.98508093,\n       110.78624859, 179.04466601])\n</code></pre> <pre><code>def predict_cate_slearner(data,model=slearner) : \n    y_1_slearner = model.predict(data.drop(['GrossBooking'],axis=1).assign(Treatment=1))\n    y_0_slearner = model.predict(data.drop(['GrossBooking'],axis=1).assign(Treatment=0))\n\n    cate_slearner = y_1_slearner - y_0_slearner\n    return cate_slearner\n</code></pre> <pre><code>sns.kdeplot(cate_slearner,ax=ax[0][1],label='S learner CATE')\nax[0][1].set_title('S Learner CATE')\nplt.tight_layout()\nfig\n</code></pre> <p></p> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#meta-learners-models-t-learners","title":"Meta Learners Models : \\(T-Learners\\)","text":"<p>The difference from S Learner is that T Learner  separately fit model to estimate \\(Y\\) for both treatment and non treatment group </p> <ol> <li>Train each model to predict \\(Y\\) for both treatment and control group</li> </ol> <pre><code>tlearner_1 = AdaBoostRegressor(n_estimators=500)\ntlearner_0 = AdaBoostRegressor(n_estimators=500)\n\nX_treatment = (data_train.loc[data_train['Treatment']==1]\n               .drop(treatment_col,axis=1)\n               .drop(output_col,axis=1))\n\nX_control = (data_train.loc[data_train['Treatment']==0]\n               .drop(treatment_col,axis=1)\n               .drop(output_col,axis=1))\ndisplay(X_treatment)\ny_treatment = data_train.loc[X_treatment.index,output_col]\ny_control = data_train.loc[X_control.index,output_col]\n\n\n\ntlearner_1.fit(X_treatment,y_treatment)\ntlearner_0.fit(X_control,y_control)\n</code></pre> Age NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week female male 24 18 38 1 166 0 30.742873 0.0 1.0 25 29 7 0 269 1 30.109229 0.0 1.0 33 21 33 1 277 0 31.526687 0.0 1.0 56 59 14 0 283 0 30.395622 0.0 1.0 64 45 14 0 132 0 30.900356 0.0 1.0 ... ... ... ... ... ... ... ... ... 2328 47 3 0 260 0 30.093732 1.0 0.0 2329 27 17 0 352 0 31.272954 0.0 1.0 2337 45 6 0 245 0 30.311971 0.0 1.0 2349 48 3 0 98 0 33.666700 1.0 0.0 2385 28 36 0 292 1 30.105533 1.0 0.0 <p>225 rows \u00d7 8 columns</p> <pre>AdaBoostRegressor(n_estimators=500)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostRegressor<pre>AdaBoostRegressor(n_estimators=500)</pre> <pre><code>X_treatment\n</code></pre> Age NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week female male 24 18 38 1 166 0 30.742873 0.0 1.0 25 29 7 0 269 1 30.109229 0.0 1.0 33 21 33 1 277 0 31.526687 0.0 1.0 56 59 14 0 283 0 30.395622 0.0 1.0 64 45 14 0 132 0 30.900356 0.0 1.0 ... ... ... ... ... ... ... ... ... 2328 47 3 0 260 0 30.093732 1.0 0.0 2329 27 17 0 352 0 31.272954 0.0 1.0 2337 45 6 0 245 0 30.311971 0.0 1.0 2349 48 3 0 98 0 33.666700 1.0 0.0 2385 28 36 0 292 1 30.105533 1.0 0.0 <p>225 rows \u00d7 8 columns</p> <p>Estimate the CATE</p> <pre><code>\n</code></pre> <pre><code>tlearner_y1 = tlearner_1.predict(X_train_encoded.drop(treatment_col,axis=1))\ntlearner_y0 = tlearner_0.predict(X_train_encoded.drop(treatment_col,axis=1))\n\ncate_tlearner = tlearner_y1 - tlearner_y0\n</code></pre> <pre><code>def predict_cate_tlearner(data,model1=tlearner_1,model0=tlearner_0) : \n\n    tlearner_y1 = model1.predict(data.drop(['GrossBooking','Treatment'],axis=1))\n    tlearner_y0 = model0.predict(data.drop(['GrossBooking','Treatment'],axis=1))\n\n    cate_tlearner = tlearner_y1 - tlearner_y0\n    return cate_tlearner\n</code></pre> <pre><code>sns.kdeplot(cate_tlearner,ax=ax[1][0],label='T learner CATE')\nax[1][0].set_title('T Learner CATE')\nplt.tight_layout()\nfig\n</code></pre> <p></p> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#meta-learners-models-x-learners","title":"Meta Learners Models : \\(X-Learners\\)","text":"<p>Previously we see progression from s learner to t learner</p> <ol> <li>S - T Learner : Separating Model for Estimating Conterfactual depends on group </li> </ol> <p>However often the case that treatment assignment is not equal, leading to bad estimation of CATE. What should we do next ? </p> <p>Instead of Treating equally the treatment effect, we can utilize some weighting mechanism to weight CATE from treatment group and CATE from non treated group. That is the idea of X- learner </p> <p>Procedure :  1. Train separate model to estimate Y for both treatment and control group </p> <p>\\(Model 1_{\\text{Stage 1}} = E[Y|X,T=1]\\)</p> <p>\\(Model 0_{\\text{Stage 2}} = E[Y|X,T=0]\\) 2. For each group </p> <pre><code>- Estimate CATE from treatment group\n\n  $\\tau_1 = Y_{T=1} - \\text{Model 0}_{\\text{Stage 1}}$\n</code></pre> <ul> <li> <p>Estimate CATE from control group </p> <p>\\(\\tau_0 = \\text{Model 1}_{\\text{Stage 1}} - Y_{T=0}\\) </p> </li> <li> <p>Fit Model to Estimate </p> <p>$\\text{Model 0}_{\\text{Stage 2}} = E[\\tau_0|X,T=0] $</p> <p>$\\text{Model 1}_{\\text{Stage 2}} = E[\\tau_1|X,T=1] $</p> </li> <li> <p>Fit Propensity Score Model </p> <p>$\\text{Propensity Model} = E[T=1|X] $</p> </li> <li> <p>During Prediction     Given Input \\(X_i\\) </p> </li> </ul> <p>$\\tau_i = (E[T=1|X_i]) * E[\\tau_1|X,T=1] + (1 - (E[T=1|X_i])) * E[\\tau_0|X,T=0] $</p> <p>hence \\(\\tau_i\\) is weighted prediction , ensemble of ensemble, yeay!</p> <ol> <li>Train separate model to estimate Y for both treatment and control group </li> </ol> <p>\\(Model 1_{\\text{Stage 1}} = E[Y|X,T=1]\\)</p> <p>\\(Model 0_{\\text{Stage 2}} = E[Y|X,T=0]\\)</p> <pre><code>xlearner_t0_s1 = AdaBoostRegressor(n_estimators=500)\nxlearner_t1_s1 = AdaBoostRegressor(n_estimators=500)\n\n\nX_treatment = (data_train.loc[data_train['Treatment']==1]\n               .drop(treatment_col,axis=1)\n               .drop(output_col,axis=1))\n\nX_control = (data_train.loc[data_train['Treatment']==0]\n               .drop(treatment_col,axis=1)\n               .drop(output_col,axis=1))\n\n\ny_treatment = data_train.loc[X_treatment.index,output_col]\ny_control = data_train.loc[X_control.index,output_col]\n\n\n\nxlearner_t1_s1.fit(X_treatment,y_treatment)\nxlearner_t0_s1.fit(X_control,y_control)\n</code></pre> <pre>AdaBoostRegressor(n_estimators=500)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostRegressor<pre>AdaBoostRegressor(n_estimators=500)</pre> <ol> <li> <p>For each group </p> <ul> <li>Estimate CATE from treatment group </li> </ul> <p>\\(\\tau_1 = Y_{T=1} - \\text{Model 0}_{\\text{Stage 1}}\\)</p> </li> <li> <p>Estimate CATE from control group </p> <p>\\(\\tau_0 = \\text{Model 1}_{\\text{Stage 1}} - Y_{T=0}\\) </p> </li> </ol> <pre><code>def estimate_partial_cate(data,model0,model1,Y,T) :\n    data = data.copy()\n    col_names = [x for x in data.index if x not in [Y,T]]\n\n\n    if data['Treatment'] ==1 :\n\n        counterfactual =  model0.predict(data[col_names].values.reshape(1,-1))\n\n        partial_tau = data[Y] - counterfactual\n        return partial_tau.item()\n\n    elif data['Treatment'] ==0 :\n\n        counterfactual =  model1.predict(data[col_names].values.reshape(1,-1))\n\n        partial_tau = counterfactual - data[Y]\n\n        return partial_tau.item()\n</code></pre> <pre><code>tau = data_train.apply(estimate_partial_cate,model0=xlearner_t0_s1,\n                                     model1=xlearner_t1_s1,Y='GrossBooking',T='Treatment',axis=1)\n</code></pre> <ol> <li> <p>Fit Model to Estimate </p> <p>$\\text{Model 0}_{\\text{Stage 2}} = E[\\tau_0|X,T=0] $</p> <p>$\\text{Model 1}_{\\text{Stage 2}} = E[\\tau_1|X,T=1] $</p> </li> </ol> <pre><code>type(tau)\n</code></pre> <pre><code>pandas.core.series.Series\n</code></pre> <pre><code>xlearner_t1_s2 = AdaBoostRegressor(n_estimators=500)\nxlearner_t0_s2 = AdaBoostRegressor(n_estimators=500)\n\ncol_to_exclude = ['GrossBooking','Treatment']\n\nX_treatment = (data_train.loc[data_train['Treatment']==1]\n               .drop(col_to_exclude,axis=1))\n\nX_control = (data_train.loc[data_train['Treatment']==0]\n               .drop(col_to_exclude,axis=1))\n\n\ny_treatment = tau.loc[X_treatment.index]\ny_control = tau.loc[X_control.index] \n\n\nxlearner_t1_s2.fit(X_treatment,y_treatment)\nxlearner_t0_s2.fit(X_control,y_control)\n</code></pre> <pre>AdaBoostRegressor(n_estimators=500)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostRegressor<pre>AdaBoostRegressor(n_estimators=500)</pre> <ol> <li> <p>Fit Propensity Score Model </p> <p>$\\text{Propensity Model} = E[T=1|X] $</p> </li> </ol> <pre><code>from sklearn.linear_model import LogisticRegression\n\npropensity_model = LogisticRegression()\n\n\npropensity_model.fit(data_train.drop(['Treatment','GrossBooking'],axis=1),data_train['Treatment'])\n</code></pre> <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression<pre>LogisticRegression()</pre> <ol> <li>During Prediction     Given Input \\(X_i\\) </li> </ol> <p>$\\tau_i = (E[T=1|X_i]) * E[\\tau_1|X,T=1] + (1 - (E[T=1|X_i])) * E[\\tau_0|X,T=0] $</p> <p>hence \\(\\tau_i\\) is weighted prediction , ensemble of ensemble, yeay!</p> <pre><code>propensity_score = propensity_model.predict_proba(data_train.drop(['Treatment','GrossBooking'],axis=1))\n\nxlearner_final_cate = (xlearner_t1_s2.predict(data_train.drop(['Treatment','GrossBooking'],axis=1))*propensity_score[:,1]) \\\n                        + (xlearner_t0_s2.predict(data_train.drop(['Treatment','GrossBooking'],axis=1))*propensity_score[:,0])\n</code></pre> <pre><code>def predict_cate_xlearner(data,model1_s2=xlearner_t1_s2,model0_s2=xlearner_t0_s2,propensity_model=propensity_model) : \n    propensity_score = propensity_model.predict_proba(data.drop(['Treatment','GrossBooking'],axis=1))\n\n    xlearner_final_cate = (model1_s2.predict(data.drop(['Treatment','GrossBooking'],axis=1))*propensity_score[:,1]) \\\n                        + (model0_s2.predict(data.drop(['Treatment','GrossBooking'],axis=1))*propensity_score[:,0])\n    return xlearner_final_cate\n</code></pre> <pre><code>sns.kdeplot(xlearner_final_cate,ax=ax[1][1],label='X learner CATE')\nax[1][1].set_title('X Learner CATE')\nplt.tight_layout()\nfig\n</code></pre> <p></p> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#3rd-modelling-using-target-transformation","title":"\\(3^{rd}\\)  Modelling using target transformation","text":"<p>the problem with CATE is that we cannot observe label of true CATE, only the estimate of it, can we a bit hackish so that we could estimate CATE on each individual observation ? Yess (Forcefully  ) we can use Target Transformation </p> <p>Why we need target transformation ? It's because in common machine learning problem , obtaining target variables mean :  1. Easy for evaluation simply like Regression Metrics (MAE,MSE) 2. Optimization Solution (MSE Loss for example)</p> <p>In Matheus Facure Book we can perform target transformation </p> <p>by first taking a look whether the experiment control and group are equal 50% : 50%, if so then : </p> <p>\\(Y_i^* = 2Y_i*T_i - 2Y_i*(1-T_i)\\)</p> <p>\\(2Y_i*T_i\\) = Outcome on Treatment \\(Y_i^1\\)</p> <p>\\(2Y_i*(1-T_i)\\) = Outcome on Treatment \\(Y_i^0\\)</p> <p>to check its validity we can verify  by checking whether </p> <p>\\(E[Y_i^*|X_i=x] = \\tau(x)_i\\)</p> <p>$$  </p> <p>$$</p> <p>However in our condition we are not possible to do so ? It's because we don't have 50 : 50 proportion , we can use </p> <p>\\(Y_i^* = Y_i * \\cfrac{T-e(X_i)}{e(X_i)(1-e(X_i))}\\)</p> <p>with  $e(X_i) $ is propensity model given \\(X_i\\)</p> <pre><code>def transform_y_star(data,propensity_model=propensity_model,treatment_col='Treatment',outcome_col='GrossBooking') : \n\n    # predict propensity score \n    propensity_score = propensity_model.predict_proba(data.drop([treatment_col,outcome_col],axis=1))\n\n    y_star = data[outcome_col] * (data[treatment_col] - propensity_score[:,1]) / (propensity_score[:,0]*propensity_score[:,1])\n    return y_star\n</code></pre> <pre><code>Y_star_train = transform_y_star(data_train)\ndisplay(Y_star_train)\n</code></pre> <pre><code>0      -110.755648\n1      -113.294355\n2      -106.189810\n3      -106.571512\n4      -116.448215\n           ...    \n2395   -101.712382\n2396   -116.399907\n2397   -111.188511\n2398   -119.168422\n2399   -110.466606\nLength: 2400, dtype: float64\n</code></pre> <pre><code>Y_star_val = transform_y_star(data_val)\ndisplay(Y_star_val)\n</code></pre> <pre><code>0      -119.085197\n1      -116.263022\n2       -99.919909\n3      -109.527690\n4      -117.176332\n          ...     \n295    -114.648191\n296     -98.236654\n297    1400.780810\n298    -113.283261\n299    -114.136317\nLength: 300, dtype: float64\n</code></pre> <pre><code>Y_star_test = transform_y_star(data_test)\ndisplay(Y_star_test)\n</code></pre> <pre><code>0     -113.693352\n1     -117.231432\n2     -109.337136\n3     -106.225375\n4     -118.357761\n          ...    \n295   -111.603392\n296   -120.213009\n297   -110.320215\n298    667.131541\n299   -105.251826\nLength: 300, dtype: float64\n</code></pre> <p>After this you can train those using any machine learning model </p> <pre><code>from sklearn.ensemble import RandomForestRegressor\n\nmodel_target_transform = RandomForestRegressor()\n\nmodel_target_transform.fit(data_train.drop(['GrossBooking','Treatment'],axis=1),Y_star_train)\n</code></pre> <pre>RandomForestRegressor()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor<pre>RandomForestRegressor()</pre> <pre><code>y_val_pred = model_target_transform.predict(data_val.drop(['GrossBooking','Treatment'],axis=1))\n</code></pre> <pre><code>from sklearn.metrics import mean_squared_error \n\n\nmean_squared_error(y_true=Y_star_val,y_pred=y_val_pred)\n</code></pre> <pre><code>1079896.9001854945\n</code></pre>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#evaluating-model","title":"Evaluating Model","text":"<p>The   standard of machine learning evaluation : </p> <ol> <li>Split Data into 2 (Training, Test) / 3 (Training, Test, &amp; Validation )</li> <li>Train model on training data </li> <li>Evaluate on Test / Validation Data </li> </ol> <p>The problem is : We don't have label of CATE , above mentioned target transformation still can't reflect the true labels of CATE just like unknown function. </p> <p>In this survey paper highlight some strategies to evaluate uplift modelling </p> <ol> <li>Using Traditional Uplift Metrics</li> </ol> <p>Using Uplift Metrics in forms of bin / groups </p> <p>Steps :     - Predict the uplift or CATE in both treatment and control groups     - Compute the average for each decile in each group     - Calculate the difference in each decile group (gain)</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#gain","title":"Gain","text":"<ol> <li>Predict CATE in both groups </li> </ol> <pre><code>inference_train = data_train.copy()\ninference_test = data_test.copy()\n\n\n# slearner  \n# inference_train['cate_lr'] = predict_cate_lr(model=model_cate_lr,X=inference_train\\\n#                             .drop(['GrossBooking','Treatment'],axis=1))\n# inference_test['cate_lr'] = predict_cate_lr(model=model_cate_lr,X=inference_test\\\n#                             .drop(['GrossBooking','Treatment'],axis=1))\n\n\ninference_train['cate_slearner'] = predict_cate_slearner(data=data_train.copy())\ninference_test['cate_slearner'] = predict_cate_slearner(data=data_test.copy())\n</code></pre> <ol> <li>Compute Average CATE in each groups decile </li> </ol> <pre><code># get groups \ncate_decile_treatment = inference_train.query('Treatment==1').assign(bin=pd.qcut(x=(inference_train.query('Treatment==1').\\\n                        loc[:,'cate_slearner']),q=10,labels=False ))\\\n                        .groupby('bin').agg({'cate_slearner':'mean'})\n\ncate_decile_control = inference_train.query('Treatment==0').assign(bin=pd.qcut(x=(inference_train.query('Treatment==0').\\\n                        loc[:,'cate_slearner']),q=10,labels=False ))\\\n                        .groupby('bin').agg({'cate_slearner':'mean'})\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\nCell In[59], line 2\n      1 # get groups \n----&gt; 2 cate_decile_treatment = inference_train.query('Treatment==1').assign(bin=pd.qcut(x=(inference_train.query('Treatment==1').\\\n      3                         loc[:,'cate_slearner']),q=10,labels=False ))\\\n      4                         .groupby('bin').agg({'cate_slearner':'mean'})\n      6 cate_decile_control = inference_train.query('Treatment==0').assign(bin=pd.qcut(x=(inference_train.query('Treatment==0').\\\n      7                         loc[:,'cate_slearner']),q=10,labels=False ))\\\n      8                         .groupby('bin').agg({'cate_slearner':'mean'})\n\n\nFile c:\\Users\\fakhr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\tile.py:340, in qcut(x, q, labels, retbins, precision, duplicates)\n    336 quantiles = np.linspace(0, 1, q + 1) if is_integer(q) else q\n    338 bins = x_idx.to_series().dropna().quantile(quantiles)\n--&gt; 340 fac, bins = _bins_to_cuts(\n    341     x_idx,\n    342     Index(bins),\n    343     labels=labels,\n    344     precision=precision,\n    345     include_lowest=True,\n    346     duplicates=duplicates,\n    347 )\n    349 return _postprocess_for_cut(fac, bins, retbins, original)\n\n\nFile c:\\Users\\fakhr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\tile.py:443, in _bins_to_cuts(x_idx, bins, right, labels, precision, include_lowest, duplicates, ordered)\n    441 if len(unique_bins) &lt; len(bins) and len(bins) != 2:\n    442     if duplicates == \"raise\":\n--&gt; 443         raise ValueError(\n    444             f\"Bin edges must be unique: {repr(bins)}.\\n\"\n    445             f\"You can drop duplicate edges by setting the 'duplicates' kwarg\"\n    446         )\n    447     bins = unique_bins\n    449 side: Literal[\"left\", \"right\"] = \"left\" if right else \"right\"\n\n\nValueError: Bin edges must be unique: Index([28.364680498458796,   65.0463209597256,  66.05373312568801,\n        66.05373312568801,  69.18811402071225,  85.11635587044222,\n       110.36429339664477, 117.96849741970071, 122.24310318631733,\n        155.1510308498945, 191.09766778653574],\n      dtype='float64', name='cate_slearner').\nYou can drop duplicate edges by setting the 'duplicates' kwarg\n</code></pre> <pre><code>display(cate_decile_treatment)\ndisplay(cate_decile_control)\n</code></pre> cate_slearner bin 0 57.149450 1 65.328915 2 74.068734 3 87.726081 4 90.563524 5 95.141914 6 100.721393 7 107.287273 8 134.848450 9 164.555192 cate_slearner bin 0 53.206568 1 65.003058 2 67.604734 3 86.066899 4 91.947500 5 96.878758 6 103.224171 7 121.331323 8 142.836045 9 172.046919 <ol> <li>Measure the difference (gain) for CATE</li> </ol> <pre><code>gain_ = cate_decile_treatment - cate_decile_control\n</code></pre> <p>Visualize </p> <pre><code>plot_slearner = sns.barplot(x=[x for x in range(10)],y=gain_.values.flatten())\nplt.title('Gain of CATE by decile S Learner')\n\n# Add data labels on the bars\nfor p in plot_slearner.patches:\n    plot_slearner.annotate(format(p.get_height(), '.1f'), \n                (p.get_x() + p.get_width() / 2., p.get_height()), \n                ha = 'center', va = 'center', \n                xytext = (0, 9), \n                textcoords = 'offset points')\n\n# Show the plot\nplt.show()\n</code></pre> <p></p> <pre><code>def evalute_uplift_bin(prediction_result,cate_column,t_col,model_name) : \n    cate_decile_treatment = prediction_result.query(f'{t_col}==1').assign(bin=pd.qcut(x=(prediction_result.query(f'{t_col}==1').\\\n                        loc[:,cate_column]),q=10,labels=False ))\\\n                        .groupby('bin').agg({cate_column:'mean'})\n\n    cate_decile_control = prediction_result.query(f'{t_col}==0').assign(bin=pd.qcut(x=(prediction_result.query(f'{t_col}==0').\\\n                            loc[:,cate_column]),q=10,labels=False ))\\\n                            .groupby('bin').agg({cate_column:'mean'})  \n\n    gain_ = cate_decile_treatment - cate_decile_control\n\n\n    plot_slearner = sns.barplot(x=[x for x in range(10)],y=gain_.values.flatten())\n\n    plt.title(f'Gain of CATE by decile {model_name}')\n\n    # Add data labels on the bars\n    for p in plot_slearner.patches:\n        plot_slearner.annotate(format(p.get_height(), '.1f'), \n                    (p.get_x() + p.get_width() / 2., p.get_height()), \n                    ha = 'center', va = 'center', \n                    xytext = (0, 9), \n                    textcoords = 'offset points')\n\n    # Show the plot\n    plt.show()\n\n    return gain_\n</code></pre> <pre><code>evalute_uplift_bin(prediction_result=inference_test,\n                   cate_column='cate_slearner',\n                   t_col='Treatment',\n                   model_name='Slearner')\n</code></pre> <p></p> cate_slearner bin 0 -6.903539 1 -4.941637 2 -11.728246 3 -22.237055 4 -8.996468 5 -3.817163 6 -4.217261 7 -11.009020 8 5.488645 9 -4.999156 <p>Let's Compare the models</p> <pre><code>inference_train['cate_xlearner'] = predict_cate_xlearner(data=data_train)\ninference_test['cate_xlearner'] = predict_cate_xlearner(data=data_test)\n\n\ninference_train['cate_tlearner'] = predict_cate_tlearner(data=data_train)\ninference_test['cate_tlearner'] = predict_cate_tlearner(data=data_test)\n\n\ninference_train['cate_tlearner'] = predict_cate_tlearner(data=data_train)\ninference_test['cate_tlearner'] = predict_cate_tlearner(data=data_test)\n</code></pre> <pre><code>_ = evalute_uplift_bin(prediction_result=inference_train,\n                   cate_column='cate_xlearner',\n                   t_col='Treatment',\n                   model_name='Xlearner Training')\n\n_ = evalute_uplift_bin(prediction_result=inference_train,\n                   cate_column='cate_tlearner',\n                   t_col='Treatment',\n                   model_name='Tlearner Training')\n</code></pre> <p></p> <p></p> <p>We can also craft it in cumulative way </p> <p>If we can see it's hard to compare both models, the good models should can we use some sort of direct comparative that is similar to AUC in common machine learning method, yes we can </p> <p>Uplift Curve </p> <p>\\(f(t) = \\left (    \\cfrac{Y_t^T}{N_t^T} - \\cfrac{Y_t^C}{N_t^C}    \\right ) \\left (  N_t^T + N_t^C\\right )\\)</p> <p>with :  1. \\(Y_t^T\\) : Cumulative Outcome Variable for Treatment Group up to \\(t^{th}\\) observation  2. \\(Y_t^C\\) : Cumulative Outcome Variable for Control Group up to \\(t^{th}\\) observation 3. \\(N_t^T\\) : Number of Observation for Treatment Group up to \\(t^{th}\\) observation 4. \\(N_t^C\\) : Number of Observation for Control Group up to \\(t^{th}\\) observation</p> <p>Essence : If we have good causal model then it will sort the same as the real outcome data  Procedure  1. Sort the data based on <code>uplift</code> or <code>cate columns</code> 2. Create Running Cumulation</p> <pre><code>def calculate_uplift_curve(data,treatment_col,outcome_col,uplift_col,min_periods=30, steps=100,normalize=True) : \n    size = data.shape[0]\n    data = data.copy()\n\n    # sort based on uplift_col\n    data = data.sort_values(uplift_col,ascending=False)\n    encoded_treatment = pd.get_dummies(data[treatment_col]).astype(int)\n    encoded_treatment.columns = [f'{treatment_col}_0',f'{treatment_col}_1']\n    data = data.drop(treatment_col,axis=1)\n    data = pd.concat([data,encoded_treatment],axis=1)\n\n    # multiply f'{treatment_col}_0',f'{treatment_col}_1' to outcome column \n    data[f'{outcome_col}_{treatment_col}_0'] = data[f'{treatment_col}_0'] * data[outcome_col] # our $Y_t^T$\n    data[f'{outcome_col}_{treatment_col}_1'] = data[f'{treatment_col}_1'] * data[outcome_col] # our $Y_t^C$\n\n    # calculate cumsum \n    data['Y_T'] = data[f'{outcome_col}_{treatment_col}_1'].cumsum()\n    data['Y_C'] = data[f'{outcome_col}_{treatment_col}_0'].cumsum()\n\n\n    data['N_T'] = data[f'{treatment_col}_1'].cumsum()\n    data['N_C'] = data[f'{treatment_col}_0'].cumsum()\n\n\n\n    # calculate f_t \n\n    # uplift =  ( ( (data['Y_T']/data['N_T']) \\\n    #                     - (data['Y_C']/data['N_C']) ) * (data['N_T'] + data['N_C']) ).values\n    # if normalize : \n    #     max_value = uplift.max()\n    #     uplift = uplift/max_value\n\n    # instead of calculating uplift for each row / observation calculate at each step \n    # inspired from https://matheusfacure.github.io/python-causality-handbook/19-Evaluating-Causal-Models.html#references\n    n_rows = list(range(min_periods, size, size // steps)) + [size] \n    n_rows = [x-1 for x in n_rows]\n    print('n rows',n_rows)\n    cumulative_uplifts = np.zeros(len(n_rows))\n    for idx,rows in enumerate(n_rows) : \n        data_at = data.iloc[rows,:]\n\n        uplift =   ( (data_at['Y_T']/data_at['N_T']) \\\n                                - (data_at['Y_C']/data_at['N_C']) ) * (data_at['N_T'] + data_at['N_C']) \n\n        cumulative_uplifts[idx] = uplift\n\n\n    pct = [rows/size for rows in n_rows]\n\n    if normalize : \n        # normalizing the uplift into 0 to 1 scale so that comparable, \n        max_value = cumulative_uplifts.max()\n        cumulative_uplifts = cumulative_uplifts/max_value\n\n\n    return pct, cumulative_uplifts\n</code></pre> <pre><code>pct,slearner_gain = calculate_uplift_curve(data=inference_train,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_slearner',normalize=True)\npct,xlearner_gain = calculate_uplift_curve(data=inference_train,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_xlearner',normalize=True)\npct,tlearner_gain =   calculate_uplift_curve(data=inference_train,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_tlearner',normalize=True)\n</code></pre> <pre><code>n rows [29, 53, 77, 101, 125, 149, 173, 197, 221, 245, 269, 293, 317, 341, 365, 389, 413, 437, 461, 485, 509, 533, 557, 581, 605, 629, 653, 677, 701, 725, 749, 773, 797, 821, 845, 869, 893, 917, 941, 965, 989, 1013, 1037, 1061, 1085, 1109, 1133, 1157, 1181, 1205, 1229, 1253, 1277, 1301, 1325, 1349, 1373, 1397, 1421, 1445, 1469, 1493, 1517, 1541, 1565, 1589, 1613, 1637, 1661, 1685, 1709, 1733, 1757, 1781, 1805, 1829, 1853, 1877, 1901, 1925, 1949, 1973, 1997, 2021, 2045, 2069, 2093, 2117, 2141, 2165, 2189, 2213, 2237, 2261, 2285, 2309, 2333, 2357, 2381, 2399]\nn rows [29, 53, 77, 101, 125, 149, 173, 197, 221, 245, 269, 293, 317, 341, 365, 389, 413, 437, 461, 485, 509, 533, 557, 581, 605, 629, 653, 677, 701, 725, 749, 773, 797, 821, 845, 869, 893, 917, 941, 965, 989, 1013, 1037, 1061, 1085, 1109, 1133, 1157, 1181, 1205, 1229, 1253, 1277, 1301, 1325, 1349, 1373, 1397, 1421, 1445, 1469, 1493, 1517, 1541, 1565, 1589, 1613, 1637, 1661, 1685, 1709, 1733, 1757, 1781, 1805, 1829, 1853, 1877, 1901, 1925, 1949, 1973, 1997, 2021, 2045, 2069, 2093, 2117, 2141, 2165, 2189, 2213, 2237, 2261, 2285, 2309, 2333, 2357, 2381, 2399]\nn rows [29, 53, 77, 101, 125, 149, 173, 197, 221, 245, 269, 293, 317, 341, 365, 389, 413, 437, 461, 485, 509, 533, 557, 581, 605, 629, 653, 677, 701, 725, 749, 773, 797, 821, 845, 869, 893, 917, 941, 965, 989, 1013, 1037, 1061, 1085, 1109, 1133, 1157, 1181, 1205, 1229, 1253, 1277, 1301, 1325, 1349, 1373, 1397, 1421, 1445, 1469, 1493, 1517, 1541, 1565, 1589, 1613, 1637, 1661, 1685, 1709, 1733, 1757, 1781, 1805, 1829, 1853, 1877, 1901, 1925, 1949, 1973, 1997, 2021, 2045, 2069, 2093, 2117, 2141, 2165, 2189, 2213, 2237, 2261, 2285, 2309, 2333, 2357, 2381, 2399]\n</code></pre> <pre><code>\n</code></pre> <pre><code>from sklearn.metrics import auc\n\nsns.lineplot(x =pct,y=xlearner_gain,label=f'X-learner, AUC {auc(pct,xlearner_gain).round(3)}')\nsns.lineplot(x =pct,y=slearner_gain,label=f'S-learner, AUC {auc(pct,slearner_gain).round(3)}')\nsns.lineplot(x =pct,y=tlearner_gain,label=f'T-learner, AUC {auc(pct,tlearner_gain).round(3)}')\nsns.lineplot(x=[0, 1],y= [0, 1],markers='--',label='Random treatment, AUC = 0.5')\nplt.title('Cumulative Gain Curve in Training Data')\n</code></pre> <pre><code>Text(0.5, 1.0, 'Cumulative Gain Curve in Training Data')\n</code></pre> <p></p> <p>In uplift curve still there are limitation, where if not in randomized experiment it fails to make group in cumulative comparable. Hence we need other solution</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#qini-curve","title":"Qini Curve","text":"<p>The next one to eva</p> <p>According to Radcliffe (2007) qini curve can be defined as :  \\(g(t) = \\left (    Y_t^T - \\cfrac{Y_t^C N_t^T} {N_t^C}    \\right )\\)</p> <p>if we dig up little bit </p> <p>\\(g(t) = \\left (   \\cfrac{ Y_t^T N_t^C}{N_t^C} - \\cfrac{Y_t^C N_t^T} {N_t^C}    \\right )\\)</p> <p>to make it comparable outcome for both treatment and control are scaled / multiplied by their counterpart</p> <p>with :  1. \\(Y_t^T\\) : Cumulative Outcome Variable for Treatment Group up to \\(t^{th}\\) observation  2. \\(Y_t^C\\) : Cumulative Outcome Variable for Control Group up to \\(t^{th}\\) observation 3. \\(N_t^T\\) : Number of Observation for Treatment Group up to \\(t^{th}\\) observation 4. \\(N_t^C\\) : Number of Observation for Control Group up to \\(t^{th}\\) observation</p> <pre><code>def calculate_qini_curve(data,treatment_col,outcome_col,uplift_col,min_periods=30, steps=100,normalize=True) : \n    size = data.shape[0]\n    data = data.copy()\n\n    # sort based on uplift_col\n    data = data.sort_values(uplift_col,ascending=False)\n    encoded_treatment = pd.get_dummies(data[treatment_col]).astype(int)\n    encoded_treatment.columns = [f'{treatment_col}_0',f'{treatment_col}_1']\n    data = data.drop(treatment_col,axis=1)\n    data = pd.concat([data,encoded_treatment],axis=1)\n\n    # multiply f'{treatment_col}_0',f'{treatment_col}_1' to outcome column \n    data[f'{outcome_col}_{treatment_col}_0'] = data[f'{treatment_col}_0'] * data[outcome_col] # our $Y_t^T$\n    data[f'{outcome_col}_{treatment_col}_1'] = data[f'{treatment_col}_1'] * data[outcome_col] # our $Y_t^C$\n\n    # calculate cumsum \n    data['Y_T'] = data[f'{outcome_col}_{treatment_col}_1'].cumsum()\n    data['Y_C'] = data[f'{outcome_col}_{treatment_col}_0'].cumsum()\n\n\n    data['N_T'] = data[f'{treatment_col}_1'].cumsum()\n    data['N_C'] = data[f'{treatment_col}_0'].cumsum()\n\n\n\n    # calculate f_t \n\n    # uplift =  ( ( (data['Y_T']/data['N_T']) \\\n    #                     - (data['Y_C']/data['N_C']) ) * (data['N_T'] + data['N_C']) ).values\n    # if normalize : \n    #     max_value = uplift.max()\n    #     uplift = uplift/max_value\n\n    # instead of calculating uplift for each row / observation calculate at each step \n    # inspired from https://matheusfacure.github.io/python-causality-handbook/19-Evaluating-Causal-Models.html#references\n    n_rows = list(range(min_periods, size, size // steps)) + [size] \n    n_rows = [x-1 for x in n_rows]\n    print('n rows',n_rows)\n    cumulative_uplifts = np.zeros(len(n_rows))\n    for idx,rows in enumerate(n_rows) : \n        data_at = data.iloc[rows,:]\n\n        uplift =    (data_at['Y_T']) \\\n                                - ( (data_at['Y_C']*data_at['N_T']) / data_at['N_C'] ) \n\n        cumulative_uplifts[idx] = uplift\n\n\n    pct = [rows/size for rows in n_rows]\n\n    if normalize : \n        # normalizing the uplift into 0 to 1 scale so that comparable, \n        max_value = cumulative_uplifts.max()\n        cumulative_uplifts = cumulative_uplifts/max_value\n\n\n    return pct, cumulative_uplifts\n</code></pre> <pre><code>pct,slearner_qini = calculate_qini_curve(data=inference_train,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_slearner',normalize=True)\npct,xlearner_qini = calculate_qini_curve(data=inference_train,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_xlearner',normalize=True)\npct,tlearner_qini =   calculate_qini_curve(data=inference_train,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_tlearner',normalize=True)\n</code></pre> <pre><code>n rows [29, 53, 77, 101, 125, 149, 173, 197, 221, 245, 269, 293, 317, 341, 365, 389, 413, 437, 461, 485, 509, 533, 557, 581, 605, 629, 653, 677, 701, 725, 749, 773, 797, 821, 845, 869, 893, 917, 941, 965, 989, 1013, 1037, 1061, 1085, 1109, 1133, 1157, 1181, 1205, 1229, 1253, 1277, 1301, 1325, 1349, 1373, 1397, 1421, 1445, 1469, 1493, 1517, 1541, 1565, 1589, 1613, 1637, 1661, 1685, 1709, 1733, 1757, 1781, 1805, 1829, 1853, 1877, 1901, 1925, 1949, 1973, 1997, 2021, 2045, 2069, 2093, 2117, 2141, 2165, 2189, 2213, 2237, 2261, 2285, 2309, 2333, 2357, 2381, 2399]\nn rows [29, 53, 77, 101, 125, 149, 173, 197, 221, 245, 269, 293, 317, 341, 365, 389, 413, 437, 461, 485, 509, 533, 557, 581, 605, 629, 653, 677, 701, 725, 749, 773, 797, 821, 845, 869, 893, 917, 941, 965, 989, 1013, 1037, 1061, 1085, 1109, 1133, 1157, 1181, 1205, 1229, 1253, 1277, 1301, 1325, 1349, 1373, 1397, 1421, 1445, 1469, 1493, 1517, 1541, 1565, 1589, 1613, 1637, 1661, 1685, 1709, 1733, 1757, 1781, 1805, 1829, 1853, 1877, 1901, 1925, 1949, 1973, 1997, 2021, 2045, 2069, 2093, 2117, 2141, 2165, 2189, 2213, 2237, 2261, 2285, 2309, 2333, 2357, 2381, 2399]\nn rows [29, 53, 77, 101, 125, 149, 173, 197, 221, 245, 269, 293, 317, 341, 365, 389, 413, 437, 461, 485, 509, 533, 557, 581, 605, 629, 653, 677, 701, 725, 749, 773, 797, 821, 845, 869, 893, 917, 941, 965, 989, 1013, 1037, 1061, 1085, 1109, 1133, 1157, 1181, 1205, 1229, 1253, 1277, 1301, 1325, 1349, 1373, 1397, 1421, 1445, 1469, 1493, 1517, 1541, 1565, 1589, 1613, 1637, 1661, 1685, 1709, 1733, 1757, 1781, 1805, 1829, 1853, 1877, 1901, 1925, 1949, 1973, 1997, 2021, 2045, 2069, 2093, 2117, 2141, 2165, 2189, 2213, 2237, 2261, 2285, 2309, 2333, 2357, 2381, 2399]\n</code></pre> <pre><code>from sklearn.metrics import auc\n\nsns.lineplot(x =pct,y=xlearner_qini,label=f'X-learner , AUC {auc(pct,xlearner_qini).round(3)}')\nsns.lineplot(x =pct,y=slearner_qini,label=f'S-learner, AUC {auc(pct,slearner_qini).round(3)}')\nsns.lineplot(x =pct,y=tlearner_qini,label=f'T-learner, AUC {auc(pct,tlearner_qini).round(3)}')\nsns.lineplot(x=[0, 1],y= [0, 1],markers='--',label='Random treatment, AUC = 0.5')\nplt.title('Cumulative Qini Curve in Training Data')\n</code></pre> <pre><code>Text(0.5, 1.0, 'Cumulative Qini Curve in Training Data')\n</code></pre> <p></p> <ol> <li>Evaluation Metrics Based on Target Transformation \\((Y^*)\\)</li> </ol> <p>The problem we have when estimating CATE is based on our target transformation \\(Y_i^*\\)</p> <p>our goal is to minimize </p> <p>\\(\\text{min } E[(\\tau_i - \\hat{\\tau_i})^2]\\)</p> <p>\\(\\text{min } E[(Y_i^* - \\hat{\\tau_i})^2]\\)</p> <p>easy task right? you find often in common machine learning </p> <pre><code>train_xlearner_cate = predict_cate_xlearner(data=data_train)\ntrain_slearner_cate = predict_cate_slearner(data=data_train)\ntrain_tlearner_cate = predict_cate_tlearner(data=data_train)\n\ntrain_target_transformation = model_target_transform.predict(data_train.drop(['GrossBooking','Treatment'],axis=1))\n</code></pre> <pre><code>train_target_transformation\n</code></pre> <pre><code>array([-112.40042316,  -58.37587738,  -81.14681886, ..., -108.8789247 ,\n       -103.30859488,  -94.30339274])\n</code></pre> <pre><code>mse_xlearner = mean_squared_error(Y_star_train,train_xlearner_cate)\nmse_slearner = mean_squared_error(Y_star_train,train_slearner_cate)\nmse_tlearner = mean_squared_error(Y_star_train,train_tlearner_cate)\nmse_target_transform = mean_squared_error(Y_star_train,train_target_transformation)\n</code></pre> <pre><code>pd.DataFrame({\n    'model_name' : ['X Learner','S Learner',\n                    'T Learner','Target Transformation'] , \n    'mse' : [mse_xlearner,mse_slearner,mse_tlearner,mse_target_transform]\n})\n</code></pre> model_name mse 0 X Learner 617944.724506 1 S Learner 618012.490589 2 T Learner 617297.753014 3 Target Transformation 374992.073824"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#finding-best-model","title":"Finding Best Model","text":"<p>Common ways to pick best model in machine learning can be done by  1. Cross Validation  2. Finding Model that has the best metrics in Validation Set </p> <p>Both are good, but now we will try second approach as it simple. </p> <p>Steps : </p> <ol> <li> <p>Choose Metrics </p> <p>If we see the that target transformation rely on propensity model, since then when we estimate the error it would be biased to the model fit </p> <p>Hence for now we will use Qini score as metrics </p> </li> <li> <p>Predict on Validation Data </p> </li> </ol> <pre><code>inference_val = data_val.copy()\n</code></pre> <pre><code>inference_val['cate_xlearner'] = predict_cate_xlearner(data=data_val)\ninference_val['cate_slearner'] = predict_cate_slearner(data=data_val)\ninference_val['cate_tlearner'] = predict_cate_tlearner(data=data_val)\n\ninference_val['cate_targettransform'] = model_target_transform.predict(data_val.drop(['GrossBooking','Treatment'],axis=1))\n</code></pre> <ol> <li>Evalute </li> </ol> <pre><code>pct,slearner_qini_val = calculate_qini_curve(data=inference_val,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_slearner',normalize=True)\npct,xlearner_qini_val = calculate_qini_curve(data=inference_val,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_xlearner',normalize=True)\npct,tlearner_qini_val =   calculate_qini_curve(data=inference_val,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_tlearner',normalize=True)\npct,target_transform_qini_val =   calculate_qini_curve(data=inference_val,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_targettransform',normalize=True)\n</code></pre> <pre><code>n rows [29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299]\nn rows [29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299]\nn rows [29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299]\nn rows [29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299]\n</code></pre> <ol> <li>Summarize</li> </ol> <pre><code>from sklearn.metrics import auc\n\nsns.lineplot(x =pct,y=xlearner_qini_val,label=f'X-learner , AUC {auc(pct,xlearner_qini_val).round(3)}')\nsns.lineplot(x =pct,y=slearner_qini_val,label=f'S-learner, AUC {auc(pct,slearner_qini_val).round(3)}')\nsns.lineplot(x =pct,y=tlearner_qini_val,label=f'T-learner, AUC {auc(pct,tlearner_qini_val).round(3)}')\nsns.lineplot(x =pct,y=target_transform_qini_val,label=f'Target Transformation, AUC {auc(pct,target_transform_qini_val).round(3)}')\nsns.lineplot(x=[0, 1],y= [0, 1],markers='--',label='Random treatment, AUC = 0.5')\nplt.title('Cumulative Qini Curve in Validation Data')\n</code></pre> <pre><code>Text(0.5, 1.0, 'Cumulative Qini Curve in Validation Data')\n</code></pre> <p></p> <p>Wee see that the best model is Target Transformation</p> <p>Next we evaluate on test data</p> <pre><code>inference_test = data_test.copy()\n</code></pre> <pre><code>inference_test['cate_xlearner'] = predict_cate_xlearner(data=data_test)\n\npct,xlearner_qini_test =   calculate_qini_curve(data=inference_test,\n                       treatment_col='Treatment',\n                       outcome_col='GrossBooking',\n                       uplift_col='cate_xlearner',normalize=True)\n\nsns.lineplot(x =pct,y=xlearner_qini_test,label=f'X-Learner, AUC {auc(pct,xlearner_qini_test).round(3)}')\nsns.lineplot(x=[0, 1],y= [0, 1],markers='--',label='Random treatment, AUC = 0.5')\n</code></pre> <pre><code>n rows [29, 32, 35, 38, 41, 44, 47, 50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98, 101, 104, 107, 110, 113, 116, 119, 122, 125, 128, 131, 134, 137, 140, 143, 146, 149, 152, 155, 158, 161, 164, 167, 170, 173, 176, 179, 182, 185, 188, 191, 194, 197, 200, 203, 206, 209, 212, 215, 218, 221, 224, 227, 230, 233, 236, 239, 242, 245, 248, 251, 254, 257, 260, 263, 266, 269, 272, 275, 278, 281, 284, 287, 290, 293, 296, 299]\n\n\n\n\n\n&lt;Axes: &gt;\n</code></pre> <p></p> <pre><code>inference_test.sort_values('cate_targettransform',ascending=False)\n</code></pre> Age NumberOfBooking ResponseToPastPromo LastBooking(Days) isMember average_apps_open_week Treatment female male GrossBooking cate_targettransform 86 46 47 1 148 0 31.044854 1 0.0 1.0 360.935582 3885.275750 222 46 48 1 147 0 30.330558 1 0.0 1.0 390.989137 3410.578766 159 67 43 1 108 0 30.298712 1 0.0 1.0 338.279315 3311.544684 256 47 44 1 163 1 31.468509 1 1.0 0.0 302.332050 3279.984190 32 21 33 1 277 0 31.526687 1 0.0 1.0 320.412054 3180.702677 ... ... ... ... ... ... ... ... ... ... ... ... 127 26 9 1 25 0 30.465704 0 1.0 0.0 99.805883 -120.504732 118 36 5 0 330 0 30.113201 0 0.0 1.0 103.206083 -120.914619 135 54 29 1 357 1 30.346237 0 0.0 1.0 101.282853 -121.138202 81 24 45 0 285 0 31.675850 0 1.0 0.0 98.896170 -122.740248 124 33 1 1 224 1 30.281640 0 1.0 0.0 107.618674 -125.771994 <p>300 rows \u00d7 11 columns</p>"},{"location":"003-making%20your%20customer%20stay/targeting_problem/#references","title":"References","text":"<ol> <li>Fascure M. (2020), \"Causal Inference for the Brave and True\", https://matheusfacure.github.io/python-causality-handbook</li> <li>Double Machine Learning for Treatment and Causal Parameters</li> <li>Radcliffe, N. J., 2007a. Using Control Groups to Target on Predicted Lift: Building and Assessing Uplift Models. Direct Marketing Journal, Direct Marketing Asso-ciation Analytics Council (1), pp. 14-21.</li> <li>Houssam Nassif, Finn Kuusisto, Elizabeth S Burnside, and Jude W Shavlik. Uplift modeling with roc: An srl case study. In ILP (Late Breaking Papers), pages 40\u201345, 2013.</li> </ol>"}]}